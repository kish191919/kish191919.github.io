<!DOCTYPE html><html lang="[&quot;en&quot;,&quot;ko&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AWS Certified AI Practitioner(30) - Hyperparameter Tuning | Danny's Blog</title><meta name="author" content="Danny Ki"><meta name="copyright" content="Danny Ki"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ë‚˜ì˜ ì†Œê°œThank you for having me for an interview and my name is Sunghwan ki but you can go by DannyI work as Data Engineer with 6 years experience in building ETL process, especially in the financial ind">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS Certified AI Practitioner(30) - Hyperparameter Tuning">
<meta property="og:url" content="https://kish191919.github.io/2025/08/26/AWS-Certified-AI-Practitioner-30/index.html">
<meta property="og:site_name" content="Danny&#39;s Blog">
<meta property="og:description" content="ë‚˜ì˜ ì†Œê°œThank you for having me for an interview and my name is Sunghwan ki but you can go by DannyI work as Data Engineer with 6 years experience in building ETL process, especially in the financial ind">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://kish191919.github.io/img/my_pic.jpeg">
<meta property="article:published_time" content="2025-08-26T19:20:21.000Z">
<meta property="article:modified_time" content="2025-11-10T17:41:47.673Z">
<meta property="article:author" content="Danny Ki">
<meta property="article:tag" content="AWS">
<meta property="article:tag" content="AWS_AI_PRACTITIONER">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kish191919.github.io/img/my_pic.jpeg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AWS Certified AI Practitioner(30) - Hyperparameter Tuning",
  "url": "https://kish191919.github.io/2025/08/26/AWS-Certified-AI-Practitioner-30/",
  "image": "https://kish191919.github.io/img/my_pic.jpeg",
  "datePublished": "2025-08-26T19:20:21.000Z",
  "dateModified": "2025-11-10T17:41:47.673Z",
  "author": [
    {
      "@type": "Person",
      "name": "Danny Ki",
      "url": "https://kish191919.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kish191919.github.io/2025/08/26/AWS-Certified-AI-Practitioner-30/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="UfZT9jGVnwcVj8z_sCj9LyqnWho6ubHg1Q7o_bvXfKs"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=GTM-N66KNRNW"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'GTM-N66KNRNW')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'GTM-N66KNRNW', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AWS Certified AI Practitioner(30) - Hyperparameter Tuning',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "url": "https://kish191919.github.io/",
  "name": "Danny's Blog",
  "inLanguage": "en"
}</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Danny's Blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/my_pic.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">102</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">9</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/showcase/"><i class="fa-fw fas fa-star"></i><span> Showcase</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fas fa-id-card"></i><span> Resume</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Danny's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">AWS Certified AI Practitioner(30) - Hyperparameter Tuning</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/showcase/"><i class="fa-fw fas fa-star"></i><span> Showcase</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fas fa-id-card"></i><span> Resume</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">AWS Certified AI Practitioner(30) - Hyperparameter Tuning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-08-26T19:20:21.000Z" title="Created 2025-08-26 15:20:21">2025-08-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-10T17:41:47.673Z" title="Updated 2025-11-10 12:41:47">2025-11-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/CERTIFICATION/">CERTIFICATION</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/CERTIFICATION/AWS-AI-PRACTITIONER/">AWS_AI_PRACTITIONER</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="ë‚˜ì˜-ì†Œê°œ"><a href="#ë‚˜ì˜-ì†Œê°œ" class="headerlink" title="ë‚˜ì˜ ì†Œê°œ"></a>ë‚˜ì˜ ì†Œê°œ</h3><p>Thank you for having me for an interview and my name is Sunghwan ki but you can go by Danny<br>I work as Data Engineer with 6 years experience in building ETL process, especially in the financial industry.<br>currently I lead the projects that use the Kafka, Oracle, and Spark where I focus on near real-time data processing and optimization.</p>
<p>To better performance, I completed the Masterâ€™s degree in Data Science last year, which provided advanced knowledge in data architecture and big data processing.<br>Iâ€™m passionate about making complex data pipelines simple, efficient, and scalable so they can create real value for the business. Iâ€™m excited to bring this approach and my experience to your team.</p>
<h3 id="ì™œ-ì´ì§í•˜ë‹ˆ"><a href="#ì™œ-ì´ì§í•˜ë‹ˆ" class="headerlink" title="ì™œ ì´ì§í•˜ë‹ˆ?"></a>ì™œ ì´ì§í•˜ë‹ˆ?</h3><p>Iâ€™ve truly enjoyed my time at PNC and  Iâ€™ve spent over six years working on meaningful projects and improved my technical skills.  Now I feel lI ready for a new challenge that allows me to expand further.<br>Technology is evolving faster than ever, and I want to keep learning and developing new skills to stay at the forefront of that change.<br>for me Itâ€™s not about leaving something behind â€” itâ€™s about taking the next step toward work Iâ€™m truly passionate about.</p>
<h3 id="ì™œ-ì§€ì›í–ˆë‹ˆ-About-Accenture-federal-service"><a href="#ì™œ-ì§€ì›í–ˆë‹ˆ-About-Accenture-federal-service" class="headerlink" title="ì™œ ì§€ì›í–ˆë‹ˆ? About Accenture federal service"></a>ì™œ ì§€ì›í–ˆë‹ˆ? About Accenture federal service</h3><p>Iâ€™m especially interested in Accenture federal service because I want to use my data engineering skills to make a real impact on mission-critical federal projects.<br>Accenture federal service combines innovation in cloud, AI, and analytics with a purpose-driven culture that improves how government serves people.<br>I believe AFS is the right place to apply my technical skills toward meaningful public impact and keep growing alongside talented professionals who share that same mission.</p>
<h3 id="AFS-ì–´ëŠíŒŒíŠ¸ì—-ê´€ì‹¬ì´-ë§ë‹ˆ"><a href="#AFS-ì–´ëŠíŒŒíŠ¸ì—-ê´€ì‹¬ì´-ë§ë‹ˆ" class="headerlink" title="AFS ì–´ëŠíŒŒíŠ¸ì— ê´€ì‹¬ì´ ë§ë‹ˆ?"></a>AFS ì–´ëŠíŒŒíŠ¸ì— ê´€ì‹¬ì´ ë§ë‹ˆ?</h3><p>Iâ€™m particularly interested in Data &amp; AIâ€“driven transformation for federal agencies.<br>Accenture helps government organizations modernize their operations through data, analytics, and automation<br>and that really aligns with my experience in real-time data engineering.</p>
<p>Iâ€™m also very interested in ì—ì„¼ì² â€™s partnership with Palantir Technologies, where they are working together to build solutions like enterprise-to-edge data fusion and predictive supply chain orchestration. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜<br>With my experience in designing and optimizing large-scale data pipelines using Kafka and Spark, along with my background in data quality and governance, I believe I can make a meaningful contribution to missions like these.</p>
<h3 id="Recent-project-ìµœê·¼-í”„ë¡œì íŠ¸"><a href="#Recent-project-ìµœê·¼-í”„ë¡œì íŠ¸" class="headerlink" title="Recent project (ìµœê·¼ í”„ë¡œì íŠ¸)"></a>Recent project (ìµœê·¼ í”„ë¡œì íŠ¸)</h3><p>Currently, I am working on building a near real time pipeline that ingests kafka topic data into Oracle Exadata and then into Hadoop platform.<br>In the past, stakeholders had to rely on the previous dayâ€™s data to make decisions. But now with this new pipeline, data from Kafka is ingested into Hadoop in every 10 minutes and then visualized through Tableau dashboards.<br>This project significantly reduced data latency and helped business team to make faster decision.</p>
<h4 id="Kafka-based-streaming-data-pipeline-ê³ ë ¤í•œ-ë¶€ë¶„"><a href="#Kafka-based-streaming-data-pipeline-ê³ ë ¤í•œ-ë¶€ë¶„" class="headerlink" title="Kafka-based streaming data pipeline ê³ ë ¤í•œ ë¶€ë¶„."></a>Kafka-based streaming data pipeline ê³ ë ¤í•œ ë¶€ë¶„.</h4><p>When designing Kafka pipelines, I focus on a few key areas to ensure performance and reliability<br>First, I choose the right topic partitioning strategy based on data size and the level of parallel processing needed.<br>And then I make sure that Kafka connectors are properly configured with retry mechanisms in case of failures.<br>For monitoring, I built a script that uses a curl command to check the status of the all Kafka sink connectors every 10 minutes. If the one of the connectors is down or thereâ€™s an issue with the Kafka broker, the script automatically generates an incident, triggering an alert to my team. This setup helped us catch issues and significantly reduced downtime.</p>
<h3 id="Designing-ETL-pipelines-CA7ì™€-Glue-ì´ìš©"><a href="#Designing-ETL-pipelines-CA7ì™€-Glue-ì´ìš©" class="headerlink" title="Designing ETL pipelines - CA7ì™€ Glue ì´ìš©"></a>Designing ETL pipelines - CA7ì™€ Glue ì´ìš©</h3><p>Iâ€™ve designed and implemented both batch and near real-time ETL pipelines.<br>I built near real-time ETL workflows using Kafka, Oracle Exadata, spark and Hadoop platform ingesting Kafka streaming data every 10 minutes.<br>I used PySpark scripts for transformation, and CA7 mainframe for job orchestration.</p>
<p>Also, I have experiences with AWS Glue to build the ETL pipelines. In one of my project, I developed an ETL process leveraging AWS Glueâ€™s visual interface as well as PySpark scripts to transform and load data efficiently.<br>Glueâ€™s dynamic frame and job orchestration features useful in simplifying complex transformations, and I was able to design reusable, scalable ETL workflows with minimal code by using the Glue Studio interface.</p>
<p>On top of this, I worked with AWS EMR and Spark for data processing and aggregation pipelines.<br>My background in data pipeline architecture and real-time ingestion across systems like Kafka, Oracle and hadoop has helped me successfully implement end-to-end solutions.<br>These pipelines were designed with scalability and data quality validation in mind, and optimized for minimal latency and high reliability.</p>
<h4 id="ê²½í—˜-ETL-pipeline-ìµœì í™”"><a href="#ê²½í—˜-ETL-pipeline-ìµœì í™”" class="headerlink" title="ê²½í—˜ - ETL pipeline ìµœì í™”"></a>ê²½í—˜ - ETL pipeline ìµœì í™”</h4><p>I had a situation where our SLA required the data to be fully available by 6 AM, But one day, the amount of source data suddenly increased â€” almost three times more than usual. Because of that, our Spark job didnâ€™t finish until 8 AM.<br>So I increased the number of  partitions to allow more parallel processing.<br>I also checked our resource settings and made sure the job had enough memory and CPU by adjusting the scheduler pool and YARN resorce manager.</p>
<p>After these changes, the job completed before 6 AM the next day, and we were able to meet the SLA again.<br>This experience helped me understand how important it is to tune the Spark jobs and monitor them carefully, especially when data volume suddenly increase.</p>
<h3 id="AWS-Glue-ì‚¬ìš©ê²½í—˜-ETL-services"><a href="#AWS-Glue-ì‚¬ìš©ê²½í—˜-ETL-services" class="headerlink" title="AWS Glue ì‚¬ìš©ê²½í—˜ - ETL services"></a>AWS Glue ì‚¬ìš©ê²½í—˜ - ETL services</h3><p>I have experience building ETL workflows using AWS Glue.<br>In one of my project, I built an analytics pipeline to process and analyze mobile user log data.</p>
<p>Iâ€™ve used Glue to extract data from S3 and then transform and load into Redshift every 10 minutes<br>Also, Iâ€™ve utilized Glue Crawlers to automatically detect schema changes and keep the data catalog updated for querying in Athena as well.</p>
<h4 id="ê·¸ëŸ¬ë‚˜-í˜„ì¬-glueë¥¼-ì‚¬ìš©í•˜ì§€ëŠ”-ì•ŠëŠ”ë‹¤"><a href="#ê·¸ëŸ¬ë‚˜-í˜„ì¬-glueë¥¼-ì‚¬ìš©í•˜ì§€ëŠ”-ì•ŠëŠ”ë‹¤" class="headerlink" title="ê·¸ëŸ¬ë‚˜ í˜„ì¬ glueë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤."></a>ê·¸ëŸ¬ë‚˜ í˜„ì¬ glueë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.</h4><p>We are currently using CA7 mainframe along with PySpark scripts for our ETL processes mainly.<br>CA7 is a mainframe-based job scheduling and workflow automation tool. itâ€™s used to manage and schedule batch jobs, ensuring the tasks run in the right order and at the right time.</p>
<p>we have not changed this Orchestration tool Because Our data workflows have been integrated into a mainframe-based CA7 scheduling system for a long time.<br>Also it introduces additional operational costs. Lastly Our team has  managed and monitored ETL workflows within CA7 system.</p>
<h3 id="Data-Orchestration-CA7-Glue-Airflow"><a href="#Data-Orchestration-CA7-Glue-Airflow" class="headerlink" title="Data Orchestration (CA7, Glue, Airflow)"></a>Data Orchestration (CA7, Glue, Airflow)</h3><p>For data orchestration, Iâ€™ve worked with schedulers like CA7 in mainframe environments to automate PySpark-based ETL workflows.<br>CA7 is a mainframe-based job scheduling and workflow automation tool. Itâ€™s used to manage and schedule batch jobs, ensuring the tasks run in the right order and at the right time.<br>Iâ€™ve worked with CA7 to trigger and monitor ETL workflows.</p>
<p>In cloud projects, Iâ€™ve also used tools like AWS Glue to build scheduled and automated data pipelines.<br>Additionally, I have experience working with managed orchestration platforms like Apache Airflow for pipeline automation and monitoring.</p>
<h3 id="ë°ì´í„°-ì›¨ì–´í•˜ìš°ìŠ¤-ê¸°ìˆ -e-g-Amazon-Redshift-Snowflake-ì‚¬ìš©ê²½í—˜"><a href="#ë°ì´í„°-ì›¨ì–´í•˜ìš°ìŠ¤-ê¸°ìˆ -e-g-Amazon-Redshift-Snowflake-ì‚¬ìš©ê²½í—˜" class="headerlink" title="ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ê¸°ìˆ  (e.g. Amazon Redshift, Snowflake ì‚¬ìš©ê²½í—˜)"></a>ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ê¸°ìˆ  (e.g. Amazon Redshift, Snowflake ì‚¬ìš©ê²½í—˜)</h3><p>I have experience using Redshift to build the cloud data warehousing.<br>In one of my project, I built an analytics pipeline to process and analyze mobile user log data.</p>
<p>To achieve this, I designed a workflow where the logs data were first ingested into Amazon S3, and then processed using AWS Glue, which loaded the data into Redshift.<br>Once the data was in Redshift, I used Amazon QuickSight to build interactive dashboards that visualized key user activity such as session duration, clickstream patterns, and device usage.<br>This solution provided business stakeholders with actionable insights.</p>
<p>In terms of Snowflake, I used it as a cloud data warehouse to support BI-driven insights. I connected Snowflake to ingest the S3 data by using snowpipe and then connected to Tableau to build interactive dashboards that visualized key metrics.</p>
<h4 id="Redshift-ë€"><a href="#Redshift-ë€" class="headerlink" title="Redshift ë€?"></a>Redshift ë€?</h4><p>Redshift is designed for high-speed querying using massively parallel processing (MPP). This makes it great for analyzing large datasets quickly.<br>we can start small and scale up by increasing the node size or number of nodes as the data grows.<br>Data is stored in columnar format, which speeds up analytical queries and reduces I&#x2F;O.</p>
<h4 id="Redshift-Columnar-Storage"><a href="#Redshift-Columnar-Storage" class="headerlink" title="Redshift Columnar Storage"></a>Redshift Columnar Storage</h4><p>When we execute queries like SUM(), AVG(), or filtering on specific columns, the database only needs to read the relevant columns, not entire rows.<br>This speeds up reading performance in data warehousing and analytics.<br>Since each column typically contains similar types of data, it compresses more efficiently than row-based data.<br>Also, it only reads the selected columns, the amount of data scanned is reduced.</p>
<h4 id="ìŠ¤ë…¸ìš°í”Œë ˆì´í¬-ì¥ì -zero-copy-cloning-and-time-travle"><a href="#ìŠ¤ë…¸ìš°í”Œë ˆì´í¬-ì¥ì -zero-copy-cloning-and-time-travle" class="headerlink" title="ìŠ¤ë…¸ìš°í”Œë ˆì´í¬ ì¥ì  (zero-copy cloning and time travle)"></a>ìŠ¤ë…¸ìš°í”Œë ˆì´í¬ ì¥ì  (zero-copy cloning and time travle)</h4><p>I can instantly clone entire databases or tables without duplicating data and it saves cost and time.<br>There is a Time Travel function and it lets us query or restore data from a previous point in time. it is useful for recovering the data.</p>
<h3 id="Databricks-ì‚¬ìš©ê²½í—˜"><a href="#Databricks-ì‚¬ìš©ê²½í—˜" class="headerlink" title="Databricks ì‚¬ìš©ê²½í—˜"></a>Databricks ì‚¬ìš©ê²½í—˜</h3><p>On the Databricks side, I primarily work with the Azure-hosted version of Databricks.<br>Recently, I developed an end-to-end scalable pipeline for computer vision anomaly detection.<br>As you can see my portfolio website. You can see its notebook and model. I use the PyTorch and Hugging Face to train and build the model.</p>
<h3 id="ë°ì´í„°-í’ˆì§ˆê´€ë¦¬"><a href="#ë°ì´í„°-í’ˆì§ˆê´€ë¦¬" class="headerlink" title="ë°ì´í„° í’ˆì§ˆê´€ë¦¬"></a>ë°ì´í„° í’ˆì§ˆê´€ë¦¬</h3><p>Regarding data quality, I implement validation at multiple points in the pipeline. This includes schema validation during data ingestion, null or error checks after transformations, and enforcement of business rule before loading data into target systems. At PNC, Iâ€™ve helped establish data quality checkpoints to ensure consistency across Oracle Exadata, Hadoop, and analytics systems. I also monitor pipeline health through logging.</p>
<h4 id="ê²½í—˜-ë°ì´í„°-í’ˆì§ˆê´€ë¦¬"><a href="#ê²½í—˜-ë°ì´í„°-í’ˆì§ˆê´€ë¦¬" class="headerlink" title="ê²½í—˜ - ë°ì´í„° í’ˆì§ˆê´€ë¦¬"></a>ê²½í—˜ - ë°ì´í„° í’ˆì§ˆê´€ë¦¬</h4><p>In my projects, I have focused a lot on ensuring data quality and governance.<br>I established validation procedures using PySpark. For example, I created validation rules to check for duplicates, missing values, and schema mismatches before loading data into Hadoop. This reduced errors and improved the data quality.</p>
<p>In addition, I worked closely with business team to define what â€œgood dataâ€ means for their use cases.  And we ensured that the reporting dashboards in Tableau reflected accurate and meaningful information for decision-making.</p>
<p>In one project, I worked with the fraud prevention team, where my role was to deliver data they could fully trust. For them, â€œgood dataâ€ meant accurate, up-to-date, and reliable information without duplication or errors. Because the quality of data directly impacted their fraud detection models, I focused not only on data delivery but also on maintaining high quality through continuous validation and monitoring.</p>
<h4 id="ê²½í—˜-ìœ ë‹›í…ŒìŠ¤-ê´€ë ¨í•´ì„œ"><a href="#ê²½í—˜-ìœ ë‹›í…ŒìŠ¤-ê´€ë ¨í•´ì„œ" class="headerlink" title="ê²½í—˜ - ìœ ë‹›í…ŒìŠ¤ ê´€ë ¨í•´ì„œ"></a>ê²½í—˜ - ìœ ë‹›í…ŒìŠ¤ ê´€ë ¨í•´ì„œ</h4><p>I usually use Pytest for unit testing in Python. Itâ€™s simpler and more readable than the built-in unittest module,<br>and it allows to write tests quickly without creating test classes. In Pytest, test functions simply start with test_,<br>and I use the assert statement to verify the results.</p>
<h3 id="ë³´ì•ˆê³¼-ê·œì •ê´€ë ¨-in-cloud-environments-e-g-AWS-Azure"><a href="#ë³´ì•ˆê³¼-ê·œì •ê´€ë ¨-in-cloud-environments-e-g-AWS-Azure" class="headerlink" title="ë³´ì•ˆê³¼ ê·œì •ê´€ë ¨ in cloud environments (e.g. AWS, Azure)"></a>ë³´ì•ˆê³¼ ê·œì •ê´€ë ¨ in cloud environments (e.g. AWS, Azure)</h3><p>In my current role, we have a dedicated security and compliance team that handles overall data governance.<br>So if I need access to certain sensitive databases or tables, I first have to get approval from that team.<br>This helps make sure only the right people can have an access to the data.</p>
<p>On the data engineering side, I am responsible for protecting sensitive data during our ETL processes.<br>That means identifying PII data and masking them, so even if someone sees the data, itâ€™s not readable.</p>
<p>Actually, We strictly follow the principle of least privilege. We assign only the minimum required permissions.</p>
<h3 id="í–‰ë™-ë©´ì ‘-ì§ˆë¬¸-Behavioral-Team-fit"><a href="#í–‰ë™-ë©´ì ‘-ì§ˆë¬¸-Behavioral-Team-fit" class="headerlink" title="í–‰ë™ ë©´ì ‘ ì§ˆë¬¸ (Behavioral &#x2F; Team fit)"></a>í–‰ë™ ë©´ì ‘ ì§ˆë¬¸ (Behavioral &#x2F; Team fit)</h3><h4 id="íŒ€ì›ë“¤ê³¼-ì˜ê²¬ì´-ë§ì§€-ì•Šì„ë•Œ-Disagreement-within-a-team"><a href="#íŒ€ì›ë“¤ê³¼-ì˜ê²¬ì´-ë§ì§€-ì•Šì„ë•Œ-Disagreement-within-a-team" class="headerlink" title="íŒ€ì›ë“¤ê³¼ ì˜ê²¬ì´ ë§ì§€ ì•Šì„ë•Œ (Disagreement within a team)"></a>íŒ€ì›ë“¤ê³¼ ì˜ê²¬ì´ ë§ì§€ ì•Šì„ë•Œ (Disagreement within a team)</h4><p>Yes, Iâ€™ve experienced disagreements within the team. One example was during a near real-time data pipeline project.<br>We were loading Kafka data into Hadoop, and the pipeline often missed our 10-minute SLA â€” sometimes it took over 20 minutes. The team had different opinions: some wanted to focus on improving Spark performance, while others, including me, thought the main issue was data quality because of inconsistent records and schema mismatches.</p>
<p>To find the real cause, I checked the logs and monitoring reports and found that about 70% of the delays were due to data validation errors, not Spark speed.<br>Based on that insight, I proposed a short proof-of-concept: implement stronger schema validation and fallback rules for one production cycle. After implementing it, the number of failures dropped significantly and we regained our SLA.<br>Once the team saw the data and results, everyone agreed to proceed with quality improvements first, then revisit performance tuning.</p>
<p>This experience reinforced for me that using measurable data, clear communication, and structured tests is much more effective than letting opinions dominate technical decisions.</p>
<h4 id="ìƒì‚°ì—-ë¬¸ì œê°€-ë°œìƒí–ˆì„ë•Œ-ëŒ€ì‘ë°©ì‹-Respond-when-an-urgent-production-issue-occurred"><a href="#ìƒì‚°ì—-ë¬¸ì œê°€-ë°œìƒí–ˆì„ë•Œ-ëŒ€ì‘ë°©ì‹-Respond-when-an-urgent-production-issue-occurred" class="headerlink" title="ìƒì‚°ì— ë¬¸ì œê°€ ë°œìƒí–ˆì„ë•Œ ëŒ€ì‘ë°©ì‹ (Respond when an urgent production issue occurred)"></a>ìƒì‚°ì— ë¬¸ì œê°€ ë°œìƒí–ˆì„ë•Œ ëŒ€ì‘ë°©ì‹ (Respond when an urgent production issue occurred)</h4><p>When a production issue happens, I stay calm and focus on finding the root cause quickly.<br>For example, one night our Kafka-to-Hadoop pipeline failed, and the business dashboards in Tableau were missing data the next morning. I immediately worked with another engineer to check the Kafka Connect logs and found that the sink connector had stopped due to a network timeout.</p>
<p>We manually restarted the connector and confirmed that the data started flowing again. Afterward, I created a monitoring script using curl that checks the connector status every 10 minutes. If it fails, the script automatically creates an incident and sends an alert to our team.<br>This experience taught me the importance of not only fixing issues quickly but also building automation to prevent the same problem from happening again.</p>
<h4 id="í”„ë¡œì íŠ¸ê°€-ì§€ì—°ë˜ì—ˆì„ë•Œ-ëŒ€ì‘ë°©ì‹-Faced-a-situation-where-a-project-delay-caused-customer-dissatisfaction"><a href="#í”„ë¡œì íŠ¸ê°€-ì§€ì—°ë˜ì—ˆì„ë•Œ-ëŒ€ì‘ë°©ì‹-Faced-a-situation-where-a-project-delay-caused-customer-dissatisfaction" class="headerlink" title="í”„ë¡œì íŠ¸ê°€ ì§€ì—°ë˜ì—ˆì„ë•Œ ëŒ€ì‘ë°©ì‹ (Faced a situation where a project delay caused customer dissatisfaction)"></a>í”„ë¡œì íŠ¸ê°€ ì§€ì—°ë˜ì—ˆì„ë•Œ ëŒ€ì‘ë°©ì‹ (Faced a situation where a project delay caused customer dissatisfaction)</h4><p>Yes, Iâ€™ve experienced that before.<br>In one project, our data ingestion pipeline was delayed because the upstream system changed its schema without notice.<br>This caused our ETL jobs to fail and delayed daily reports for the client.</p>
<p>As soon as the client raised concerns, I explained the issue clearly, shared the revised delivery plan, and sent daily updates so they could see our progress.<br>Meanwhile, I worked with my team to add automatic schema validation and fallback logic in the pipeline, so future schema changes wouldnâ€™t break the process again.</p>
<p>After we implemented the fix, the pipeline became more stable, and the client appreciated our quick communication and the long-term solution we put in place.</p>
<h4 id="íŒ€-ë¦¬ë“œ-ì†”ì„ ìˆ˜ë²”-lead-a-team-or-take-initiative"><a href="#íŒ€-ë¦¬ë“œ-ì†”ì„ ìˆ˜ë²”-lead-a-team-or-take-initiative" class="headerlink" title="íŒ€ ë¦¬ë“œ &amp; ì†”ì„ ìˆ˜ë²” (lead a team or take initiative)"></a>íŒ€ ë¦¬ë“œ &amp; ì†”ì„ ìˆ˜ë²” (lead a team or take initiative)</h4><p>In one project, we had a very tight deadline to deliver a new ETL workflow for daily reporting. However, a few tasks were delayed because some external data sources were not delivered on schedule, and that caused downstream jobs in Spark to fail during testing.<br>To get things back on track, I took the initiative to organize short daily stand-up meetings and created a shared progress tracker in Confluence so everyone â€” including the data and QA teams â€” could see real-time task status. This helped us identify blockers early, communicate dependencies clearly, and reassign tasks based on team availability.</p>
<p>Within a week, we recovered the lost time and successfully completed the workflow before the deadline. The reporting system went live as planned, and we avoided last-minute production issues.<br>That experience taught me that strong coordination and clear communication are just as important as technical skills when leading a project under tight timelines.</p>
<h4 id="íƒ€ì´íŠ¸í•œ-ìŠ¤ì¼€ì¤„-ë†’ì€-ì••ë°•-ìƒí™˜-Handle-tight-deadlines-or-high-pressure-situations"><a href="#íƒ€ì´íŠ¸í•œ-ìŠ¤ì¼€ì¤„-ë†’ì€-ì••ë°•-ìƒí™˜-Handle-tight-deadlines-or-high-pressure-situations" class="headerlink" title="íƒ€ì´íŠ¸í•œ ìŠ¤ì¼€ì¤„ &amp; ë†’ì€ ì••ë°• ìƒí™˜ (Handle tight deadlines or high-pressure situations)"></a>íƒ€ì´íŠ¸í•œ ìŠ¤ì¼€ì¤„ &amp; ë†’ì€ ì••ë°• ìƒí™˜ (Handle tight deadlines or high-pressure situations)</h4><p>When I face tight deadlines or high-pressure situations, I stay calm and focus on breaking the work into smaller, manageable parts.<br>For example, in one project, our team had to build a new ETL workflow in less than two weeks due to a last-minute client request. Instead of panicking, I quickly prioritized tasks, delegated responsibilities, and set up short daily check-ins to track progress.</p>
<p>I also made sure to communicate clearly with both the team and stakeholders about what could realistically be delivered. By keeping everyone aligned and focusing on one milestone at a time, we finished the project on schedule and maintained high quality.<br>This experience taught me that under pressure, clear priorities, steady communication, and teamwork make all the difference.</p>
<h4 id="ì‹¤íŒ¨-ë˜ëŠ”-ì‹¤ìˆ˜í•œ-ê²½ìš°-Failed-or-made-a-mistake"><a href="#ì‹¤íŒ¨-ë˜ëŠ”-ì‹¤ìˆ˜í•œ-ê²½ìš°-Failed-or-made-a-mistake" class="headerlink" title="ì‹¤íŒ¨ ë˜ëŠ” ì‹¤ìˆ˜í•œ ê²½ìš° (Failed or made a mistake)"></a>ì‹¤íŒ¨ ë˜ëŠ” ì‹¤ìˆ˜í•œ ê²½ìš° (Failed or made a mistake)</h4><p>Yes, I made a mistake once during a data validation process.<br>I was in charge of checking the output of a new ETL job before it went live. I verified the total record count but forgot to double-check the column-level transformations.<br>After deployment, we found that one column had an incorrect currency conversion rate, and it caused wrong numbers to show up in a few business reports.</p>
<p>As soon as I realized the issue, I corrected the transformation logic, reprocessed the data, and updated the reports. After that, I added column-level validation rules and a simple Python script that automatically compares key fields between the source and target tables before every deployment.<br>That experience taught me how even a small mistake can affect business reports, so now I always check both the data structure and actual values carefully before sign-off.</p>
<h4 id="ì—¬ëŸ¬-ë¯¼ì¡±ê³¼-ê°™ì´-ê·¼ë¬´í•œ-ê²½ìš°-team-with-diverse-personalities-or-backgrounds"><a href="#ì—¬ëŸ¬-ë¯¼ì¡±ê³¼-ê°™ì´-ê·¼ë¬´í•œ-ê²½ìš°-team-with-diverse-personalities-or-backgrounds" class="headerlink" title="ì—¬ëŸ¬ ë¯¼ì¡±ê³¼ ê°™ì´ ê·¼ë¬´í•œ ê²½ìš° (team with diverse personalities or backgrounds)"></a>ì—¬ëŸ¬ ë¯¼ì¡±ê³¼ ê°™ì´ ê·¼ë¬´í•œ ê²½ìš° (team with diverse personalities or backgrounds)</h4><p>Currently, I work on a project with team members from the U.S., India, and Europe. Each person had a different communication style and approach to problem-solving, which made coordination a bit challenging at first.<br>To improve collaboration, I organized short daily sync meetings and encouraged open discussions so everyone could share their progress or blockers. I also adjusted my communication style â€” for example, sending written summaries after meetings to help teammates in different time zones stay aligned.</p>
<p>As a result, our workflow became much smoother, and we completed the project two weeks ahead of schedule. This experience taught me how to adapt to different working styles and turn diversity into a real strength for achieving better team outcomes.</p>
<h3 id="Partitioning-strategy-Spark-Redshift-Snowflake"><a href="#Partitioning-strategy-Spark-Redshift-Snowflake" class="headerlink" title="Partitioning strategy (Spark, Redshift, Snowflake)"></a>Partitioning strategy (Spark, Redshift, Snowflake)</h3><p>Partitioning strategy depends on query patterns and data volume.<br>Regarding the Oracle Exadata, I Used range partitioning by date column to support daily ingestion and quickly purge old data by simply dropping partition.</p>
<p>In Spark, I used dynamic partition overwrite with partitionBy(â€œdateâ€) when writing Parquet files, and adjusted the number of partitions with coalesce or repartition commands to avoid creating too many small files.</p>
<p>In Redshift, I defined DISTKEY and SORTKEY based on the columns that were most frequently used in joins and filters, which helped improve query performance and reduce data movement across nodes.</p>
<p>In Snowflake, I rely on its automatic micro-partitioning feature, which breaks data into 16MB blocks and optimizes storage and query performance without any manual intervention.<br>However, for very large tables where queries frequently filter on specific columnsâ€”such as date, I define a cluster key to further improve performance.</p>
<p>These approach improved query speed and simplified lifecycle management (e.g., dropping old partitions instead of delete operations)</p>
<h4 id="ê²½í—˜-ë°ì´í„°-íŒŒí‹°ì…˜"><a href="#ê²½í—˜-ë°ì´í„°-íŒŒí‹°ì…˜" class="headerlink" title="ê²½í—˜ - ë°ì´í„° íŒŒí‹°ì…˜"></a>ê²½í—˜ - ë°ì´í„° íŒŒí‹°ì…˜</h4><p>I have strong experience in data modeling and architecture, especially in designing data pipelines at PNC<br>In Oracle Exadata, I designed the tables using range partitioning by day, so that Kafka data was automatically separated into daily partitions. This made it much easier to manage large volumes of data, speed up queries, and improve overall performance. For example, instead of using a traditional delete command, we could simply drop an entire partition when the data was no longer needed.  And This is not only optimized storage space but also kept query performance fast and efficient, since queries only scanned the relevant partitions rather than the entire table.</p>
<h3 id="ë°ì´í„°-ì •ê·œí™”ì™€-ë¹„ì •ê·œí™”ì˜-ì°¨ì´ì "><a href="#ë°ì´í„°-ì •ê·œí™”ì™€-ë¹„ì •ê·œí™”ì˜-ì°¨ì´ì " class="headerlink" title="ë°ì´í„° ì •ê·œí™”ì™€ ë¹„ì •ê·œí™”ì˜ ì°¨ì´ì "></a>ë°ì´í„° ì •ê·œí™”ì™€ ë¹„ì •ê·œí™”ì˜ ì°¨ì´ì </h3><p>Normalized data is typically used in OLTP systems. It separates data into multiple related tables to reduce redundancy and maintain data integrity.<br>This helps ensure consistency during frequent insert, update, and delete operations, but often requires multiple joins to retrieve data.</p>
<p>Denormalized data is more common in OLAP systems. It intentionally duplicates data by combining related fields into fewer tables, which improves read performance and speeds up complex analytical queries.</p>
<h4 id="ìŠ¤íƒ€ìŠ¤í‚¤ë§ˆ-ìŠ¤ë…¸ìš°í”Œë ˆì´í¬-ìŠ¤í‚¤ë§ˆ-ëŒ€ë¶€ë¶€ì˜-ë°ì´í„°-ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œëŠ”-ì‚¬ìš©í•˜ëŠ”-ìŠ¤í‚¤ë§ˆ"><a href="#ìŠ¤íƒ€ìŠ¤í‚¤ë§ˆ-ìŠ¤ë…¸ìš°í”Œë ˆì´í¬-ìŠ¤í‚¤ë§ˆ-ëŒ€ë¶€ë¶€ì˜-ë°ì´í„°-ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œëŠ”-ì‚¬ìš©í•˜ëŠ”-ìŠ¤í‚¤ë§ˆ" class="headerlink" title="ìŠ¤íƒ€ìŠ¤í‚¤ë§ˆ &amp; ìŠ¤ë…¸ìš°í”Œë ˆì´í¬ ìŠ¤í‚¤ë§ˆ (ëŒ€ë¶€ë¶€ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œëŠ” ì‚¬ìš©í•˜ëŠ” ìŠ¤í‚¤ë§ˆ)"></a>ìŠ¤íƒ€ìŠ¤í‚¤ë§ˆ &amp; ìŠ¤ë…¸ìš°í”Œë ˆì´í¬ ìŠ¤í‚¤ë§ˆ (ëŒ€ë¶€ë¶€ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œëŠ” ì‚¬ìš©í•˜ëŠ” ìŠ¤í‚¤ë§ˆ)</h4><p>In most data warehouses, the Star Schema is used because it provides high query performance, especially for analytical workloads, and has a simple structure consisting of a central fact table connected to denormalized dimension tables.<br>This simplicity also makes it well suited for BI and reporting tools like Tableau or Power BI.</p>
<p>But the Snowflake Schema is also usedâ€”especially when storage efficiency or data normalization is a higher priority.<br>it tends to introduce more joins and complexity, which can affect query performance.</p>
<p>Therefore, Star Schema is generally preferred in data warehouse environments.</p>
<h3 id="Python-SQL-Spark-and-PySpark"><a href="#Python-SQL-Spark-and-PySpark" class="headerlink" title="Python, SQL, Spark, and PySpark"></a>Python, SQL, Spark, and PySpark</h3><p>Currently I am working with Python, SQL, Spark, and PySpark throughout my career as a data engineer.<br>Python has been my primary programming language for building ETL pipelines. Iâ€™ve used it in both production and QA environments, including developing data ingestion frameworks.</p>
<p>SQL is a core part of my daily workflow. Iâ€™ve written complex analytical queries and optimized SQL for performance on databases like Oracle Exadata.</p>
<p>With Spark, Iâ€™ve built scalable data processing pipelines for both batch and near real-time use cases. Iâ€™ve used Spark in distributed environments, primarily through PySpark, to perform transformations, aggregations, and joins on large datasets.</p>
<h4 id="ì´ìŠˆ-Spark-memory-ë¬¸ì œ"><a href="#ì´ìŠˆ-Spark-memory-ë¬¸ì œ" class="headerlink" title="ì´ìŠˆ - Spark memory ë¬¸ì œ"></a>ì´ìŠˆ - Spark memory ë¬¸ì œ</h4><p>One of the most common issues I encounter is out-of-memory (OOM) errors.<br>To address this, First, I review the PySpark code to identify any operational command like collect() or toPandas() that might be pulling too much data into the driver. If I find them, I either remove or replace them.</p>
<p>I also use broadcast joins when dealing with small tables to minimize shuffle operations, it can reduce memory usage.<br>Another important step is avoiding Python UDFs if it is possible to use native Spark SQL functions.<br>Additionally, when I need to reuse the intermediate results, I use caching it and also I use MEMORY_AND_DISK storage option to avoid overwhelming the memory.</p>
<p>Finally, I adjust partition sizes using coalesce() or repartition() to optimize resource usage during shuffle operations.<br>By applying these techniques, Iâ€™ve been able to effectively prevent and troubleshoot memory-related issues in Spark jobs.</p>
<h4 id="ì´ìŠˆ-ìŠ¤í‚¤ë§ˆ-ë³€ê²½"><a href="#ì´ìŠˆ-ìŠ¤í‚¤ë§ˆ-ë³€ê²½" class="headerlink" title="ì´ìŠˆ - ìŠ¤í‚¤ë§ˆ ë³€ê²½"></a>ì´ìŠˆ - ìŠ¤í‚¤ë§ˆ ë³€ê²½</h4><p>I remember there was a project that we were integrating data from multiple sources into a central data warehouse. The challenge was that one of the upstream systems frequently changed its schema without notice. And it caused our ETL jobs to fail and delayed reporting for business users.</p>
<p>My responsibility was to make the pipeline more resilient so that these schema changes would not break the entire data flow.<br>I implemented a schema validation and auto-adjustment process. Using PySpark, I built a job that compared incoming data schemas against our expected schema. If a non-critical column changed such as a new column being added, the pipeline could adapt automatically without failing.</p>
<p>For critical mismatches, the system flagged the issue, generated incident, and provided fallback logic to continue processing the  data.<br>This reduced ETL job failures by more than 90% and ensured that the business team continued to receive the data even when upstream systems changed its schema unexpectedly.</p>
<h4 id="ì´ìŠˆ-ë°ì´í„°-ë³¼ë¥¨ì¦ê°€"><a href="#ì´ìŠˆ-ë°ì´í„°-ë³¼ë¥¨ì¦ê°€" class="headerlink" title="ì´ìŠˆ - ë°ì´í„° ë³¼ë¥¨ì¦ê°€"></a>ì´ìŠˆ - ë°ì´í„° ë³¼ë¥¨ì¦ê°€</h4><p>One of the biggest challenges I faced recently was with a Kafka-to-Hadoop data pipeline, where Oracle Exadata was used as a staging area. Initially, the volume of data coming from Kafka was about 1 TB per day, but it suddenly increased to 3 or 4 TB per day<br>Even though the data was automatically deleted after being loaded into Hadoop, new data was coming in faster than it could be deleted, so Exadata started running out of space.</p>
<p>To handle this, I increased the number of Spark jobs to speed up data movement into Hadoop. But this caused to slow down the Exadata and it created a bottleneck issue.</p>
<p>Then I suddenly thought about compressing the data, and fortunately, I discovered that EXADATA has a built-in compression feature â€” and the best part is that the data doesnâ€™t need to be decompressed when itâ€™s moved to Hadoop.</p>
<p>Using this compression method, I was able to reduce the data size by almost 70%.  After that, I reduced the number of Spark jobs, which helped Exadata run better and stabilized the pipeline.<br>That experience really taught me how to stay calm under pressure, look at the problem from a different angle, and find a solution that improves both performance and efficiency.</p>
<h3 id="ê²½í—˜-ìŠ¤íŒŒí¬-í•˜ë‘¡-ë°ì´í„°-ingestion"><a href="#ê²½í—˜-ìŠ¤íŒŒí¬-í•˜ë‘¡-ë°ì´í„°-ingestion" class="headerlink" title="ê²½í—˜ - ìŠ¤íŒŒí¬&#x2F;í•˜ë‘¡ ë°ì´í„° ingestion"></a>ê²½í—˜ - ìŠ¤íŒŒí¬&#x2F;í•˜ë‘¡ ë°ì´í„° ingestion</h3><p>On the Hadoop and Spark side, I designed frameworks to handle large-scale data ingestion and transformation. For example, data coming from Oracle first needed to be cleaned before it could be used for reporting. I built PySpark jobs that automatically parse the data and removed duplicate records, handled missing values, and converted the data into optimized formats like Parquet. At the same time, I added metadata and validation rules so that we could easily track the data and confirm its accuracy.</p>
<h3 id="ê²½í—˜-AWS-ë§ì´-ì‚¬ìš©í–ˆë‹ˆ"><a href="#ê²½í—˜-AWS-ë§ì´-ì‚¬ìš©í–ˆë‹ˆ" class="headerlink" title="ê²½í—˜ - AWS ë§ì´ ì‚¬ìš©í–ˆë‹ˆ?"></a>ê²½í—˜ - AWS ë§ì´ ì‚¬ìš©í–ˆë‹ˆ?</h3><p>If you take a look at my portfolio website, youâ€™ll see that most of my projects are built using AWS<br>I actively use AWS to quickly build and experiment with different data architectures.</p>
<p>Since data tools are evolving so fast, I use EMR service to easily install and try out big data tools like Spark, Hadoop, and Kafka. For storing data, I normally use RDS or S3<br>Overall, AWS has been a great platform for me to learn, experiment, and build end-to-end data pipelines.</p>
<h3 id="ì¥ì ê³¼-ë‹¨ì "><a href="#ì¥ì ê³¼-ë‹¨ì " class="headerlink" title="ì¥ì ê³¼ ë‹¨ì "></a>ì¥ì ê³¼ ë‹¨ì </h3><p>My biggest strengths are my flexibility and adaptability. Wherever I work, work environments change daily and throughout the day. And there are certain projects that require individual attention and others that involve a teamwork approach. My flexibility and adaptability have allowed me to meet the expectations and even go beyond them.</p>
<p>Also, I get along with people around me. This kind of personality makes the work environment more comfortable and easier<br>As far as my weaknesses, I sometimes put in too much time on what I like to do. With my mentorâ€™s help, I started using a daily checklist to plan and prioritize my work. Now I make sure I pace myself better and focus on finishing the most important tasks first. Itâ€™s helped me become more balanced and efficient.</p>
<h3 id="ë§ˆì§€ë§‰ìœ¼ë¡œ-í•˜ê³ -ì‹¶ì€-ë§"><a href="#ë§ˆì§€ë§‰ìœ¼ë¡œ-í•˜ê³ -ì‹¶ì€-ë§" class="headerlink" title="ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ê³  ì‹¶ì€ ë§"></a>ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ê³  ì‹¶ì€ ë§</h3><p>may I ask which technologies your team works with most often, and what types of projects are currently the main focus?</p>
<h1 id="Hyperparameter-Tuning"><a href="#Hyperparameter-Tuning" class="headerlink" title="Hyperparameter Tuning"></a>Hyperparameter Tuning</h1><h2 id="1-What-is-a-Hyperparameter"><a href="#1-What-is-a-Hyperparameter" class="headerlink" title="1. What is a Hyperparameter?"></a>1. What is a Hyperparameter?</h2><ul>
<li><strong>Definition</strong>: Settings that define how the model is structured and how the learning algorithm works.</li>
<li><strong>Set before training begins</strong> (they are not learned from the data).</li>
<li><strong>Examples</strong>:<ul>
<li><strong>Learning rate</strong></li>
<li><strong>Batch size</strong></li>
<li><strong>Number of epochs</strong></li>
<li><strong>Regularization</strong></li>
</ul>
</li>
</ul>
<p>ğŸ‘‰ <strong>Exam Tip</strong>: Hyperparameters are <strong>not learned</strong> during training. They are chosen before training and tuned for best performance.</p>
<hr>
<h2 id="2-Why-Hyperparameter-Tuning-Matters"><a href="#2-Why-Hyperparameter-Tuning-Matters" class="headerlink" title="2. Why Hyperparameter Tuning Matters"></a>2. Why Hyperparameter Tuning Matters</h2><ul>
<li><strong>Goal</strong>: Find the best combination of hyperparameters to optimize model performance.</li>
<li><strong>Benefits</strong>:<ul>
<li>Improves accuracy</li>
<li>Reduces overfitting</li>
<li>Enhances generalization to new data</li>
</ul>
</li>
<li><strong>Methods</strong>:<ul>
<li><strong>Grid Search</strong>: Tries all possible parameter combinations.</li>
<li><strong>Random Search</strong>: Tests random parameter values.</li>
<li><strong>Automated Services</strong>:<ul>
<li><strong>Amazon SageMaker Automatic Model Tuning (AMT)</strong> runs multiple training jobs and finds the best settings.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-Key-Hyperparameters"><a href="#3-Key-Hyperparameters" class="headerlink" title="3. Key Hyperparameters"></a>3. Key Hyperparameters</h2><h3 id="1-Learning-Rate"><a href="#1-Learning-Rate" class="headerlink" title="(1) Learning Rate"></a>(1) Learning Rate</h3><ul>
<li>Controls <strong>how big the steps are</strong> when updating model weights.</li>
<li><strong>High learning rate</strong>: Faster training, but may overshoot the optimal solution.</li>
<li><strong>Low learning rate</strong>: More stable and precise, but much slower.</li>
</ul>
<hr>
<h3 id="2-Batch-Size"><a href="#2-Batch-Size" class="headerlink" title="(2) Batch Size"></a>(2) Batch Size</h3><ul>
<li>Number of training examples processed in one iteration.</li>
<li><strong>Small batches</strong>: More stable, but slower.</li>
<li><strong>Large batches</strong>: Faster, but may cause less stable updates.</li>
</ul>
<hr>
<h3 id="3-Number-of-Epochs"><a href="#3-Number-of-Epochs" class="headerlink" title="(3) Number of Epochs"></a>(3) Number of Epochs</h3><ul>
<li>How many times the model goes through the <strong>entire training dataset</strong>.</li>
<li><strong>Too few</strong>: Underfitting (model doesnâ€™t learn enough).</li>
<li><strong>Too many</strong>: Overfitting (model memorizes the data, performs poorly on new data).</li>
</ul>
<hr>
<h3 id="4-Regularization"><a href="#4-Regularization" class="headerlink" title="(4) Regularization"></a>(4) Regularization</h3><ul>
<li>Controls the <strong>balance between a simple and complex model</strong>.</li>
<li>More regularization â†’ less overfitting.</li>
</ul>
<p>ğŸ‘‰ <strong>Exam Tip</strong>: If asked how to reduce overfitting, <strong>increasing regularization</strong> is often the correct answer.</p>
<hr>
<h2 id="4-Overfitting"><a href="#4-Overfitting" class="headerlink" title="4. Overfitting"></a>4. Overfitting</h2><h3 id="What-is-it"><a href="#What-is-it" class="headerlink" title="What is it?"></a>What is it?</h3><ul>
<li>The model performs very well on training data but poorly on new, unseen data.</li>
</ul>
<h3 id="Causes"><a href="#Causes" class="headerlink" title="Causes"></a>Causes</h3><ul>
<li>Too little training data â†’ not representative.</li>
<li>Training for too many epochs.</li>
<li>Model too complex â†’ learns noise instead of patterns.</li>
</ul>
<h3 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h3><ul>
<li>Increase training data size (best option).</li>
<li>Use <strong>early stopping</strong> (stop training before overfitting).</li>
<li>Apply <strong>data augmentation</strong> (add diversity to training data).</li>
<li>Adjust hyperparameters (e.g., increase regularization, change batch size).</li>
</ul>
<p>ğŸ‘‰ <strong>Exam Tip</strong>: If the question is <strong>â€œbest way to prevent overfittingâ€</strong>, the answer is usually <strong>increase training data</strong>.</p>
<hr>
<h2 id="5-When-NOT-to-Use-Machine-Learning"><a href="#5-When-NOT-to-Use-Machine-Learning" class="headerlink" title="5. When NOT to Use Machine Learning"></a>5. When NOT to Use Machine Learning</h2><ul>
<li><strong>Example</strong>:<br>You have a deck of 10 cards (5 red, 3 blue, 2 yellow).<br>Q: What is the probability of drawing a blue card?<br>A: 3&#x2F;10 &#x3D; 0.3</li>
</ul>
   <p align="center">
  <img src="/images/aws_basic_143.png" width="80%">
</p> 

<ul>
<li><p>This is a <strong>deterministic problem</strong>:</p>
<ul>
<li>The exact answer can be computed mathematically.</li>
<li>Writing simple code is the best solution.</li>
</ul>
</li>
<li><p>If we used ML (supervised, unsupervised, or reinforcement learning), weâ€™d only get an <strong>approximation</strong>, not an exact result.</p>
</li>
</ul>
<p>ğŸ‘‰ <strong>Exam Tip</strong>:<br>Machine Learning is <strong>not appropriate</strong> for problems that have a <strong>clear, deterministic answer</strong>. It is designed for problems where patterns must be learned from data.</p>
<hr>
<h2 id="6-AWS-Specific-Notes-for-Exams"><a href="#6-AWS-Specific-Notes-for-Exams" class="headerlink" title="6. AWS-Specific Notes for Exams"></a>6. AWS-Specific Notes for Exams</h2><ul>
<li><strong>Amazon SageMaker Automatic Model Tuning (AMT)</strong>: Automates hyperparameter tuning by running multiple jobs in parallel.</li>
<li><strong>Common Exam Questions</strong>:<ul>
<li>How to fix overfitting â†’ Increase data &#x2F; regularization.</li>
<li>What hyperparameter affects convergence speed â†’ Learning rate.</li>
<li>Which AWS service automates tuning â†’ SageMaker AMT.</li>
<li>When NOT to use ML â†’ Deterministic problem with exact answers.</li>
</ul>
</li>
</ul>
<hr>
<p>âœ… <strong>Summary</strong> - <strong>Hyperparameters</strong> (learning rate, batch size, epochs regularization) must be tuned for best performance.</p>
<ul>
<li><strong>Tuning</strong> improves accuracy, reduces overfitting, and enhances generalization.</li>
<li><strong>Overfitting</strong> occurs when the model memorizes training data â†’ fix by more data, regularization, early stopping.</li>
<li><strong>ML is not appropriate</strong> for deterministic problems.</li>
<li>On AWS, <strong>SageMaker AMT</strong> is the go-to tool for automated hyperparameter tuning.</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://kish191919.github.io">Danny Ki</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://kish191919.github.io/2025/08/26/AWS-Certified-AI-Practitioner-30/">https://kish191919.github.io/2025/08/26/AWS-Certified-AI-Practitioner-30/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AWS/">AWS</a><a class="post-meta__tags" href="/tags/AWS-AI-PRACTITIONER/">AWS_AI_PRACTITIONER</a></div><div class="post-share"><div class="social-share" data-image="/img/my_pic.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/26/AWS-Certified-AI-Practitioner-31/" title="AWS Certified AI Practitioner(31) - AWS AI Managed Services"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">AWS Certified AI Practitioner(31) - AWS AI Managed Services</div></div><div class="info-2"><div class="info-item-1">AWS AI Managed ServicesWhy AWS AI Managed Services?AWS AI Managed Services provide pre-trained ML models designed for specific use cases, without requiring you to build or train models from scratch. Key Benefits: Responsiveness and Availability: Always accessible, deployed across multiple Availability Zones and AWS Regions. Redundancy and Reliability: Services remain available even if one AZ experiences downtime. Performance: Use of specialized CPUs and GPUs optimized for ML workloads â†’ cost ...</div></div></div></a><a class="pagination-related" href="/2025/08/26/KO-AWS-Certified-AI-Practitioner-30/" title="(í•œêµ­ì–´) AWS Certified AI Practitioner (30) - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">(í•œêµ­ì–´) AWS Certified AI Practitioner (30) - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹</div></div><div class="info-2"><div class="info-item-1">í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)1. í•˜ì´í¼íŒŒë¼ë¯¸í„°ë€? ì •ì˜: ëª¨ë¸ êµ¬ì¡°ì™€ í•™ìŠµ ë°©ì‹ì„ ê²°ì •í•˜ëŠ” ì„¤ì •ê°’ íŠ¹ì§•: í•™ìŠµì´ ì‹œì‘ë˜ê¸° ì „ì— ì •í•´ì§ ë°ì´í„° ìì²´ê°€ ì•„ë‹ˆë¼, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ ë°©ì‹ì— ì˜í–¥ì„ ì¤Œ   ëŒ€í‘œ ì˜ˆì‹œ: í•™ìŠµë¥ (Learning rate) ë°°ì¹˜ í¬ê¸°(Batch size) ì—í¬í¬ ìˆ˜(Number of epochs) ì •ê·œí™”(Regularization)    ğŸ‘‰ ì‹œí—˜ í¬ì¸íŠ¸:í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµë˜ëŠ” ê°’ì´ ì•„ë‹ˆë¼, ì‚¬ì „ì— ì„¤ì •í•˜ëŠ” ê°’ì´ë‹¤.  2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Hyperparameter Tuning) ëª©ì : ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì°¾ì•„ ëª¨ë¸ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”\ íš¨ê³¼: ì •í™•ë„ í–¥ìƒ ê³¼ì í•©(Overfitting) ê°ì†Œ ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”   ë°©ë²•: Grid Search: ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•© íƒìƒ‰ Random Search: ì„ì˜ì˜ ì¡°í•©ì„ íƒìƒ‰ ìë™í™” ì„œë¹„ìŠ¤: Amazon SageMaker Automatic Model ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/14/AWS-Certified-AI-Practitioner-1/" title="AWS Certified AI Practitioner(1) - IT &amp; AWS Basics"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-14</div><div class="info-item-2">AWS Certified AI Practitioner(1) - IT &amp; AWS Basics</div></div><div class="info-2"><div class="info-item-1">ğŸ“š IT &amp; AWS Basics Summary1. Basic IT Terms Network: A connection of cables, routers, and servers.   Router: A device that decides where to send data packets over the internet.   Switch: Sends a packet to the correct server or client within the network.         2. Five Key Characteristics of Cloud Computing On-demand self service â€“ Instantly get resources without human help.   Broad network access â€“ Access resources from different devices via the internet.   Multi-tenancy &amp; Resource p...</div></div></div></a><a class="pagination-related" href="/2025/08/19/AWS-Certified-AI-Practitioner-10/" title="AWS Certified AI Practitioner(10) - Agents"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-19</div><div class="info-item-2">AWS Certified AI Practitioner(10) - Agents</div></div><div class="info-2"><div class="info-item-1">ğŸ¤– Amazon Bedrock â€“ AgentsğŸ“Œ What Are Agents?Agents in Amazon Bedrock are advanced components that can think, plan, and act on multi-step tasks.Unlike regular models that only provide answers, agents can perform real actions such as:  Provisioning infrastructure   Deploying applications   Executing operations on systems   Interacting with APIs, databases, and knowledge bases          ğŸ”‘ Key Features of Bedrock Agents Multi-step task execution: Agents can follow a sequence of steps to complete...</div></div></div></a><a class="pagination-related" href="/2025/08/19/AWS-Certified-AI-Practitioner-11/" title="AWS Certified AI Practitioner(11) - Agents"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-19</div><div class="info-item-2">AWS Certified AI Practitioner(11) - Agents</div></div><div class="info-2"><div class="info-item-1">ğŸ“Š Amazon Bedrock &amp; CloudWatchğŸ“Œ What is CloudWatch?Amazon CloudWatch is a monitoring service for AWS resources and applications.It provides:  Logs â€“ Detailed records of events and invocations   Metrics â€“ Numerical measurements of system performance   Alarms â€“ Notifications when thresholds are crossed   Dashboards â€“ Visualizations for monitoring   ğŸ”‘ Bedrock &amp; CloudWatch Integration1. Model Invocation Logging Logs all inputs and outputs from Bedrock model invocations.   Data can inclu...</div></div></div></a><a class="pagination-related" href="/2025/08/20/AWS-Certified-AI-Practitioner-12/" title="AWS Certified AI Practitioner(12) - Pricing &amp; Model Improvement"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-20</div><div class="info-item-2">AWS Certified AI Practitioner(12) - Pricing &amp; Model Improvement</div></div><div class="info-2"><div class="info-item-1">ğŸ“˜ Amazon Bedrock â€“ Pricing &amp; Model Improvement1ï¸âƒ£ Pricing OptionsğŸ”¹ On-Demand (Pay-as-you-go) How it works: Pay only for what you use, like an electricity bill.   Pricing basis   Text Models â†’ Input&#x2F;Output token count   Embedding Models â†’ Input token count   Image Models â†’ Number of images generated   Available Models: Base Models only   âœ… Pros: Flexible, good for unpredictable workloads   âŒ Cons: Can become expensive if used continuously over time   ğŸ”¹ Batch Mode (Bulk processing, ...</div></div></div></a><a class="pagination-related" href="/2025/08/20/AWS-Certified-AI-Practitioner-13/" title="AWS Certified AI Practitioner(13) - End-to-End Use Case (AI Stylist Demo)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-20</div><div class="info-item-2">AWS Certified AI Practitioner(13) - End-to-End Use Case (AI Stylist Demo)</div></div><div class="info-2"><div class="info-item-1">ğŸ‘— Amazon Bedrock End-to-End Use Case (AI Stylist Demo)ğŸ“Œ Why This Demo MattersSo far, weâ€™ve explored many features of Amazon Bedrock. But in reality, using Bedrock isnâ€™t just about clicking around in the console.To build a real-world application, you need to make API calls to Bedrock and integrate those capabilities directly into your service.   To demonstrate this, AWS provides an AI Stylist demo application.This demo shows how end users actually experience an application built on top of Be...</div></div></div></a><a class="pagination-related" href="/2025/08/22/AWS-Certified-AI-Practitioner-14/" title="AWS Certified AI Practitioner(14) - Prompt Engineering"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-22</div><div class="info-item-2">AWS Certified AI Practitioner(14) - Prompt Engineering</div></div><div class="info-2"><div class="info-item-1">ğŸ“˜ Prompt EngineeringWhat is Prompt Engineering?Prompt Engineering is the process of designing, refining, andoptimizing prompts to guide a foundation model (FM) or large languagemodel (LLM) toward producing the best possible output for your needs. A naÃ¯ve prompt gives little guidance and leaves interpretation up tothe model.Example: â€œSummarize what is AWS.â€This works, but the answer may not be clear or focused. By contrast, Prompt Engineering uses a structured approach toimprove results.  Com...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/my_pic.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Danny Ki</div><div class="author-info-description">A data engineer's journey in coding, analytics, and building real-world systems.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">102</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kish191919"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/kish191919" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EB%82%98%EC%9D%98-%EC%86%8C%EA%B0%9C"><span class="toc-number">1.</span> <span class="toc-text">ë‚˜ì˜ ì†Œê°œ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EC%99%9C-%EC%9D%B4%EC%A7%81%ED%95%98%EB%8B%88"><span class="toc-number">2.</span> <span class="toc-text">ì™œ ì´ì§í•˜ë‹ˆ?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EC%99%9C-%EC%A7%80%EC%9B%90%ED%96%88%EB%8B%88-About-Accenture-federal-service"><span class="toc-number">3.</span> <span class="toc-text">ì™œ ì§€ì›í–ˆë‹ˆ? About Accenture federal service</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AFS-%EC%96%B4%EB%8A%90%ED%8C%8C%ED%8A%B8%EC%97%90-%EA%B4%80%EC%8B%AC%EC%9D%B4-%EB%A7%8E%EB%8B%88"><span class="toc-number">4.</span> <span class="toc-text">AFS ì–´ëŠíŒŒíŠ¸ì— ê´€ì‹¬ì´ ë§ë‹ˆ?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Recent-project-%EC%B5%9C%EA%B7%BC-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8"><span class="toc-number">5.</span> <span class="toc-text">Recent project (ìµœê·¼ í”„ë¡œì íŠ¸)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka-based-streaming-data-pipeline-%EA%B3%A0%EB%A0%A4%ED%95%9C-%EB%B6%80%EB%B6%84"><span class="toc-number">5.1.</span> <span class="toc-text">Kafka-based streaming data pipeline ê³ ë ¤í•œ ë¶€ë¶„.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Designing-ETL-pipelines-CA7%EC%99%80-Glue-%EC%9D%B4%EC%9A%A9"><span class="toc-number">6.</span> <span class="toc-text">Designing ETL pipelines - CA7ì™€ Glue ì´ìš©</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EA%B2%BD%ED%97%98-ETL-pipeline-%EC%B5%9C%EC%A0%81%ED%99%94"><span class="toc-number">6.1.</span> <span class="toc-text">ê²½í—˜ - ETL pipeline ìµœì í™”</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AWS-Glue-%EC%82%AC%EC%9A%A9%EA%B2%BD%ED%97%98-ETL-services"><span class="toc-number">7.</span> <span class="toc-text">AWS Glue ì‚¬ìš©ê²½í—˜ - ETL services</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EA%B7%B8%EB%9F%AC%EB%82%98-%ED%98%84%EC%9E%AC-glue%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EC%A7%80%EB%8A%94-%EC%95%8A%EB%8A%94%EB%8B%A4"><span class="toc-number">7.1.</span> <span class="toc-text">ê·¸ëŸ¬ë‚˜ í˜„ì¬ glueë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Orchestration-CA7-Glue-Airflow"><span class="toc-number">8.</span> <span class="toc-text">Data Orchestration (CA7, Glue, Airflow)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9B%A8%EC%96%B4%ED%95%98%EC%9A%B0%EC%8A%A4-%EA%B8%B0%EC%88%A0-e-g-Amazon-Redshift-Snowflake-%EC%82%AC%EC%9A%A9%EA%B2%BD%ED%97%98"><span class="toc-number">9.</span> <span class="toc-text">ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ê¸°ìˆ  (e.g. Amazon Redshift, Snowflake ì‚¬ìš©ê²½í—˜)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Redshift-%EB%9E%80"><span class="toc-number">9.1.</span> <span class="toc-text">Redshift ë€?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Redshift-Columnar-Storage"><span class="toc-number">9.2.</span> <span class="toc-text">Redshift Columnar Storage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%8A%A4%EB%85%B8%EC%9A%B0%ED%94%8C%EB%A0%88%EC%9D%B4%ED%81%AC-%EC%9E%A5%EC%A0%90-zero-copy-cloning-and-time-travle"><span class="toc-number">9.3.</span> <span class="toc-text">ìŠ¤ë…¸ìš°í”Œë ˆì´í¬ ì¥ì  (zero-copy cloning and time travle)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Databricks-%EC%82%AC%EC%9A%A9%EA%B2%BD%ED%97%98"><span class="toc-number">10.</span> <span class="toc-text">Databricks ì‚¬ìš©ê²½í—˜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%92%88%EC%A7%88%EA%B4%80%EB%A6%AC"><span class="toc-number">11.</span> <span class="toc-text">ë°ì´í„° í’ˆì§ˆê´€ë¦¬</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EA%B2%BD%ED%97%98-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%92%88%EC%A7%88%EA%B4%80%EB%A6%AC"><span class="toc-number">11.1.</span> <span class="toc-text">ê²½í—˜ - ë°ì´í„° í’ˆì§ˆê´€ë¦¬</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EA%B2%BD%ED%97%98-%EC%9C%A0%EB%8B%9B%ED%85%8C%EC%8A%A4-%EA%B4%80%EB%A0%A8%ED%95%B4%EC%84%9C"><span class="toc-number">11.2.</span> <span class="toc-text">ê²½í—˜ - ìœ ë‹›í…ŒìŠ¤ ê´€ë ¨í•´ì„œ</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EB%B3%B4%EC%95%88%EA%B3%BC-%EA%B7%9C%EC%A0%95%EA%B4%80%EB%A0%A8-in-cloud-environments-e-g-AWS-Azure"><span class="toc-number">12.</span> <span class="toc-text">ë³´ì•ˆê³¼ ê·œì •ê´€ë ¨ in cloud environments (e.g. AWS, Azure)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%ED%96%89%EB%8F%99-%EB%A9%B4%EC%A0%91-%EC%A7%88%EB%AC%B8-Behavioral-Team-fit"><span class="toc-number">13.</span> <span class="toc-text">í–‰ë™ ë©´ì ‘ ì§ˆë¬¸ (Behavioral &#x2F; Team fit)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%ED%8C%80%EC%9B%90%EB%93%A4%EA%B3%BC-%EC%9D%98%EA%B2%AC%EC%9D%B4-%EB%A7%9E%EC%A7%80-%EC%95%8A%EC%9D%84%EB%95%8C-Disagreement-within-a-team"><span class="toc-number">13.1.</span> <span class="toc-text">íŒ€ì›ë“¤ê³¼ ì˜ê²¬ì´ ë§ì§€ ì•Šì„ë•Œ (Disagreement within a team)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%83%9D%EC%82%B0%EC%97%90-%EB%AC%B8%EC%A0%9C%EA%B0%80-%EB%B0%9C%EC%83%9D%ED%96%88%EC%9D%84%EB%95%8C-%EB%8C%80%EC%9D%91%EB%B0%A9%EC%8B%9D-Respond-when-an-urgent-production-issue-occurred"><span class="toc-number">13.2.</span> <span class="toc-text">ìƒì‚°ì— ë¬¸ì œê°€ ë°œìƒí–ˆì„ë•Œ ëŒ€ì‘ë°©ì‹ (Respond when an urgent production issue occurred)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EA%B0%80-%EC%A7%80%EC%97%B0%EB%90%98%EC%97%88%EC%9D%84%EB%95%8C-%EB%8C%80%EC%9D%91%EB%B0%A9%EC%8B%9D-Faced-a-situation-where-a-project-delay-caused-customer-dissatisfaction"><span class="toc-number">13.3.</span> <span class="toc-text">í”„ë¡œì íŠ¸ê°€ ì§€ì—°ë˜ì—ˆì„ë•Œ ëŒ€ì‘ë°©ì‹ (Faced a situation where a project delay caused customer dissatisfaction)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%ED%8C%80-%EB%A6%AC%EB%93%9C-%EC%86%94%EC%84%A0%EC%88%98%EB%B2%94-lead-a-team-or-take-initiative"><span class="toc-number">13.4.</span> <span class="toc-text">íŒ€ ë¦¬ë“œ &amp; ì†”ì„ ìˆ˜ë²” (lead a team or take initiative)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%ED%83%80%EC%9D%B4%ED%8A%B8%ED%95%9C-%EC%8A%A4%EC%BC%80%EC%A4%84-%EB%86%92%EC%9D%80-%EC%95%95%EB%B0%95-%EC%83%81%ED%99%98-Handle-tight-deadlines-or-high-pressure-situations"><span class="toc-number">13.5.</span> <span class="toc-text">íƒ€ì´íŠ¸í•œ ìŠ¤ì¼€ì¤„ &amp; ë†’ì€ ì••ë°• ìƒí™˜ (Handle tight deadlines or high-pressure situations)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%8B%A4%ED%8C%A8-%EB%98%90%EB%8A%94-%EC%8B%A4%EC%88%98%ED%95%9C-%EA%B2%BD%EC%9A%B0-Failed-or-made-a-mistake"><span class="toc-number">13.6.</span> <span class="toc-text">ì‹¤íŒ¨ ë˜ëŠ” ì‹¤ìˆ˜í•œ ê²½ìš° (Failed or made a mistake)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%97%AC%EB%9F%AC-%EB%AF%BC%EC%A1%B1%EA%B3%BC-%EA%B0%99%EC%9D%B4-%EA%B7%BC%EB%AC%B4%ED%95%9C-%EA%B2%BD%EC%9A%B0-team-with-diverse-personalities-or-backgrounds"><span class="toc-number">13.7.</span> <span class="toc-text">ì—¬ëŸ¬ ë¯¼ì¡±ê³¼ ê°™ì´ ê·¼ë¬´í•œ ê²½ìš° (team with diverse personalities or backgrounds)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Partitioning-strategy-Spark-Redshift-Snowflake"><span class="toc-number">14.</span> <span class="toc-text">Partitioning strategy (Spark, Redshift, Snowflake)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EA%B2%BD%ED%97%98-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%ED%8B%B0%EC%85%98"><span class="toc-number">14.1.</span> <span class="toc-text">ê²½í—˜ - ë°ì´í„° íŒŒí‹°ì…˜</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EA%B7%9C%ED%99%94%EC%99%80-%EB%B9%84%EC%A0%95%EA%B7%9C%ED%99%94%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90"><span class="toc-number">15.</span> <span class="toc-text">ë°ì´í„° ì •ê·œí™”ì™€ ë¹„ì •ê·œí™”ì˜ ì°¨ì´ì </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%8A%A4%ED%83%80%EC%8A%A4%ED%82%A4%EB%A7%88-%EC%8A%A4%EB%85%B8%EC%9A%B0%ED%94%8C%EB%A0%88%EC%9D%B4%ED%81%AC-%EC%8A%A4%ED%82%A4%EB%A7%88-%EB%8C%80%EB%B6%80%EB%B6%80%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9B%A8%EC%96%B4%ED%95%98%EC%9A%B0%EC%8A%A4%EC%97%90%EC%84%9C%EB%8A%94-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EC%8A%A4%ED%82%A4%EB%A7%88"><span class="toc-number">15.1.</span> <span class="toc-text">ìŠ¤íƒ€ìŠ¤í‚¤ë§ˆ &amp; ìŠ¤ë…¸ìš°í”Œë ˆì´í¬ ìŠ¤í‚¤ë§ˆ (ëŒ€ë¶€ë¶€ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œëŠ” ì‚¬ìš©í•˜ëŠ” ìŠ¤í‚¤ë§ˆ)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Python-SQL-Spark-and-PySpark"><span class="toc-number">16.</span> <span class="toc-text">Python, SQL, Spark, and PySpark</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%9D%B4%EC%8A%88-Spark-memory-%EB%AC%B8%EC%A0%9C"><span class="toc-number">16.1.</span> <span class="toc-text">ì´ìŠˆ - Spark memory ë¬¸ì œ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%9D%B4%EC%8A%88-%EC%8A%A4%ED%82%A4%EB%A7%88-%EB%B3%80%EA%B2%BD"><span class="toc-number">16.2.</span> <span class="toc-text">ì´ìŠˆ - ìŠ¤í‚¤ë§ˆ ë³€ê²½</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EC%9D%B4%EC%8A%88-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B3%BC%EB%A5%A8%EC%A6%9D%EA%B0%80"><span class="toc-number">16.3.</span> <span class="toc-text">ì´ìŠˆ - ë°ì´í„° ë³¼ë¥¨ì¦ê°€</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EA%B2%BD%ED%97%98-%EC%8A%A4%ED%8C%8C%ED%81%AC-%ED%95%98%EB%91%A1-%EB%8D%B0%EC%9D%B4%ED%84%B0-ingestion"><span class="toc-number">17.</span> <span class="toc-text">ê²½í—˜ - ìŠ¤íŒŒí¬&#x2F;í•˜ë‘¡ ë°ì´í„° ingestion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EA%B2%BD%ED%97%98-AWS-%EB%A7%8E%EC%9D%B4-%EC%82%AC%EC%9A%A9%ED%96%88%EB%8B%88"><span class="toc-number">18.</span> <span class="toc-text">ê²½í—˜ - AWS ë§ì´ ì‚¬ìš©í–ˆë‹ˆ?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EC%9E%A5%EC%A0%90%EA%B3%BC-%EB%8B%A8%EC%A0%90"><span class="toc-number">19.</span> <span class="toc-text">ì¥ì ê³¼ ë‹¨ì </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EB%A7%88%EC%A7%80%EB%A7%89%EC%9C%BC%EB%A1%9C-%ED%95%98%EA%B3%A0-%EC%8B%B6%EC%9D%80-%EB%A7%90"><span class="toc-number">20.</span> <span class="toc-text">ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ê³  ì‹¶ì€ ë§</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hyperparameter-Tuning"><span class="toc-number"></span> <span class="toc-text">Hyperparameter Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-What-is-a-Hyperparameter"><span class="toc-number"></span> <span class="toc-text">1. What is a Hyperparameter?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Why-Hyperparameter-Tuning-Matters"><span class="toc-number"></span> <span class="toc-text">2. Why Hyperparameter Tuning Matters</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Key-Hyperparameters"><span class="toc-number"></span> <span class="toc-text">3. Key Hyperparameters</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Learning-Rate"><span class="toc-number">1.</span> <span class="toc-text">(1) Learning Rate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Batch-Size"><span class="toc-number">2.</span> <span class="toc-text">(2) Batch Size</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Number-of-Epochs"><span class="toc-number">3.</span> <span class="toc-text">(3) Number of Epochs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Regularization"><span class="toc-number">4.</span> <span class="toc-text">(4) Regularization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Overfitting"><span class="toc-number"></span> <span class="toc-text">4. Overfitting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-it"><span class="toc-number">1.</span> <span class="toc-text">What is it?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Causes"><span class="toc-number">2.</span> <span class="toc-text">Causes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solutions"><span class="toc-number">3.</span> <span class="toc-text">Solutions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-When-NOT-to-Use-Machine-Learning"><span class="toc-number"></span> <span class="toc-text">5. When NOT to Use Machine Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-AWS-Specific-Notes-for-Exams"><span class="toc-number"></span> <span class="toc-text">6. AWS-Specific Notes for Exams</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/17/Coding-Quiz-SQL-1/" title="Coding_Quiz_SQL_1">Coding_Quiz_SQL_1</a><time datetime="2025-09-18T02:07:37.000Z" title="Created 2025-09-17 22:07:37">2025-09-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/17/Coding-Quiz-Python-1/" title="Coding_Quiz_Python_1">Coding_Quiz_Python_1</a><time datetime="2025-09-17T20:29:17.000Z" title="Created 2025-09-17 16:29:17">2025-09-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/15/Databricks-CV-Anomaly-Detection/" title="Databricks CV Anomaly Detection">Databricks CV Anomaly Detection</a><time datetime="2025-09-15T20:40:23.000Z" title="Created 2025-09-15 16:40:23">2025-09-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/KO-AWS-ML-Associate-6/" title="(í•œêµ­ì–´) AWS ML Associate (6) - Amazon S3 í•µì‹¬ ì •ë¦¬">(í•œêµ­ì–´) AWS ML Associate (6) - Amazon S3 í•µì‹¬ ì •ë¦¬</a><time datetime="2025-09-15T03:49:25.000Z" title="Created 2025-09-14 23:49:25">2025-09-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/KO-AWS-ML-Associate-5/" title="(í•œêµ­ì–´) AWS ML Associate (5) - Amazon S3 í•µì‹¬ ì •ë¦¬">(í•œêµ­ì–´) AWS ML Associate (5) - Amazon S3 í•µì‹¬ ì •ë¦¬</a><time datetime="2025-09-15T02:33:24.000Z" title="Created 2025-09-14 22:33:24">2025-09-14</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Danny Ki</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>