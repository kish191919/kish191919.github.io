<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Danny&#39;s Blog</title>
  
  <subtitle>Data, AI, and Tech Insight</subtitle>
  <link href="https://kish191919.github.io/atom.xml" rel="self"/>
  
  <link href="https://kish191919.github.io/"/>
  <updated>2025-12-12T17:10:47.449Z</updated>
  <id>https://kish191919.github.io/</id>
  
  <author>
    <name>Danny Ki</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DATABRICKS-Fundamentals-5</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-5/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-5/</id>
    <published>2025-12-12T17:01:18.000Z</published>
    <updated>2025-12-12T17:10:47.449Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-오늘-강의에서-다룰-질문-3가지"><a href="#1-오늘-강의에서-다룰-질문-3가지" class="headerlink" title="1. 오늘 강의에서 다룰 질문 3가지"></a>1. 오늘 강의에서 다룰 질문 3가지</h2><p>여러분, Databricks 워크스페이스를 만들면 “웹에서 접속하는 화면”은 바로 보이죠.<br>그런데 수업을 하거나 실무에서 설명할 때는 <strong>아래 3가지 질문을 꼭 답할 수 있어야 합니다.</strong></p><ol><li><strong>Databricks는 Azure 어디에 설치되나요?</strong></li><li><strong>노트북&#x2F;잡&#x2F;클러스터&#x2F;데이터는 각각 어디에 있나요?</strong></li><li><strong>왜 굳이 Control Plane &#x2F; Data Plane으로 나누나요?</strong></li></ol><p>이 3가지를 이해하면, Databricks가 “그냥 웹앱”이 아니라 <strong>클라우드 네이티브 데이터 플랫폼</strong>이라는 게 보입니다.</p><hr><h2 id="2-큰-그림-Control-Plane-vs-Data-Plane"><a href="#2-큰-그림-Control-Plane-vs-Data-Plane" class="headerlink" title="2. 큰 그림: Control Plane vs Data Plane"></a>2. 큰 그림: Control Plane vs Data Plane</h2><h3 id="2-1-한-문장-요약"><a href="#2-1-한-문장-요약" class="headerlink" title="2.1 한 문장 요약"></a>2.1 한 문장 요약</h3><ul><li><strong>Control Plane(컨트롤 플레인)</strong>: Databricks가 제공하는 “관리&#x2F;제어” 영역 (UI, 노트북&#x2F;잡&#x2F;클러스터 설정 등)</li><li><strong>Data Plane(데이터 플레인)</strong>: 고객(Azure 구독) 안에서 실제로 돈이 나가는 “실행&#x2F;데이터” 영역 (클러스터 VM, 스토리지 등)</li></ul><blockquote><p>강의 멘트(핵심):<br>“**설정&#x2F;관리(컨트롤)**는 Databricks가, **실행&#x2F;데이터(데이터)**는 고객 구독에서 돌아간다.”</p></blockquote><hr><h2 id="3-Control-Plane-컨트롤-플레인-—-“Databricks의-두뇌”"><a href="#3-Control-Plane-컨트롤-플레인-—-“Databricks의-두뇌”" class="headerlink" title="3. Control Plane(컨트롤 플레인) — “Databricks의 두뇌”"></a>3. Control Plane(컨트롤 플레인) — “Databricks의 두뇌”</h2><h3 id="3-1-Control-Plane에-있는-것들-왜-중요한가"><a href="#3-1-Control-Plane에-있는-것들-왜-중요한가" class="headerlink" title="3.1 Control Plane에 있는 것들 (왜 중요한가?)"></a>3.1 Control Plane에 있는 것들 (왜 중요한가?)</h3><p>여기에는 Databricks가 제공하는 <strong>웹 애플리케이션(Workspace UI)</strong> 과 “메타데이터&#x2F;설정 정보”가 들어있습니다.</p><ul><li>Databricks Web Application (브라우저에서 보는 Workspace UI)</li><li>Notebooks (노트북 자체, 또는 노트북 메타&#x2F;버전&#x2F;권한 등)</li><li>Jobs &#x2F; Workflows 설정</li><li>Queries &#x2F; Alerts &#x2F; Dashboards 설정(특히 SQL 기능)</li><li>Cluster 설정(클러스터 생성 옵션, 노드 타입, 런타임 등)</li><li>사용자&#x2F;권한&#x2F;기본 정책(일부는 Account&#x2F;Workspace 레벨로 나뉨)</li></ul><p><strong>왜 컨트롤 플레인이 필요한가?</strong><br>Databricks는 사용자가 “직접 VM을 만들어서 Spark 설치하고 튜닝”하는 대신,<br><strong>UI&#x2F;REST API로 ‘요청’만 하면</strong> Databricks가 적절한 리소스를 띄우고 관리해줍니다.<br>즉, 컨트롤 플레인은 <strong>클라우드 인프라를 추상화해주는 제어 계층</strong>이에요.</p><h3 id="3-2-Control-Plane에-“직접-접속”-못하는-이유"><a href="#3-2-Control-Plane에-“직접-접속”-못하는-이유" class="headerlink" title="3.2 Control Plane에 “직접 접속” 못하는 이유"></a>3.2 Control Plane에 “직접 접속” 못하는 이유</h3><ul><li>사용자는 컨트롤 플레인 내부 서버에 SSH 같은 방식으로 들어가는 게 아니라,<ul><li><strong>브라우저(UI)</strong> 또는</li><li><strong>REST API&#x2F;CLI</strong><br>로만 기능을 사용합니다.</li></ul></li></ul><p><strong>이렇게 막아두는 이유</strong></p><ul><li>보안&#x2F;컴플라이언스: 관리 계층에 대한 직접 접근을 줄여 공격면을 최소화</li><li>운영 안정성: Databricks가 서비스 품질(SLA)을 보장하려면 관리 영역은 일관된 방식으로 운영되어야 함</li></ul><hr><h2 id="4-Data-Plane-데이터-플레인-—-“실제-실행과-비용이-발생하는-곳”"><a href="#4-Data-Plane-데이터-플레인-—-“실제-실행과-비용이-발생하는-곳”" class="headerlink" title="4. Data Plane(데이터 플레인) — “실제 실행과 비용이 발생하는 곳”"></a>4. Data Plane(데이터 플레인) — “실제 실행과 비용이 발생하는 곳”</h2><h3 id="4-1-Data-Plane에-있는-핵심-요소"><a href="#4-1-Data-Plane에-있는-핵심-요소" class="headerlink" title="4.1 Data Plane에 있는 핵심 요소"></a>4.1 Data Plane에 있는 핵심 요소</h3><p>Data Plane은 <strong>고객의 Azure Subscription</strong> 안에 있습니다. 그래서 비용도 여기서 발생합니다.</p><ul><li><strong>Compute(클러스터&#x2F;SQL Warehouse 실행 리소스)</strong><ul><li>Spark 작업을 돌리는 VM&#x2F;노드들(드라이버&#x2F;워커)</li></ul></li><li><strong>Storage(데이터 저장소)</strong><ul><li>고객 데이터: 보통 ADLS Gen2 &#x2F; Blob Storage</li><li>워크스페이스 기본 스토리지(Workspace Root Storage &#x2F; Root DBFS)</li></ul></li></ul><blockquote><p>강의 멘트(핵심):<br>“컨트롤 플레인은 ‘설정과 명령’, 데이터 플레인은 ‘실제 실행과 데이터’입니다.<br>비용이 큰 건 대부분 데이터 플레인에서 나옵니다.”</p></blockquote><h3 id="4-2-“클러스터는-왜-고객-구독에-만들까-”"><a href="#4-2-“클러스터는-왜-고객-구독에-만들까-”" class="headerlink" title="4.2 “클러스터는 왜 고객 구독에 만들까?”"></a>4.2 “클러스터는 왜 고객 구독에 만들까?”</h3><ul><li><strong>비용&#x2F;과금 주체가 고객</strong>이기 때문</li><li><strong>네트워크&#x2F;보안(예: VNet, Private Link, 방화벽, NSG) 정책을 고객이 통제</strong>하기 때문</li><li>기업 환경에서는 데이터&#x2F;네트워크 통제가 매우 중요 → 실행 인프라가 고객 구독에 있어야 함</li></ul><hr><h2 id="5-DBFS와-Workspace-Root-Storage-—-“기본-저장소지만-메인-데이터레이크는-아니다”"><a href="#5-DBFS와-Workspace-Root-Storage-—-“기본-저장소지만-메인-데이터레이크는-아니다”" class="headerlink" title="5. DBFS와 Workspace Root Storage — “기본 저장소지만, 메인 데이터레이크는 아니다”"></a>5. DBFS와 Workspace Root Storage — “기본 저장소지만, 메인 데이터레이크는 아니다”</h2><h3 id="5-1-DBFS-Databricks-File-System-가-뭔가요"><a href="#5-1-DBFS-Databricks-File-System-가-뭔가요" class="headerlink" title="5.1 DBFS( Databricks File System )가 뭔가요?"></a>5.1 DBFS( Databricks File System )가 뭔가요?</h3><p>초보자 기준으로는 이렇게 이해하면 됩니다.</p><ul><li>DBFS는 Databricks에서 파일을 다루기 위한 <strong>가상 파일시스템 인터페이스</strong></li><li>내부적으로는 Azure Storage(ADLS&#x2F;Blob) 같은 곳을 기반으로 동작할 수 있음</li><li>예: 라이브러리 업로드, 임시 파일, 일부 로그&#x2F;결과 등</li></ul><h3 id="5-2-Workspace-Root-Storage-Root-DBFS-주의사항"><a href="#5-2-Workspace-Root-Storage-Root-DBFS-주의사항" class="headerlink" title="5.2 Workspace Root Storage(&#x3D; Root DBFS) 주의사항"></a>5.2 Workspace Root Storage(&#x3D; Root DBFS) 주의사항</h3><p>워크스페이스를 만들면 자동으로 “기본 저장소”가 하나 생기는데, 이것이 Root DBFS입니다.</p><ul><li>워크스페이스 시스템 용도(일부 로그&#x2F;시스템 데이터)에 적합</li><li>하지만 <strong>실무에서 데이터레이크(원천&#x2F;정제&#x2F;골드 데이터)</strong> 를 여기에 넣는 건 권장되지 않음</li></ul><p><strong>왜 별도 Storage Account가 필요할까?</strong></p><ul><li>데이터 거버넌스&#x2F;보안: “데이터는 고객이 직접 통제하는 저장소에”</li><li>운영&#x2F;백업&#x2F;수명주기(Lifecycle): 워크스페이스와 데이터의 생명주기를 분리</li><li>멀티 워크스페이스 공유: 하나의 데이터레이크를 여러 워크스페이스에서 접근</li><li>비용&#x2F;정책&#x2F;권한 모델을 더 명확하게 설계 가능</li></ul><blockquote><p>강의 멘트:<br>“Root DBFS는 ‘워크스페이스의 기본 공용창고’ 같은 느낌이고,<br>우리 회사의 핵심 데이터는 ‘별도의 공식 창고(ADLS Gen2)’에 보관하는 게 정석입니다.”</p></blockquote><hr><h2 id="6-인증과-접근-방식-—-SSO-API-CLI"><a href="#6-인증과-접근-방식-—-SSO-API-CLI" class="headerlink" title="6. 인증과 접근 방식 — SSO &#x2F; API &#x2F; CLI"></a>6. 인증과 접근 방식 — SSO &#x2F; API &#x2F; CLI</h2><h3 id="6-1-사용자-Web-UI-접근"><a href="#6-1-사용자-Web-UI-접근" class="headerlink" title="6.1 사용자(Web UI) 접근"></a>6.1 사용자(Web UI) 접근</h3><ul><li>일반적으로 <strong>SSO</strong>(Microsoft Entra ID 기반)로 로그인</li><li>즉, “회사 계정”으로 Databricks에 접속하는 흐름이 자연스럽게 구성됨</li></ul><h3 id="6-2-자동화-도구-접근-REST-API-CLI"><a href="#6-2-자동화-도구-접근-REST-API-CLI" class="headerlink" title="6.2 자동화&#x2F;도구 접근(REST API &#x2F; CLI)"></a>6.2 자동화&#x2F;도구 접근(REST API &#x2F; CLI)</h3><ul><li>운영&#x2F;배포&#x2F;자동화에서는 UI보다 API&#x2F;CLI가 중요할 때가 많음</li><li>예: 노트북 배포, 잡 생성&#x2F;실행, 클러스터 관리, 리포지토리 연동 등</li></ul><blockquote><p>강의 팁:<br>“팀이 커지면 ‘수동 클릭’은 한계가 옵니다.<br>그래서 컨트롤 플레인이 REST API를 제공하는 게 매우 중요해요.”</p></blockquote><hr><h2 id="7-그림에서-보이는-추가-포인트-Serverless-Compute-Plane"><a href="#7-그림에서-보이는-추가-포인트-Serverless-Compute-Plane" class="headerlink" title="7. (그림에서 보이는) 추가 포인트: Serverless Compute Plane"></a>7. (그림에서 보이는) 추가 포인트: Serverless Compute Plane</h2><p>일부 기능(예: Serverless SQL Warehouses, Model Serving 등)은<br>“서버리스” 형태의 컴퓨팅 플레인을 사용하기도 합니다.</p><p>초보자 기준으로는 이렇게만 기억해도 충분합니다.</p><ul><li><strong>Classic compute plane</strong>: 일반적인 Spark 클러스터(드라이버&#x2F;워커 VM 기반)</li><li><strong>Serverless compute plane</strong>: 관리 부담을 더 줄인 실행 방식(서비스형)</li></ul><hr><h2 id="8-실무-베스트-프랙티스-체크리스트-강의에서-꼭-강조"><a href="#8-실무-베스트-프랙티스-체크리스트-강의에서-꼭-강조" class="headerlink" title="8. 실무 베스트 프랙티스 체크리스트 (강의에서 꼭 강조)"></a>8. 실무 베스트 프랙티스 체크리스트 (강의에서 꼭 강조)</h2><h3 id="8-1-리소스-정리-전략"><a href="#8-1-리소스-정리-전략" class="headerlink" title="8.1 리소스 정리 전략"></a>8.1 리소스 정리 전략</h3><ul><li>프로젝트마다 <strong>Resource Group</strong>을 분리 → 나중에 삭제&#x2F;정리 쉬움</li><li>실습 끝난 후 <strong>클러스터 종료&#x2F;삭제</strong> 습관화 (비용 폭탄 방지)</li></ul><h3 id="8-2-데이터-저장소-설계"><a href="#8-2-데이터-저장소-설계" class="headerlink" title="8.2 데이터 저장소 설계"></a>8.2 데이터 저장소 설계</h3><ul><li>Root DBFS에 핵심 데이터 적재 ❌</li><li>별도 <strong>ADLS Gen2 Storage Account</strong> 만들고,<ul><li>Raw &#x2F; Silver &#x2F; Gold 구조(또는 Bronze&#x2F;Silver&#x2F;Gold)</li><li>접근 권한(ACL&#x2F;RBAC) 명확히 설계</li></ul></li></ul><h3 id="8-3-네트워크-보안-기업-환경"><a href="#8-3-네트워크-보안-기업-환경" class="headerlink" title="8.3 네트워크&#x2F;보안(기업 환경)"></a>8.3 네트워크&#x2F;보안(기업 환경)</h3><ul><li>VNet 인젝션, Private Link, 방화벽&#x2F;NSG 정책 등은<br>“데이터 플레인”에서 고객이 통제한다는 점을 연결해서 설명</li></ul><hr><h2 id="9-자격증-인터뷰-포인트-따로-정리"><a href="#9-자격증-인터뷰-포인트-따로-정리" class="headerlink" title="9. 자격증&#x2F;인터뷰 포인트 (따로 정리)"></a>9. 자격증&#x2F;인터뷰 포인트 (따로 정리)</h2><h3 id="9-1-자주-나오는-핵심-질문"><a href="#9-1-자주-나오는-핵심-질문" class="headerlink" title="9.1 자주 나오는 핵심 질문"></a>9.1 자주 나오는 핵심 질문</h3><ol><li><strong>Control Plane과 Data Plane의 차이?</strong><ul><li>Control Plane: 관리&#x2F;UI&#x2F;설정&#x2F;메타데이터</li><li>Data Plane: 실행(클러스터)&#x2F;데이터 저장소(ADLS&#x2F;Blob)</li></ul></li><li><strong>비용은 어디서 발생?</strong><ul><li>대부분 Data Plane(클러스터 VM, 스토리지, 네트워크 egress 등)</li></ul></li><li><strong>노트북&#x2F;잡 설정은 어디에 저장?</strong><ul><li>Control Plane(설정&#x2F;메타), 실행은 Data Plane</li></ul></li><li><strong>DBFS Root Storage를 메인 데이터레이크로 쓰면 안 되는 이유?</strong><ul><li>데이터 거버넌스&#x2F;통제, 워크스페이스와 데이터 생명주기 분리, 공유&#x2F;보안&#x2F;운영 측면</li></ul></li><li><strong>Databricks에 접속하는 방법 2가지 이상 설명</strong><ul><li>Workspace UI(SSO), REST API&#x2F;CLI(토큰&#x2F;자격증명 기반)</li></ul></li></ol><h3 id="9-2-인터뷰에서-“좋아-보이는”-한-문장-답변-예시"><a href="#9-2-인터뷰에서-“좋아-보이는”-한-문장-답변-예시" class="headerlink" title="9.2 인터뷰에서 “좋아 보이는” 한 문장 답변 예시"></a>9.2 인터뷰에서 “좋아 보이는” 한 문장 답변 예시</h3><ul><li>“Databricks는 관리 계층(Control Plane)과 실행&#x2F;데이터 계층(Data Plane)을 분리해서<br><strong>운영&#x2F;보안&#x2F;확장성</strong>을 확보하고, 고객은 Data Plane에서 <strong>비용과 네트워크&#x2F;데이터 통제</strong>를 가져갑니다.”</li></ul><hr><h2 id="10-마무리-다음-단계-연결-멘트"><a href="#10-마무리-다음-단계-연결-멘트" class="headerlink" title="10. 마무리 (다음 단계 연결 멘트)"></a>10. 마무리 (다음 단계 연결 멘트)</h2><p>오늘은 “Databricks가 Azure 안에서 어떻게 나뉘어 배치되는지”를 봤습니다.<br>다음 단계는 아주 자연스럽게 이렇게 이어집니다.</p><ul><li><strong>Compute 만들기(Cluster &#x2F; SQL Warehouse)</strong></li><li><strong>Storage 연결하기(ADLS Gen2 Mount &#x2F; External Location)</strong></li><li><strong>데이터 적재&#x2F;변환 파이프라인 만들기(Notebook, Jobs, Workflows)</strong></li></ul><blockquote><p>엔딩 멘트(강의용):<br>“오늘 아키텍처를 이해하면, 앞으로 어떤 기능을 배워도 ‘이게 어디에서 돌아가는지’가 보이기 시작합니다.”</p></blockquote><hr><h2 id="부록-A-초보자-Q-A-강의-중-자주-받는-질문"><a href="#부록-A-초보자-Q-A-강의-중-자주-받는-질문" class="headerlink" title="부록 A. 초보자 Q&amp;A (강의 중 자주 받는 질문)"></a>부록 A. 초보자 Q&amp;A (강의 중 자주 받는 질문)</h2><h3 id="Q1-“워크스페이스가-서버리스-앱이면-클러스터는-왜-또-만들어요-”"><a href="#Q1-“워크스페이스가-서버리스-앱이면-클러스터는-왜-또-만들어요-”" class="headerlink" title="Q1) “워크스페이스가 서버리스 앱이면, 클러스터는 왜 또 만들어요?”"></a>Q1) “워크스페이스가 서버리스 앱이면, 클러스터는 왜 또 만들어요?”</h3><ul><li>워크스페이스(UI&#x2F;설정)는 “관리용”</li><li>실제 Spark 작업을 돌리는 계산 자원이 “클러스터”</li><li>즉, <strong>워크스페이스는 조종석, 클러스터는 엔진</strong>이라고 비유하면 쉬워요.</li></ul><h3 id="Q2-“Control-Plane이-Databricks에-있으면-데이터도-Databricks로-가나요-”"><a href="#Q2-“Control-Plane이-Databricks에-있으면-데이터도-Databricks로-가나요-”" class="headerlink" title="Q2) “Control Plane이 Databricks에 있으면 데이터도 Databricks로 가나요?”"></a>Q2) “Control Plane이 Databricks에 있으면 데이터도 Databricks로 가나요?”</h3><ul><li>핵심 데이터는 보통 고객 Storage(ADLS Gen2)에 둡니다.</li><li>Databricks는 그 데이터를 “처리”하고 결과를 다시 저장하는 역할.</li></ul><h3 id="Q3-“Data-Plane에-있는-VM은-내가-직접-접속-SSH-하나요-”"><a href="#Q3-“Data-Plane에-있는-VM은-내가-직접-접속-SSH-하나요-”" class="headerlink" title="Q3) “Data Plane에 있는 VM은 내가 직접 접속(SSH)하나요?”"></a>Q3) “Data Plane에 있는 VM은 내가 직접 접속(SSH)하나요?”</h3><ul><li>일반적으로는 Databricks가 관리하므로 직접 SSH는 권장되지 않는 경우가 많습니다.</li><li>운영 방식은 회사 정책&#x2F;보안 구성(VNet, Private Link 등)에 따라 달라질 수 있어요.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-오늘-강의에서-다룰-질문-3가지&quot;&gt;&lt;a href=&quot;#1-오늘-강의에서-다룰-질문-3가지&quot; class=&quot;headerlink&quot; title=&quot;1. 오늘 강의에서 다룰 질문 3가지&quot;&gt;&lt;/a&gt;1. 오늘 강의에서 다룰 질문 3가지&lt;/h2&gt;&lt;p&gt;</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-4</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-4/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-4/</id>
    <published>2025-12-12T16:44:59.000Z</published>
    <updated>2025-12-12T16:54:32.852Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Databricks-Workspace란"><a href="#1-Databricks-Workspace란" class="headerlink" title="1. Databricks Workspace란?"></a>1. Databricks Workspace란?</h2><p>Databricks Workspace는<br>👉 <strong>Databricks 플랫폼을 사용하는 모든 작업의 출발점</strong>입니다.</p><ul><li>코드 작성 (Notebook)</li><li>데이터 처리 (Spark &#x2F; SQL)</li><li>클러스터 생성 및 관리</li><li>워크플로우(Job) 생성</li><li>데이터 거버넌스, ML, SQL 분석</li></ul><p>👉 <strong>Databricks에서 하는 모든 작업은 Workspace에서 시작</strong>합니다.</p><hr><h2 id="2-Workspace-홈-화면-개요"><a href="#2-Workspace-홈-화면-개요" class="headerlink" title="2. Workspace 홈 화면 개요"></a>2. Workspace 홈 화면 개요</h2><h3 id="상단-Home-영역"><a href="#상단-Home-영역" class="headerlink" title="상단(Home) 영역"></a>상단(Home) 영역</h3><ul><li>최근 사용한 항목 (Recent)</li><li>즐겨찾기 (Favorites)</li><li>빠른 시작용 바로가기</li></ul><blockquote><p>강의 팁<br>👉 초반에는 <strong>거의 사용하지 않음</strong><br>👉 실제 작업은 <strong>왼쪽 메뉴가 핵심</strong></p></blockquote><hr><h2 id="3-왼쪽-메뉴-Left-Navigation-Bar"><a href="#3-왼쪽-메뉴-Left-Navigation-Bar" class="headerlink" title="3. 왼쪽 메뉴 (Left Navigation Bar)"></a>3. 왼쪽 메뉴 (Left Navigation Bar)</h2><h3 id="메뉴-확장-축소"><a href="#메뉴-확장-축소" class="headerlink" title="메뉴 확장&#x2F;축소"></a>메뉴 확장&#x2F;축소</h3><ul><li>기본은 축소 상태일 수 있음</li><li>마우스를 가져가면 자동 확장</li><li>⚙️ 강의 시에는 <strong>항상 확장 상태 추천</strong></li></ul><hr><h2 id="4-Workspace-메뉴-가장-중요-⭐"><a href="#4-Workspace-메뉴-가장-중요-⭐" class="headerlink" title="4. Workspace 메뉴 (가장 중요 ⭐)"></a>4. Workspace 메뉴 (가장 중요 ⭐)</h2><h3 id="역할"><a href="#역할" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>코드와 파일이 저장되는 공간</strong></p><h3 id="구조"><a href="#구조" class="headerlink" title="구조"></a>구조</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Workspace</span><br><span class="line"> ├── Shared</span><br><span class="line"> └── Users</span><br><span class="line">     └── &lt;사용자명&gt;</span><br></pre></td></tr></table></figure><h3 id="Users-내-홈-디렉토리"><a href="#Users-내-홈-디렉토리" class="headerlink" title="Users &gt; 내 홈 디렉토리"></a>Users &gt; 내 홈 디렉토리</h3><ul><li>개인 작업 공간</li><li>생성 가능 항목:<ul><li>📁 Folder</li><li>📓 Notebook (Python &#x2F; SQL &#x2F; Scala)</li><li>📄 File</li><li>📊 Dashboard</li><li>🔔 Alert</li></ul></li></ul><blockquote><p>중요 포인트<br>👉 “Notebook은 결국 Workspace 안에 저장된다”</p></blockquote><hr><h2 id="5-Repos-소스코드-관리"><a href="#5-Repos-소스코드-관리" class="headerlink" title="5. Repos (소스코드 관리)"></a>5. Repos (소스코드 관리)</h2><h3 id="기능"><a href="#기능" class="headerlink" title="기능"></a>기능</h3><ul><li>GitHub &#x2F; GitHub Enterprise</li><li>Azure DevOps</li><li>Bitbucket 등 연동 가능</li></ul><h3 id="활용"><a href="#활용" class="headerlink" title="활용"></a>활용</h3><ul><li>Git 기반 개발</li><li>Commit &#x2F; Pull &#x2F; Push 가능</li><li>협업 필수 기능</li></ul><blockquote><p>실무 팁<br>👉 개인 학습: Workspace<br>👉 팀&#x2F;프로젝트: <strong>Repos 필수</strong></p></blockquote><hr><h2 id="6-Catalog-Catalog-Explorer"><a href="#6-Catalog-Catalog-Explorer" class="headerlink" title="6. Catalog (Catalog Explorer)"></a>6. Catalog (Catalog Explorer)</h2><h3 id="역할-1"><a href="#역할-1" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>메타데이터 관리 화면</strong></p><p>표시되는 항목:</p><ul><li>Databases</li><li>Tables</li><li>Views</li><li>Functions</li></ul><blockquote><p>이후 학습 주제</p><ul><li>Unity Catalog</li><li>데이터 권한 관리</li></ul></blockquote><hr><h2 id="7-Workflow-Jobs"><a href="#7-Workflow-Jobs" class="headerlink" title="7. Workflow (Jobs)"></a>7. Workflow (Jobs)</h2><h3 id="역할-2"><a href="#역할-2" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>배치 작업 &amp; 파이프라인 관리</strong></p><p>가능한 작업:</p><ul><li>Job 생성</li><li>Task 간 의존성 설정</li><li>스케줄링 (Cron)</li><li>재시도 &#x2F; 실패 처리</li></ul><h3 id="하위-기능"><a href="#하위-기능" class="headerlink" title="하위 기능"></a>하위 기능</h3><ul><li>Jobs</li><li>Job Runs</li><li>Delta Live Tables (DLT)</li></ul><hr><h2 id="8-Compute-클러스터-관리"><a href="#8-Compute-클러스터-관리" class="headerlink" title="8. Compute (클러스터 관리)"></a>8. Compute (클러스터 관리)</h2><h3 id="역할-3"><a href="#역할-3" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>Spark 실행을 위한 컴퓨트 리소스 관리</strong></p><p>생성 가능한 리소스:</p><ul><li>All-purpose Cluster</li><li>Job Cluster</li><li>SQL Warehouse</li><li>Cluster Pool</li><li>Cluster Policy</li></ul><blockquote><p>핵심 메시지<br>👉 <strong>Azure Portal이 아니라 Databricks에서 클러스터 생성</strong></p></blockquote><hr><h2 id="9-Data-Ingestion"><a href="#9-Data-Ingestion" class="headerlink" title="9. Data Ingestion"></a>9. Data Ingestion</h2><h3 id="목적"><a href="#목적" class="headerlink" title="목적"></a>목적</h3><p>👉 외부 데이터를 Databricks로 가져오기</p><h3 id="방식"><a href="#방식" class="headerlink" title="방식"></a>방식</h3><ul><li>Native Spark Connectors</li><li>Partner Tools (Fivetran 등)</li></ul><blockquote><p>중요 포인트<br>👉 “Databricks는 처리 플랫폼, 수집은 도구 선택”</p></blockquote><hr><h2 id="10-Delta-Live-Tables-DLT"><a href="#10-Delta-Live-Tables-DLT" class="headerlink" title="10. Delta Live Tables (DLT)"></a>10. Delta Live Tables (DLT)</h2><h3 id="개념"><a href="#개념" class="headerlink" title="개념"></a>개념</h3><ul><li>Declarative ETL</li><li>파이프라인 정의 기반 처리</li></ul><h3 id="위치"><a href="#위치" class="headerlink" title="위치"></a>위치</h3><ul><li>Workflow 메뉴 하위</li></ul><blockquote><p>이후 심화 주제</p></blockquote><hr><h2 id="11-SQL-메뉴"><a href="#11-SQL-메뉴" class="headerlink" title="11. SQL 메뉴"></a>11. SQL 메뉴</h2><p>Databricks는 <strong>서버리스 데이터 웨어하우스</strong> 역할도 수행</p><p>가능한 작업:</p><ul><li>SQL Warehouse 생성</li><li>SQL Editor</li><li>Dashboard</li><li>Alert</li><li>Query History</li></ul><blockquote><p>SQL 중심 분석가 대상 기능</p></blockquote><hr><h2 id="12-Machine-Learning-메뉴"><a href="#12-Machine-Learning-메뉴" class="headerlink" title="12. Machine Learning 메뉴"></a>12. Machine Learning 메뉴</h2><p>ML 관련 기능 제공:</p><ul><li>Experiments</li><li>Models</li><li>Feature Store</li><li>MLflow</li></ul><blockquote><p>데이터 엔지니어 → ML 엔지니어 확장 포인트</p></blockquote><hr><h2 id="13-Marketplace"><a href="#13-Marketplace" class="headerlink" title="13. Marketplace"></a>13. Marketplace</h2><h3 id="기능-1"><a href="#기능-1" class="headerlink" title="기능"></a>기능</h3><ul><li>외부 데이터 구매&#x2F;구독</li><li>무료&#x2F;유료 데이터셋</li></ul><h3 id="기반-기술"><a href="#기반-기술" class="headerlink" title="기반 기술"></a>기반 기술</h3><ul><li>Delta Sharing</li></ul><hr><h2 id="14-Partner-Connect"><a href="#14-Partner-Connect" class="headerlink" title="14. Partner Connect"></a>14. Partner Connect</h2><h3 id="목적-1"><a href="#목적-1" class="headerlink" title="목적"></a>목적</h3><p>👉 외부 솔루션과 <strong>원클릭 연동</strong></p><p>파트너 예시:</p><ul><li>Data Ingestion</li><li>Visualization (Tableau 등)</li><li>Security</li><li>Governance</li><li>ML Tools</li></ul><hr><h2 id="15-우측-상단-메뉴"><a href="#15-우측-상단-메뉴" class="headerlink" title="15. 우측 상단 메뉴"></a>15. 우측 상단 메뉴</h2><h3 id="항목"><a href="#항목" class="headerlink" title="항목"></a>항목</h3><ul><li>User Settings</li><li>Admin Settings</li><li>Manage Account (Admin Console)</li><li>Logout</li></ul><blockquote><p>관리자는 Admin Settings 자주 사용</p></blockquote><hr><h2 id="16-강의용-핵심-요약-한-문장씩"><a href="#16-강의용-핵심-요약-한-문장씩" class="headerlink" title="16. 강의용 핵심 요약 (한 문장씩)"></a>16. 강의용 핵심 요약 (한 문장씩)</h2><ul><li>Workspace &#x3D; Databricks의 모든 작업 시작점</li><li>코드 저장 &#x3D; Workspace</li><li>실행 환경 &#x3D; Compute</li><li>자동화 &#x3D; Workflow</li><li>메타데이터 &#x3D; Catalog</li><li>SQL 분석 &#x3D; SQL 메뉴</li><li>ML &#x3D; Machine Learning 메뉴</li></ul><hr><h2 id="17-추천-강의-흐름"><a href="#17-추천-강의-흐름" class="headerlink" title="17. 추천 강의 흐름"></a>17. 추천 강의 흐름</h2><ol><li>Workspace UI 전체 구조 설명</li><li>Workspace → Notebook 생성</li><li>Compute → Cluster 생성</li><li>Notebook 실행</li><li>Workflow → Job 만들기</li></ol><hr><h2 id="마무리"><a href="#마무리" class="headerlink" title="마무리"></a>마무리</h2><p>Databricks Workspace는 단순한 UI가 아니라<br>👉 <strong>데이터 엔지니어링 작업의 컨트롤 타워</strong>입니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-Databricks-Workspace란&quot;&gt;&lt;a href=&quot;#1-Databricks-Workspace란&quot; class=&quot;headerlink&quot; title=&quot;1. Databricks Workspace란?&quot;&gt;&lt;/a&gt;1. Databricks W</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-3</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-3/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-3/</id>
    <published>2025-12-12T16:27:24.000Z</published>
    <updated>2025-12-12T16:35:20.737Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Azure-Free-Account-생성"><a href="#1-Azure-Free-Account-생성" class="headerlink" title="1. Azure Free Account 생성"></a>1. Azure Free Account 생성</h2><h3 id="Azure-Free-Account-혜택"><a href="#Azure-Free-Account-혜택" class="headerlink" title="Azure Free Account 혜택"></a>Azure Free Account 혜택</h3><ul><li>💳 <strong>$200 크레딧 (30일)</strong></li><li>🆓 인기 Azure 서비스 12개월 무료</li><li>🆓 40개 이상 Always Free 서비스</li></ul><blockquote><p>⚠️ 주의<br>$200 크레딧은 <strong>30일 후 소멸</strong><br>→ 실습은 가급적 한 달 내에 완료 권장</p></blockquote><h3 id="계정-생성-절차-요약"><a href="#계정-생성-절차-요약" class="headerlink" title="계정 생성 절차 요약"></a>계정 생성 절차 요약</h3><ol><li><a href="https://azure.microsoft.com/">https://azure.microsoft.com</a> 접속</li><li><strong>Start free</strong> 클릭</li><li>Microsoft 계정 로그인 또는 신규 생성</li><li>이메일 + 휴대폰 인증</li><li>주소 입력</li><li><strong>Visa &#x2F; MasterCard 카드 등록</strong><ul><li>소액 인증만 발생</li><li>자동 과금 ❌ (수동 업그레이드 전까지)</li></ul></li></ol><hr><h2 id="2-Azure-Portal-기본-사용법"><a href="#2-Azure-Portal-기본-사용법" class="headerlink" title="2. Azure Portal 기본 사용법"></a>2. Azure Portal 기본 사용법</h2><h3 id="Azure-Portal-접속"><a href="#Azure-Portal-접속" class="headerlink" title="Azure Portal 접속"></a>Azure Portal 접속</h3><ul><li><a href="https://portal.azure.com/">https://portal.azure.com</a></li></ul><h3 id="주요-구성-요소"><a href="#주요-구성-요소" class="headerlink" title="주요 구성 요소"></a>주요 구성 요소</h3><ul><li>🔍 <strong>Search Bar</strong>: 모든 Azure 서비스 검색</li><li>📁 <strong>Resource Group</strong>: 리소스 묶음 관리</li><li>💻 <strong>Cloud Shell</strong>: Browser 기반 Bash &#x2F; PowerShell</li><li>🔐 <strong>Azure Active Directory (Entra ID)</strong> 자동 생성</li></ul><blockquote><p>실무 팁<br>👉 서비스 이름을 알고 있으면 <strong>검색창 사용이 가장 빠름</strong></p></blockquote><hr><h2 id="3-Azure-Databricks-Workspace-생성"><a href="#3-Azure-Databricks-Workspace-생성" class="headerlink" title="3. Azure Databricks Workspace 생성"></a>3. Azure Databricks Workspace 생성</h2><h3 id="Workspace란"><a href="#Workspace란" class="headerlink" title="Workspace란?"></a>Workspace란?</h3><ul><li>Azure 위에서 실행되는 <strong>Databricks 전용 관리 단위</strong></li><li>하나의 프로젝트 &#x3D; 하나의 Workspace 권장</li></ul><h3 id="생성-절차"><a href="#생성-절차" class="headerlink" title="생성 절차"></a>생성 절차</h3><ol><li>Azure Portal → 검색창에 <strong>Azure Databricks</strong></li><li><strong>Create</strong> 클릭</li></ol><h3 id="기본-설정-중요"><a href="#기본-설정-중요" class="headerlink" title="기본 설정 (중요)"></a>기본 설정 (중요)</h3><table><thead><tr><th>항목</th><th>권장 값</th></tr></thead><tbody><tr><td>Subscription</td><td>Pay-as-you-go</td></tr><tr><td>Resource Group</td><td>프로젝트별 1개</td></tr><tr><td>Workspace Name</td><td>명확한 이름</td></tr><tr><td>Region</td><td>East US (저렴)</td></tr><tr><td>Pricing Tier</td><td><strong>Premium (권장)</strong></td></tr></tbody></table><blockquote><p>Premium 선택 이유</p><ul><li>Role Based Access Control (RBAC)</li><li>Unity Catalog 사용 가능</li><li>실무 필수 기능 포함</li></ul></blockquote><h3 id="생성-완료"><a href="#생성-완료" class="headerlink" title="생성 완료"></a>생성 완료</h3><ul><li>약 <strong>5~10분 소요</strong></li><li>완료 후 <strong>Launch Workspace</strong> 버튼 활성화</li></ul><hr><h2 id="4-Databricks-Workspace-접속-방법"><a href="#4-Databricks-Workspace-접속-방법" class="headerlink" title="4. Databricks Workspace 접속 방법"></a>4. Databricks Workspace 접속 방법</h2><h3 id="방법-1-Azure-Portal-경유"><a href="#방법-1-Azure-Portal-경유" class="headerlink" title="방법 1: Azure Portal 경유"></a>방법 1: Azure Portal 경유</h3><ul><li>Azure Portal → Workspace → <strong>Launch Workspace</strong></li></ul><h3 id="방법-2-Workspace-URL-직접-접속"><a href="#방법-2-Workspace-URL-직접-접속" class="headerlink" title="방법 2: Workspace URL 직접 접속"></a>방법 2: Workspace URL 직접 접속</h3><ul><li>Workspace URL 복사 → 브라우저 직접 접속</li><li>Azure SSO로 자동 로그인</li></ul><blockquote><p>✔️ 실무에서는 <strong>URL 직접 접속 방식</strong>이 가장 흔함</p></blockquote><hr><h2 id="5-Databricks-Workspace-기본-구성"><a href="#5-Databricks-Workspace-기본-구성" class="headerlink" title="5. Databricks Workspace 기본 구성"></a>5. Databricks Workspace 기본 구성</h2><h3 id="주요-메뉴"><a href="#주요-메뉴" class="headerlink" title="주요 메뉴"></a>주요 메뉴</h3><ul><li><strong>Workspace</strong>: Notebook &#x2F; Folder 관리</li><li><strong>Compute</strong>: Cluster 관리</li><li><strong>Catalog</strong>: Unity Catalog (Premium)</li><li><strong>Jobs</strong>: 배치 작업 스케줄링</li><li><strong>Repos</strong>: Git 연동</li><li><strong>Admin</strong>: 사용자 &#x2F; 권한 &#x2F; 정책 관리</li></ul><hr><h2 id="6-실무에서-가장-먼저-하는-작업-순서"><a href="#6-실무에서-가장-먼저-하는-작업-순서" class="headerlink" title="6. 실무에서 가장 먼저 하는 작업 순서"></a>6. 실무에서 가장 먼저 하는 작업 순서</h2><h3 id="1️⃣-Cluster-생성"><a href="#1️⃣-Cluster-생성" class="headerlink" title="1️⃣ Cluster 생성"></a>1️⃣ Cluster 생성</h3><ul><li>Compute → Create Cluster</li><li>Auto Termination 설정 (비용 절감 필수)</li><li>Small VM으로 시작 권장</li></ul><h3 id="2️⃣-Notebook-생성"><a href="#2️⃣-Notebook-생성" class="headerlink" title="2️⃣ Notebook 생성"></a>2️⃣ Notebook 생성</h3><ul><li>Workspace → Create → Notebook</li><li>Python &#x2F; SQL &#x2F; Scala 선택</li></ul><h3 id="3️⃣-데이터-연결"><a href="#3️⃣-데이터-연결" class="headerlink" title="3️⃣ 데이터 연결"></a>3️⃣ 데이터 연결</h3><ul><li>Azure Blob Storage &#x2F; ADLS Gen2</li><li>Managed Identity 또는 Access Connector 사용</li></ul><h3 id="4️⃣-Delta-Lake-사용"><a href="#4️⃣-Delta-Lake-사용" class="headerlink" title="4️⃣ Delta Lake 사용"></a>4️⃣ Delta Lake 사용</h3><ul><li>ACID 트랜잭션</li><li>Time Travel</li><li>Schema Enforcement</li></ul><hr><h2 id="7-비용-관리-실전-팁-⭐"><a href="#7-비용-관리-실전-팁-⭐" class="headerlink" title="7. 비용 관리 실전 팁 ⭐"></a>7. 비용 관리 실전 팁 ⭐</h2><ul><li>⏱ <strong>Auto Termination 필수</strong></li><li>❌ 사용 안 할 때 Cluster 즉시 종료</li><li>📦 Workspace는 유지 (비용 거의 없음)</li><li>🧹 실습 종료 후 Resource Group 삭제</li></ul><hr><h2 id="8-Azure-Databricks-실무-구조-요약"><a href="#8-Azure-Databricks-실무-구조-요약" class="headerlink" title="8. Azure + Databricks 실무 구조 요약"></a>8. Azure + Databricks 실무 구조 요약</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Azure Subscription</span><br><span class="line"> └── Resource Group</span><br><span class="line">     └── Databricks Workspace</span><br><span class="line">         ├── Clusters</span><br><span class="line">         ├── Notebooks</span><br><span class="line">         ├── Jobs</span><br><span class="line">         └── Unity Catalog</span><br></pre></td></tr></table></figure><hr><h2 id="9-추천-학습-흐름"><a href="#9-추천-학습-흐름" class="headerlink" title="9. 추천 학습 흐름"></a>9. 추천 학습 흐름</h2><ol><li>Azure Account 생성</li><li>Databricks Workspace 1개 생성</li><li>Cluster 생성</li><li>Notebook 실습</li><li>ADLS + Delta Lake 연동</li><li>Jobs &amp; Automation</li></ol><hr><h2 id="마무리"><a href="#마무리" class="headerlink" title="마무리"></a>마무리</h2><p>Azure Databricks는<br>👉 <strong>Azure 인프라 + Databricks 데이터 플랫폼</strong>의 결합입니다.</p><p>처음에는 Azure Portal → Workspace 생성이 핵심이고,<br>그 이후부터는 <strong>Databricks UI 중심으로 작업</strong>하게 됩니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-Azure-Free-Account-생성&quot;&gt;&lt;a href=&quot;#1-Azure-Free-Account-생성&quot; class=&quot;headerlink&quot; title=&quot;1. Azure Free Account 생성&quot;&gt;&lt;/a&gt;1. Azure Free Ac</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-2</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-2/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-2/</id>
    <published>2025-12-12T15:17:42.000Z</published>
    <updated>2025-12-12T15:18:14.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Databricks-핵심-기능-정리"><a href="#Databricks-핵심-기능-정리" class="headerlink" title="Databricks 핵심 기능 정리"></a>Databricks 핵심 기능 정리</h1><h2 id="1-Databricks란-무엇인가"><a href="#1-Databricks란-무엇인가" class="headerlink" title="1. Databricks란 무엇인가?"></a>1. Databricks란 무엇인가?</h2><p><strong>Databricks</strong>는 Apache Spark 위에 구축된 <strong>엔터프라이즈급 Lakehouse 플랫폼</strong>이다.<br>단순한 Spark 실행 환경이 아니라, <strong>설계 · 개발 · 운영 · 보안 · 자동화</strong>까지 포함한 <strong>완성형 데이터 플랫폼</strong>이다.</p><ul><li>Data Lake + Data Warehouse &#x3D; <strong>Lakehouse</strong></li><li>Medallion Architecture (Bronze &#x2F; Silver &#x2F; Gold) 지원</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Databricks &#x3D; Spark 기반 <strong>플랫폼</strong></li><li>Spark 단독으로 부족한 기능을 보완</li></ul><hr><h2 id="2-Databricks의-핵심-가치"><a href="#2-Databricks의-핵심-가치" class="headerlink" title="2. Databricks의 핵심 가치"></a>2. Databricks의 핵심 가치</h2><p>Databricks는 아래 질문에 대한 해답이다.</p><blockquote><p>“Spark로 엔터프라이즈 데이터 플랫폼을 만들려면 무엇이 더 필요한가?”</p></blockquote><p>정답:</p><ul><li>ACID</li><li>Metadata 관리</li><li>보안</li><li>클러스터 운영</li><li>성능 최적화</li><li>자동화</li></ul><hr><h2 id="3-Databricks-주요-기능-시험-핵심-⭐⭐⭐"><a href="#3-Databricks-주요-기능-시험-핵심-⭐⭐⭐" class="headerlink" title="3. Databricks 주요 기능 (시험 핵심 ⭐⭐⭐)"></a>3. Databricks 주요 기능 (시험 핵심 ⭐⭐⭐)</h2><h3 id="3-1-Cloud-Native-Spark"><a href="#3-1-Cloud-Native-Spark" class="headerlink" title="3.1 Cloud-Native Spark"></a>3.1 Cloud-Native Spark</h3><ul><li>Spark를 <strong>Hadoop(YARN) 의존 없이</strong> 실행</li><li>Cloud 환경에 최적화된 Spark 런타임 제공</li></ul><p>📌 시험 포인트</p><ul><li>Databricks Spark &#x3D; Hadoop 기반 ❌</li><li>Databricks Spark &#x3D; Cloud Native ⭕</li></ul><hr><h3 id="3-2-Secure-Cloud-Storage-Integration"><a href="#3-2-Secure-Cloud-Storage-Integration" class="headerlink" title="3.2 Secure Cloud Storage Integration"></a>3.2 Secure Cloud Storage Integration</h3><ul><li>Cloud Storage와 <strong>네이티브 통합</strong><ul><li>AWS: S3</li><li>Azure: ADLS Gen2</li><li>GCP: GCS</li></ul></li><li>IAM &#x2F; Role 기반 보안 연동</li></ul><p>📌 시험 포인트</p><ul><li>Databricks는 <strong>스토리지를 소유하지 않음</strong></li><li>Storage는 Cloud Native</li></ul><hr><h3 id="3-3-ACID-트랜잭션-–-Delta-Lake-⭐⭐⭐"><a href="#3-3-ACID-트랜잭션-–-Delta-Lake-⭐⭐⭐" class="headerlink" title="3.3 ACID 트랜잭션 – Delta Lake ⭐⭐⭐"></a>3.3 ACID 트랜잭션 – Delta Lake ⭐⭐⭐</h3><ul><li><strong>Delta Lake</strong> &#x3D; 오픈소스 스토리지 레이어</li><li>Spark + Delta Lake → ACID 보장</li></ul><p>지원 기능:</p><ul><li>Atomicity</li><li>Consistency</li><li>Isolation</li><li>Durability</li><li>Time Travel</li><li>Schema Enforcement &#x2F; Evolution</li></ul><p>📌 시험 포인트</p><ul><li>ACID는 Spark 기본 기능 ❌</li><li>ACID는 <strong>Delta Lake 통해 제공</strong> ⭕</li></ul><hr><h3 id="3-4-Unity-Catalog-Metadata-Security-⭐⭐⭐"><a href="#3-4-Unity-Catalog-Metadata-Security-⭐⭐⭐" class="headerlink" title="3.4 Unity Catalog (Metadata + Security) ⭐⭐⭐"></a>3.4 Unity Catalog (Metadata + Security) ⭐⭐⭐</h3><p><strong>Unity Catalog</strong>는 Databricks의 핵심 엔터프라이즈 기능</p><p>기능:</p><ul><li>중앙 메타데이터 관리</li><li>테이블 &#x2F; 뷰 &#x2F; 컬럼 단위 권한</li><li>사용자 &#x2F; 그룹 관리</li><li>감사 로그 (Audit)</li></ul><p>📌 시험 포인트</p><ul><li>Unity Catalog &#x3D; Metadata + Governance + Security</li></ul><hr><h3 id="3-5-Cluster-Management"><a href="#3-5-Cluster-Management" class="headerlink" title="3.5 Cluster Management"></a>3.5 Cluster Management</h3><ul><li>Databricks UI에서 직접:<ul><li>클러스터 생성 &#x2F; 삭제</li><li>Auto Scaling</li><li>Job Cluster &#x2F; All-Purpose Cluster</li></ul></li></ul><p>📌 시험 포인트</p><ul><li>Spark 자체는 클러스터 관리 ❌</li><li>Databricks는 클러스터 관리 ⭕</li></ul><hr><h3 id="3-6-Photon-Query-Engine-⭐⭐⭐"><a href="#3-6-Photon-Query-Engine-⭐⭐⭐" class="headerlink" title="3.6 Photon Query Engine ⭐⭐⭐"></a>3.6 Photon Query Engine ⭐⭐⭐</h3><ul><li>Databricks 전용 <strong>Query Acceleration Engine</strong></li><li>Spark SQL &#x2F; DataFrame 자동 가속</li><li>설정 ON&#x2F;OFF만으로 사용 가능</li></ul><p>📌 시험 포인트</p><ul><li>Photon &#x3D; Transparent Acceleration</li><li>SQL 성능 관련 문제 단골</li></ul><hr><h3 id="3-7-Notebooks-Workspace"><a href="#3-7-Notebooks-Workspace" class="headerlink" title="3.7 Notebooks &amp; Workspace"></a>3.7 Notebooks &amp; Workspace</h3><ul><li>통합 개발 환경 (IDE)</li><li>언어 지원:<ul><li>SQL</li><li>Python</li><li>Scala</li><li>R</li></ul></li><li>Git 연동 지원</li></ul><p>📌 시험 포인트</p><ul><li>Databricks Notebook &#x3D; 협업 중심</li></ul><hr><h3 id="3-8-Administration-Security-Controls"><a href="#3-8-Administration-Security-Controls" class="headerlink" title="3.8 Administration &amp; Security Controls"></a>3.8 Administration &amp; Security Controls</h3><ul><li>Role-Based Access Control (RBAC)</li><li>정책 기반 접근 제어</li><li>Audit Log</li></ul><p>📌 시험 포인트</p><ul><li>엔터프라이즈 보안 &#x3D; Databricks 강점</li></ul><hr><h3 id="3-9-Optimized-Spark-Runtime"><a href="#3-9-Optimized-Spark-Runtime" class="headerlink" title="3.9 Optimized Spark Runtime"></a>3.9 Optimized Spark Runtime</h3><ul><li>Vanilla Spark 대비:<ul><li>성능 최적화</li><li>메모리 &#x2F; 쿼리 튜닝</li></ul></li><li>동일 코드 → Databricks에서 더 빠름</li></ul><p>📌 시험 포인트</p><ul><li>Databricks Runtime ≠ Apache Spark OSS</li></ul><hr><h3 id="3-10-Automation-Tools"><a href="#3-10-Automation-Tools" class="headerlink" title="3.10 Automation Tools"></a>3.10 Automation Tools</h3><ul><li>REST API</li><li>CLI</li><li>SDK</li><li>Terraform 연동</li></ul><p>📌 시험 포인트</p><ul><li>Databricks &#x3D; DevOps 친화적</li></ul><hr><h2 id="4-Databricks-지원-Cloud-플랫폼"><a href="#4-Databricks-지원-Cloud-플랫폼" class="headerlink" title="4. Databricks 지원 Cloud 플랫폼"></a>4. Databricks 지원 Cloud 플랫폼</h2><p>Databricks는 <strong>동일한 기능</strong>을 모든 Cloud에서 제공</p><table><thead><tr><th>Cloud</th><th>Databricks 제공</th></tr></thead><tbody><tr><td>AWS</td><td>Databricks on AWS</td></tr><tr><td>Azure</td><td>Azure Databricks</td></tr><tr><td>GCP</td><td>Databricks on GCP</td></tr></tbody></table><p>📌 시험 포인트</p><ul><li>Databricks 기능은 <strong>Cloud 간 동일</strong></li><li>선택 기준은 조직의 Cloud 전략</li></ul><hr><h2 id="5-Cloud별-주요-연동-서비스-시험-단골"><a href="#5-Cloud별-주요-연동-서비스-시험-단골" class="headerlink" title="5. Cloud별 주요 연동 서비스 (시험 단골)"></a>5. Cloud별 주요 연동 서비스 (시험 단골)</h2><h3 id="CI-CD"><a href="#CI-CD" class="headerlink" title="CI&#x2F;CD"></a>CI&#x2F;CD</h3><ul><li>Azure: Azure DevOps, GitHub</li><li>AWS: CodeBuild, CodePipeline</li><li>GCP: Cloud Build</li></ul><h3 id="Data-Warehouse"><a href="#Data-Warehouse" class="headerlink" title="Data Warehouse"></a>Data Warehouse</h3><ul><li>Azure: Synapse</li><li>AWS: Redshift</li><li>GCP: BigQuery</li></ul><h3 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h3><ul><li>Azure: Event Hubs</li><li>AWS: Kinesis</li><li>GCP: Pub&#x2F;Sub</li></ul><p>📌 시험 포인트</p><ul><li>Databricks는 Cloud 생태계와 강하게 통합됨</li></ul><hr><h2 id="6-Spark-vs-Databricks-시험-비교-문제"><a href="#6-Spark-vs-Databricks-시험-비교-문제" class="headerlink" title="6. Spark vs Databricks (시험 비교 문제)"></a>6. Spark vs Databricks (시험 비교 문제)</h2><table><thead><tr><th>구분</th><th>Spark</th><th>Databricks</th></tr></thead><tbody><tr><td>ACID</td><td>❌</td><td>⭕ (Delta)</td></tr><tr><td>Metadata</td><td>제한적</td><td>Unity Catalog</td></tr><tr><td>Cluster 관리</td><td>❌</td><td>⭕</td></tr><tr><td>성능</td><td>기본</td><td>Photon</td></tr><tr><td>Automation</td><td>제한적</td><td>풍부</td></tr><tr><td>Enterprise Ready</td><td>❌</td><td>⭕</td></tr></tbody></table><hr><h2 id="7-시험-한-줄-요약-⭐⭐⭐"><a href="#7-시험-한-줄-요약-⭐⭐⭐" class="headerlink" title="7. 시험 한 줄 요약 ⭐⭐⭐"></a>7. 시험 한 줄 요약 ⭐⭐⭐</h2><ul><li>Databricks &#x3D; Spark + Delta Lake + Unity Catalog</li><li>ACID는 Delta Lake</li><li>Governance는 Unity Catalog</li><li>성능은 Photon</li><li>Databricks는 <strong>플랫폼</strong>, Spark는 <strong>엔진</strong></li></ul><hr><h3 id="✅-최종-암기-문장"><a href="#✅-최종-암기-문장" class="headerlink" title="✅ 최종 암기 문장"></a>✅ 최종 암기 문장</h3><blockquote><p>Databricks는 Apache Spark 기반의 Cloud Native Lakehouse 플랫폼으로,<br>Delta Lake를 통해 ACID를 제공하고 Unity Catalog를 통해 메타데이터와 보안을 관리한다.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Databricks-핵심-기능-정리&quot;&gt;&lt;a href=&quot;#Databricks-핵심-기능-정리&quot; class=&quot;headerlink&quot; title=&quot;Databricks 핵심 기능 정리&quot;&gt;&lt;/a&gt;Databricks 핵심 기능 정리&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-1</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-1/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-1/</id>
    <published>2025-12-12T14:54:44.000Z</published>
    <updated>2025-12-12T15:01:56.698Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Apache-Spark란"><a href="#1-Apache-Spark란" class="headerlink" title="1. Apache Spark란?"></a>1. Apache Spark란?</h2><p><strong>Apache Spark</strong>는<br>👉 <strong>분산 클러스터 환경에서 대규모 데이터를 빠르게 처리하기 위한 데이터 처리 엔진(Engine)</strong> 입니다.</p><p>Spark는 다음 작업을 <strong>하나의 통합된 프레임워크</strong>에서 처리할 수 있습니다.</p><ul><li>배치 데이터 처리 (Batch Processing)</li><li>스트리밍 데이터 처리 (Stream Processing)</li><li>머신러닝 (Machine Learning)</li><li>그래프 처리 (Graph Processing)</li><li>SQL 기반 데이터 분석</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark는 <strong>데이터베이스가 아니다</strong></li><li>Spark는 <strong>스토리지 시스템이 아니다</strong></li><li>Spark는 <strong>데이터 처리 엔진(Processing Engine)</strong> 이다</li></ul><hr><h2 id="2-Spark가-제공하는-주요-API-Unified-Framework"><a href="#2-Spark가-제공하는-주요-API-Unified-Framework" class="headerlink" title="2. Spark가 제공하는 주요 API (Unified Framework)"></a>2. Spark가 제공하는 주요 API (Unified Framework)</h2><p>Spark는 하나의 엔진 위에서 여러 API를 제공합니다.</p><h3 id="1-Spark-SQL-DataFrame-API"><a href="#1-Spark-SQL-DataFrame-API" class="headerlink" title="(1) Spark SQL &amp; DataFrame API"></a>(1) Spark SQL &amp; DataFrame API</h3><ul><li>SQL 기반 데이터 처리</li><li>ANSI SQL 호환</li><li>가장 많이 사용됨 ⭐⭐⭐</li></ul><h3 id="2-Structured-Streaming"><a href="#2-Structured-Streaming" class="headerlink" title="(2) Structured Streaming"></a>(2) Structured Streaming</h3><ul><li>스트리밍 데이터를 <strong>배치처럼 처리</strong></li><li>Kafka, Kinesis 등과 연동</li></ul><h3 id="3-MLlib"><a href="#3-MLlib" class="headerlink" title="(3) MLlib"></a>(3) MLlib</h3><ul><li>머신러닝 라이브러리</li><li>분류, 회귀, 클러스터링 등 제공</li></ul><h3 id="4-GraphX"><a href="#4-GraphX" class="headerlink" title="(4) GraphX"></a>(4) GraphX</h3><ul><li>그래프 기반 연산 (노드, 엣지)</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>RDD API는 <strong>존재하지만 권장되지 않음</strong></li><li>시험에서는 <strong>DataFrame &#x2F; Spark SQL 중심</strong></li></ul><hr><h2 id="3-Spark-아키텍처-Spark-Stack"><a href="#3-Spark-아키텍처-Spark-Stack" class="headerlink" title="3. Spark 아키텍처 (Spark Stack)"></a>3. Spark 아키텍처 (Spark Stack)</h2><h3 id="Spark-동작-구조-아래-→-위"><a href="#Spark-동작-구조-아래-→-위" class="headerlink" title="Spark 동작 구조 (아래 → 위)"></a>Spark 동작 구조 (아래 → 위)</h3><ol><li><p><strong>Distributed Storage</strong></p><ul><li>HDFS</li><li>Amazon S3</li><li>Azure Data Lake Storage (ADLS)</li><li>Google Cloud Storage (GCS)</li></ul></li><li><p><strong>Compute Cluster</strong></p><ul><li>여러 대의 서버로 구성된 클러스터</li></ul></li><li><p><strong>Resource Manager (Cluster Manager)</strong></p><ul><li>YARN</li><li>Kubernetes</li><li>Standalone</li><li>Mesos (과거)</li></ul></li><li><p><strong>Spark Framework</strong></p><ul><li>Spark Core</li><li>Spark SQL</li><li>Streaming</li><li>MLlib</li><li>GraphX</li></ul></li><li><p><strong>Programming API &#x2F; DSL</strong></p><ul><li>Scala</li><li>Java</li><li>Python (PySpark)</li><li>R</li></ul></li></ol><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark는 <strong>반드시 Cluster Manager 위에서 실행</strong></li><li>Spark는 <strong>스토리지와 분리된 구조</strong></li></ul><hr><h2 id="4-Spark-Core-언어-지원"><a href="#4-Spark-Core-언어-지원" class="headerlink" title="4. Spark Core &amp; 언어 지원"></a>4. Spark Core &amp; 언어 지원</h2><h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><ul><li>Spark의 핵심 실행 엔진</li><li>RDD 기반 API 포함</li></ul><h3 id="지원-언어"><a href="#지원-언어" class="headerlink" title="지원 언어"></a>지원 언어</h3><ul><li>Scala (Spark의 원래 언어)</li><li>Java</li><li>Python (PySpark) ⭐</li><li>R</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark Core &#x3D; RDD API</li><li>실무 &amp; 시험에서는 <strong>DataFrame API 사용</strong></li></ul><hr><h2 id="5-Spark가-인기-있는-이유"><a href="#5-Spark가-인기-있는-이유" class="headerlink" title="5. Spark가 인기 있는 이유"></a>5. Spark가 인기 있는 이유</h2><h3 id="1-높은-추상화-High-Abstraction"><a href="#1-높은-추상화-High-Abstraction" class="headerlink" title="(1) 높은 추상화 (High Abstraction)"></a>(1) 높은 추상화 (High Abstraction)</h3><ul><li>분산 처리 복잡성 숨김</li><li>개발자는 SQL 또는 DataFrame만 작성</li></ul><h3 id="2-사용하기-쉬움"><a href="#2-사용하기-쉬움" class="headerlink" title="(2) 사용하기 쉬움"></a>(2) 사용하기 쉬움</h3><ul><li>SQL 기반 접근 가능</li><li>다양한 언어 지원</li></ul><h3 id="3-Unified-Platform"><a href="#3-Unified-Platform" class="headerlink" title="(3) Unified Platform"></a>(3) Unified Platform</h3><ul><li>SQL + Batch + Streaming + ML + Graph</li><li>하나의 엔진에서 모두 처리</li></ul><h3 id="4-Open-Source-풍부한-생태계"><a href="#4-Open-Source-풍부한-생태계" class="headerlink" title="(4) Open Source &amp; 풍부한 생태계"></a>(4) Open Source &amp; 풍부한 생태계</h3><ul><li>수많은 기업 사용</li><li>Fortune 500의 약 80% 사용</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark의 핵심 장점 &#x3D; <strong>Unified + Abstraction</strong></li></ul><hr><h2 id="6-Apache-Spark가-“아닌-것”-중요-⭐⭐⭐"><a href="#6-Apache-Spark가-“아닌-것”-중요-⭐⭐⭐" class="headerlink" title="6. Apache Spark가 “아닌 것” (중요 ⭐⭐⭐)"></a>6. Apache Spark가 “아닌 것” (중요 ⭐⭐⭐)</h2><p>Spark는 강력하지만 <strong>엔터프라이즈 솔루션으로는 부족한 부분</strong>이 있음</p><h3 id="1-자체-스토리지-❌"><a href="#1-자체-스토리지-❌" class="headerlink" title="(1) 자체 스토리지 ❌"></a>(1) 자체 스토리지 ❌</h3><ul><li>Spark는 데이터를 저장하지 않음</li><li>항상 외부 스토리지 필요 (S3, HDFS 등)</li></ul><h3 id="2-ACID-트랜잭션-❌"><a href="#2-ACID-트랜잭션-❌" class="headerlink" title="(2) ACID 트랜잭션 ❌"></a>(2) ACID 트랜잭션 ❌</h3><ul><li>Spark 자체는 ACID 보장 안 함</li><li>Atomicity, Consistency, Isolation, Durability 미지원</li></ul><h3 id="3-중앙-메타데이터-카탈로그-❌"><a href="#3-중앙-메타데이터-카탈로그-❌" class="headerlink" title="(3) 중앙 메타데이터 카탈로그 ❌"></a>(3) 중앙 메타데이터 카탈로그 ❌</h3><ul><li>단순한 내부 카탈로그만 존재</li><li>Enterprise-grade Catalog 없음</li></ul><h3 id="4-클러스터-관리-❌"><a href="#4-클러스터-관리-❌" class="headerlink" title="(4) 클러스터 관리 ❌"></a>(4) 클러스터 관리 ❌</h3><ul><li>Spark로 클러스터 생성&#x2F;삭제 불가</li><li>Cluster Manager의 역할</li></ul><h3 id="5-자동화-도구-부족-❌"><a href="#5-자동화-도구-부족-❌" class="headerlink" title="(5) 자동화 도구 부족 ❌"></a>(5) 자동화 도구 부족 ❌</h3><ul><li>배포, 모니터링, 운영 자동화 기능 미흡</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark는 <strong>엔진이지 플랫폼이 아니다</strong></li></ul><hr><h2 id="7-왜-Spark-“플랫폼”이-필요한가"><a href="#7-왜-Spark-“플랫폼”이-필요한가" class="headerlink" title="7. 왜 Spark “플랫폼”이 필요한가?"></a>7. 왜 Spark “플랫폼”이 필요한가?</h2><p>엔터프라이즈 환경에서는 다음이 필요함:</p><ul><li>ACID 보장</li><li>메타데이터 관리</li><li>보안</li><li>자동화</li><li>운영 편의성</li></ul><p>👉 그래서 <strong>Spark + Platform</strong> 조합이 필요</p><hr><h2 id="8-대표적인-Spark-플랫폼들-시험-단골"><a href="#8-대표적인-Spark-플랫폼들-시험-단골" class="headerlink" title="8. 대표적인 Spark 플랫폼들 (시험 단골)"></a>8. 대표적인 Spark 플랫폼들 (시험 단골)</h2><h3 id="1-Cloudera-Hadoop"><a href="#1-Cloudera-Hadoop" class="headerlink" title="(1) Cloudera Hadoop"></a>(1) Cloudera Hadoop</h3><ul><li>온프레미스 Hadoop 플랫폼</li><li>YARN 기반</li><li>Spark 실행 가능</li></ul><h3 id="2-Amazon-EMR"><a href="#2-Amazon-EMR" class="headerlink" title="(2) Amazon EMR"></a>(2) Amazon EMR</h3><ul><li>AWS 관리형 Hadoop&#x2F;Spark</li><li>내부적으로 Hadoop + YARN 사용</li></ul><h3 id="3-Azure-HDInsight"><a href="#3-Azure-HDInsight" class="headerlink" title="(3) Azure HDInsight"></a>(3) Azure HDInsight</h3><ul><li>Azure 기반 Hadoop&#x2F;Spark 서비스</li></ul><h3 id="4-Google-Dataproc"><a href="#4-Google-Dataproc" class="headerlink" title="(4) Google Dataproc"></a>(4) Google Dataproc</h3><ul><li>GCP 기반 Hadoop&#x2F;Spark 서비스</li></ul><p>📌 공통점</p><ul><li><strong>모두 Hadoop 기반</strong></li><li><strong>YARN 사용</strong></li></ul><hr><h2 id="9-Databricks의-차별점-⭐⭐⭐"><a href="#9-Databricks의-차별점-⭐⭐⭐" class="headerlink" title="9. Databricks의 차별점 ⭐⭐⭐"></a>9. Databricks의 차별점 ⭐⭐⭐</h2><h3 id="Databricks-특징"><a href="#Databricks-특징" class="headerlink" title="Databricks 특징"></a>Databricks 특징</h3><ul><li><strong>Hadoop 기반 아님</strong></li><li><strong>YARN 사용 안 함</strong></li><li>Spark 전용 Cloud Native 플랫폼</li><li>클라우드 최적화</li></ul><h3 id="Databricks는"><a href="#Databricks는" class="headerlink" title="Databricks는?"></a>Databricks는?</h3><ul><li>Spark + ACID + Metadata + Automation 제공</li><li>Delta Lake 기반</li><li>Medallion Architecture 구현 가능</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Databricks &#x3D; <strong>Pure Spark Platform</strong></li><li>On-Premise ❌, Cloud Only ⭕</li></ul><hr><h2 id="10-시험에-자주-나오는-한-줄-요약"><a href="#10-시험에-자주-나오는-한-줄-요약" class="headerlink" title="10. 시험에 자주 나오는 한 줄 요약"></a>10. 시험에 자주 나오는 한 줄 요약</h2><ul><li>Spark는 <strong>데이터 처리 엔진</strong></li><li>Spark는 <strong>스토리지를 포함하지 않는다</strong></li><li>Spark는 <strong>ACID를 기본 제공하지 않는다</strong></li><li>Spark는 <strong>YARN &#x2F; Kubernetes 위에서 실행</strong></li><li>Databricks는 <strong>Spark 기반 엔터프라이즈 플랫폼</strong></li><li>Hadoop 기반 플랫폼 ≠ Databricks</li></ul><hr><h2 id="11-Medallion-Architecture와-Spark-보너스"><a href="#11-Medallion-Architecture와-Spark-보너스" class="headerlink" title="11. Medallion Architecture와 Spark (보너스)"></a>11. Medallion Architecture와 Spark (보너스)</h2><ul><li>Spark 단독 ❌</li><li>Spark + Delta Lake ⭕</li><li>Databricks에서 Medallion Architecture 구현 가능<ul><li>Bronze</li><li>Silver</li><li>Gold</li></ul></li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Medallion Architecture &#x3D; Databricks + Delta Lake</li></ul><hr><h3 id="✅-마무리-한-문장-시험용"><a href="#✅-마무리-한-문장-시험용" class="headerlink" title="✅ 마무리 한 문장 (시험용)"></a>✅ 마무리 한 문장 (시험용)</h3><blockquote><p>Apache Spark는 분산 환경에서 대규모 데이터를 처리하기 위한 <strong>Unified Data Processing Engine</strong>이며,<br>엔터프라이즈 환경에서는 Databricks 같은 <strong>플랫폼과 함께 사용된다</strong>.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-Apache-Spark란&quot;&gt;&lt;a href=&quot;#1-Apache-Spark란&quot; class=&quot;headerlink&quot; title=&quot;1. Apache Spark란?&quot;&gt;&lt;/a&gt;1. Apache Spark란?&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Apache</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>Interview_Q&amp;A_1</title>
    <link href="https://kish191919.github.io/2025/12/12/Interview-Q-A-1/"/>
    <id>https://kish191919.github.io/2025/12/12/Interview-Q-A-1/</id>
    <published>2025-12-12T14:51:06.000Z</published>
    <updated>2025-12-12T14:53:32.603Z</updated>
    
    <content type="html"><![CDATA[<p><a id="top"></a></p><style>.toc-grid{  display:grid;  grid-template-columns:1fr 1fr; /* 화면 반반 */  gap:6px 20px;  align-items:start;}@media (max-width:760px){  .toc-grid{ grid-template-columns:1fr; }}/* 섹션 타이틀은 두 칼럼 전체를 차지 */.toc-section{  grid-column:1 / -1;  margin:12px 0 6px;  font-size:1.1rem;  font-weight:700;}/* 링크 공통 스타일 */.toc-grid a{ display:block; padding:2px 0; word-break:keep-all; }</style><div class="toc-grid"><h3 class="toc-section">내 소개 및 전반적인 질문</h3><a href="#a1">나의 소개</a><a href="#a2">왜 이직하니?</a><a href="#a4">보잉이란 그리고 지원동기?</a><a href="#recent-project">최근 프로젝트</a><a href="#issue-data-volume">이슈 - 데이터 볼륨증가 3~4TB</a><a href="#issue-schema-change">이슈 - 스키마 변경</a><a href="#data-warehouse-tech">데이터 웨어하우스 설계</a><a href="#data-warehouse-problem">데이터 웨어하우스 지연속도 문제 및 해결</a><a href="#dq-multi-point-good">데이터 품질관리 - 여러포인트 검증 &amp; good data</a><a href="#etl-design-ca7-glue">ETL (오케스트레이션) 설계 - CA7와 Glue 이용</a><h3 class="toc-section">데이터 품질과 보안</h3><a href="#dq-unit-test">데이터 품질관리 - 유닛테스트</a><a href="#security-compliance">보안과 규정 (e.g. AWS, Azure)</a><h3 class="toc-section">기술 내용</h3><a href="#alteryx-usage">Alteryx 사용기간 및 경험</a><a href="#teradata-usage">테라데이터 사용기간 및 경험</a><a href="#teradata-definition">테라데이터 정의 및 사용사례</a><a href="#teradata-performance">테라데이터가 높은 성능을 발휘할수 있는 이유</a><a href="#teradata-pi">테라데이터의 Primary Index (PI) 역할</a><a href="#teradata-skew">테라데이터 데이터 불균형을 어떻게 해결?</a><a href="#teradata-secondary-index">테라데이터의 Secondary Index 이란</a><a href="#teradata-partition">Teradata 파티션</a><a href="#teradata-troubleshooting">테라데이터 문제해결</a><a href="#alteryx-teradata">Alteryx와 Teradata 사용</a><a href="#neo4j">Neo4J 관련해서</a><h3 class="toc-section">행동 규정</h3><a href="#manager-absence-decision">매니저가 부재시 결정해야할 경우</a><a href="#team-disagreement-sla">팀원 의견 다름 - SLA 20분 지연 - 품질해결</a><a href="#manager-disagreement-refresh-partition">상사 의견 다름 - refresh only 변경된 파티션만</a><a href="#diffent-personality">다른 성향의 사람과 협동</a><a href="#helped-teammate">팀동료 성공시키기</a><a href="#urgent-vs-important">급한일과 중요한일의 우선순위 - 급한것 먼저</a><a href="#tech-challenge-1-5tb">기술 문제 도전 - 1.5TB 처리</a><a href="#improvement-large-data">개선 사례 - 큰 데이터 처리 (위 동일)</a><a href="#production-connect-stop">생산 문제 - 커넥트 Stop</a><a href="#process-improvement-connect-stop">자발적 프로세스 개선 - 커넥트 Stop</a><a href="#kafka-realtime-connect-check">Kafka 실시간 데이터에서 고려할 부분 - 커넥트 상태확인</a><a href="#quality-issue-currency">품질 문제 - 통화단위 에러</a><a href="#failure-column-validation">실패,실수 - 컬럼검증X, 통화단위 에러</a><a href="#project-delay-schema-change">프로젝트 지연 - 스키마 변경</a><a href="#team-lead-source-missing">팀 리드 &amp; 솔선 - 소스 입력 안됨</a><a href="#team-lead-datatype-mismatch">리더싶 사례 - data type mismatch</a><a href="#above-and-beyond-responsibility">지시받지않은 일 - add load_date / 상사 의견다름 동일</a><a href="#tight-schedule-pressure">타이트한 스케줄 &amp; 압박 - 일 나누고 대화, 트랙</a><a href="#multi-national-collaboration">여러 민족 같이 근무 - sync 미팅, 미팅요약</a><a href="#cross-team-collaboration">다른 팀 협업 - 용어 통일</a><a href="#customer-request">고객이 마지막에 변경요청시 - 스키마변경</a><a href="#customer-request-offen">고객이 자주 변경사항을 요청할때 - 요청을 그룹핑함</a><a href="#non-technical">비개발자에게 기술적인 내용을 쉽게 설명할 수 있나요?</a><a href="#issue-spark-memory">이슈 - Spark memory 문제</a><h3 class="toc-section">ETL / 오케스트레이션</h3><a href="#data-orchestration">Data Orchestration - (CA7, Glue, Airflow)</a><a href="#etl-pipeline-optimization">ETL pipeline 최적화 - SLA 6시</a><a href="#aws-glue-experience">AWS Glue 사용경험 - ETL services</a><a href="#no-current-glue">현재 glue를 사용하지는 않는다</a><h3 class="toc-section">데이터 웨어하우스</h3><a href="#redshift-intro">Redshift 란?</a><a href="#redshift-columnar">Redshift Columnar Storage</a><a href="#snowflake-advantages">Snowflake 장점 (zero-copy, time travel)</a><a href="#databricks-experience">Databricks 사용경험 - Anomaly detection</a><a href="#partition-strategy">파티션 전략</a><a href="#oracle-range-partitioning">데이터 모델링 & Architecture - 오라클 range partitioning<</a><a href="#data-normalization">정규화 vs 비정규화</a><a href="#star-snowflake-schema">Star &amp; Snowflake 스키마</a><h3 class="toc-section">파이썬/스파크/하둡</h3><a href="#python-sql-spark">Python, SQL, Spark, PySpark</a><a href="#spark-hadoop-ingestion">경험 - 스파크/하둡 ingestion</a><a href="#aws-experience">경험 - AWS 많이 사용했니?</a><a href="#strengths-weaknesses">장점과 단점</a><a href="#stress">스트레스를 어떻게 풉니까?</a><a href="#motto">삶의 모토는?</a><a href="#powertech">파워텍에서 머신러닝 모델 사용</a><a href="#final-remarks">마지막으로 하고 싶은 말</a><a href="#python-typ">파이썬 타입비교</a></div><h3 id="a1">나의 소개</h3>Thank you for having me for an interview and my name is Sunghwan ki but you can go by DannyI work as Data Engineer with 6 years experience in building ETL process, especially in the financial industry.Currently I lead the projects that use the Kafka, Oracle, and Spark where I focus on near real-time data processing and optimization.I primarily use Python to build data pipelines, and recently, I completed on a project where I built a data warehouse using AWS Glue and Redshift.Before joining PNC, I spent roughly seven years working in data analytics, where I primarily used Tableau and MySql to analyze the data<p>To better performance, I completed the Master’s degree in Data Science last year and also I hold the AWS certifications and continue to pursue additional cloud-related credentials to further strengthen my expertise</p><p><a href="#top">맨 위로</a></p><h3 id="a2">왜 이직하니?</h3>I’ve truly enjoyed my time at PNC and  I’ve spent over six years working on meaningful projects and improved my technical skills.  Now I feel I ready for a new challenge that allows me to expand further.  Technology is evolving faster than ever, and I want to keep learning and developing new skills.for me It’s not about leaving something behind — it’s about taking the next step toward work I’m truly passionate about.<p><a href="#top">맨 위로</a></p><h3 id="a4">보잉이란 그리고 지원동기</h3>Boeing is one of the world’s largest aerospace and defense companies. It designs and builds commercial airplanes like the 737 and 777.Also, Boeing’s work connects people, supports global transportation, and contributes to national security and space exploration.That's why I applied to this company to builds products with real-world impact.<p><a href="#top">맨 위로</a></p><h3 id="recent-project">Recent project (최근 프로젝트)</h3>Currently, I am working on building a near real time pipeline that ingests kafka topic data into Oracle Exadata and then into Hadoop platform.In the past, stakeholders had to rely on the previous day’s data to make decisions. But now with this new pipeline, data from Kafka is ingested into Hadoop in every 10 minutes and then visualized through Tableau dashboards.This project significantly reduced data latency and helped business team to make faster decision.<p><a href="#top">맨 위로</a></p><h3 id="issue-data-volume">이슈 - 데이터 볼륨증가 3~4TB </h3>One of the biggest challenges I faced recently was with a Kafka-to-Hadoop data pipeline, where Oracle Exadata was used as a staging area.<p>Initially, the volume of data coming from Kafka was about 1 TB per day, but it suddenly increased to 3 or 4 TB per day. Even though the data was automatically deleted after being loaded into Hadoop, new data was coming in faster than it could be deleted, so Exadata started running out of space. To handle this, I increased the number of Spark jobs to speed up data movement into Hadoop. But this caused to slow down the Exadata and it created a bottleneck issue. Then I suddenly thought about compressing the data at exadata base, and luckly, I discovered that EXADATA has a built-in compression feature — and the best part is that the data doesn’t need to be decompressed when it’s moved to Hadoop. Using this compression method, I was able to reduce the data size by almost 70% in Exadata.  After that, I reduced the number of Spark jobs, which helped Exadata run better and stabilized the pipeline.</p><p><a href="#top">맨 위로</a></p><h3 id="issue-schema-change">이슈 - 스키마 변경</h3>I remember there was a project that we were integrating data from multiple sources into a central data warehouse.<p>The challenge was that one of the upstream systems frequently changed its schema without notice. And it caused our ETL jobs to fail and delayed reporting for business users. My responsibility was to make the pipeline more resilient so that these schema changes would not break the entire data flow. I implemented a schema validation and auto-adjustment process. I updated the code to compare the incoming data schemas against our expected schema. If a non-critical column changed such as a new column being added, the pipeline could adapt automatically without failing. But For critical mismatches, the system flagged the issue, generated incident, and provided fallback logic to continue processing the data. This reduced ETL job failures by more than 90% and ensured that the business team continued to receive the data even when upstream systems changed its schema unexpectedly.</p><p>Valid records are loaded into the main HDFS path, and Invalid records are redirected to a separate reject HDFS path.</p><ul><li>When a string value like “ABC” appears in a numeric column ,</li><li>When a NULL value is provided for a NOT NULL column ,</li><li>When a date column receives a value in a completely different or invalid format ,</li><li>When the data type is correct but the length exceeds the limit (e.g., a 50-character string for a VARCHAR2(10) column)</li></ul><p>오라클에 데이터가 저장될때 : A CLOB (Character Large Object) is a data type used to store very large text data.</p><p><a href="#top">맨 위로</a></p><h3 id="data-warehouse-tech">데이터 웨어하우스 설계 (e.g. Amazon Redshift, Snowflake 사용경험)</h3>I have experience using Redshift to build the cloud data warehousing. In one of my project, I built an analytics pipeline to process and analyze mobile user login data.To achieve this, I set up a pipeline where the log data was first stored in Amazon S3. From there, AWS Glue processed and loaded data into Redshift. Once the data was in Redshift, I used Amazon QuickSight to build interactive dashboards. And it visualized key user activity such as session duration, clickstream patterns, and device usage. This solution provided business stakeholders with actionable insights.<p><a href="#top">맨 위로</a></p><h3 id="data-warehouse-problem">데이터 웨어하우스 지연속도 문제 및 해결 </h3><p>One of the challenges I ran into was that loading JSON files from S3 into Redshift was much slower than I expected.<br>Because the data was in JSON, Redshift had to parse every row, and the file sizes were all different.<br>This caused performance issues and even led to uneven data distribution across Redshift nodes.</p><p>To fix this, I redesigned the ingestion process in AWS Glue.<br>I converted the JSON data into Parquet and saved the files in S3 with same sizes—around 128 MB.<br>Since in Parquet the data is already fully parsed, Redshift didn’t have to do extra parsing during the load, which significantly sped up the loading process.<br>I also updated the DISTKEY and SORTKEY based on how the data was being queried. And it helped prevent data skew and allowed Redshift to process data more evenly across all nodes.</p><ul><li>왜 128MB 사이즈로 데이터를 만드는지?<br>I stored the Parquet files in 128 MB chunks because Redshift performs best when it reads multiple files of similar size in parallel. Consistent file sizes help avoid data skew, reduce S3 overhead, and allow Redshift to distribute the workload evenly across all nodes, which results in much faster COPY performance.</li></ul><p><a href="#top">맨 위로</a></p><h3 id="data-normalization">데이터 정규화와 비정규화의 차이점</h3>Normalized data is typically used in OLTP systems. It separates data into multiple related tables to reduce redundancy and maintain data integrity. This helps ensure consistency during insert, update, and delete operations, but often requires multiple joins to retrieve data.Denormalized data is more common in OLAP systems. It intentionally duplicates data by combining related fields into fewer tables, which improves read performance and speeds up complex analytical queries.<p><a href="#top">맨 위로</a></p><h3 id="star-snowflake-schema">Star & Snowflake 스키마 (데이터 웨어하우스에서 스타 스키마 사용)</h3>In most data warehouses, the Star Schema is used because it provides high query performance, especially for analytical workloads, and has a simple structure consisting of a central fact table connected to denormalized dimension tables. This simplicity also makes it well suited for BI tools like Tableau or Power BI.But the Snowflake Schema is also used—especially when storage efficiency or data normalization is a higher priority. It tends to introduce more joins, which can affect query performance. Therefore, Star Schema is generally preferred in data warehouse environments.<p><a href="#top">맨 위로</a></p><h3 id="dq-multi-point-good">데이터 품질관리 - 여러포인트 검증 &amp; good data</h3>When it comes to data quality, I apply validation at multiple points of the pipeline.During ingestion, I perform schema validation and basic checks such as null values, data types, and duplicates. As the data moves through transformations, I apply additional business-rule validations to ensure the results make sense before loading them into data warehouse.In addition, I worked closely with business team to define what “good data” means for their use cases.  And I ensured that the dashboards in Tableau reflected the reliable information for decision-making.<p>In one project, I worked with the fraud prevention team, where my role was to deliver data they could fully trust. For them, “good data” meant accurate, up-to-date, and reliable information without duplication or errors. Because the quality of data directly impacted their fraud detection models, I focused not only on data delivery but also on maintaining high quality through the validation and monitoring.</p><p><a href="#top">맨 위로</a></p><h3 id="etl-design-ca7-glue">ETL (오케스트레이션) 설계 - CA7와 Glue 이용</h3>I’ve designed and implemented both batch and near real-time ETL pipelines. For near real-time workloads, I built pipelines that ingest Kafka streaming data into Oracle Exadata and Hadoop every ten minutes. I used PySpark for transformations and I used the CA7, a mainframe-based scheduler, to orchestrate the dependencies across these jobs. CA7 ensured that each PySpark workflow ran in the correct sequence and at the right time and it was critical for the batch operations.I also have experience building cloud-native ETL solutions. In one project, I used AWS Glue studio to design the ETL workflows. Glue’s built-in transformations, and job orchestration features made it easier to manage the logic.<p><a href="#top">맨 위로</a></p><h2 id="기술-내용"><a href="#기술-내용" class="headerlink" title="기술 내용"></a>기술 내용</h2><h3 id="alteryx-usage">Alteryx 사용기간 및 경험</h3>I used Alteryx for about a year in the past, mainly for data preparation and automation tasks such as joining datasets, performing aggregations, and creating analytical outputs. However, I didn’t use it as the primary tool in any large-scale enterprise projects. These days, I mainly work with AWS Glue Studio. When dealing with larger datasets, I noticed that Alteryx tends to slow down since it’s not optimized for big-data workloads. But AWS Glue Studio runs on Apache Spark, which provides much better performance and scalability for heavy ETL processing.<p><a href="#top">맨 위로</a></p><h3 id="teradata-usage">테라데이터 사용기간 및 경험</h3>I have around two years of experience with Teradata, mainly using it with Hadoop system. In our setup, Hadoop stored the customer's account data, and Teradata accessed that data through QueryGrid, which allowed us to easily combine and query Hadoop datasets. We also connected Tableau to Teradata and set up hourly refreshes so the dashboards always reflected the latest customer's account data insights.<ul><li>사용할때 문제 : Network latency 이슈<br>Network latency was the biggest issue we faced. Because Teradata had to retrieve large volumes of detailed data from Hadoop over QueryGrid, there were many cases where the query didn’t return on time or failed altogether. To address this, we changed the approach so that heavy processing happened in Hadoop first. We aggregated and filtered the data using Spark, and then used QueryGrid only to bring back a much smaller dataset. This significantly reduced the amount of data being transferred, which helped avoid latency issues and made the overall query performance much more stable.</li></ul><p><a href="#top">맨 위로</a></p><h3 id="teradata-troubleshooting">테라데이터 문제해결</h3>One of the most common issues I faced with Teradata was slow query performance when working with large tables, especially when the table wasn’t partitioned. In those cases, Teradata had to scan the entire table, which made daily jobs take much longer than expected.To fix this, I added date-based partitions so Teradata only scanned the specific partition needed for each query. This small change made a big difference. the queries became much faster and more stable. It also helped reduce the load on the system and improved overall performance.<p><a href="#top">맨 위로</a></p><h3 id="teradata-performance">테라데이터가 높은 성능을 발휘할수 있는 이유</h3>Teradata leverages an MPP architecture where data is distributed across multiple AMPs (Access Module Processors). Each AMP works independently to store and process its portion of data and it enables parallel execution. Because of this distribution mechanism, Teradata can handle large volumes of data with high performance.<p><a href="#top">맨 위로</a></p><h3 id="teradata-pi">테라데이터의 Primary Index (PI) 역할</h3>A Primary Index determines how data is distributed across AMPs. Choosing the right PI is crucial because it ensures even data distribution. A well-chosen PI improves join performance and overall query efficiency. (Teradata 내부에서는 데이터를 저장할 때 Primary Index 컬럼에 해시 함수를 적용해서 Hash Value를 만들고, 그 값을 기반으로 어떤 AMP에 저장할지를 결정합니다.)<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> customer (</span><br><span class="line">    customer_id <span class="type">INTEGER</span>,</span><br><span class="line">    name <span class="type">VARCHAR</span>(<span class="number">100</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">PRIMARY</span> INDEX (customer_id);</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="teradata-skew">테라데이터 데이터 불균형을 어떻게 해결?</h3>Data skew occurs when data is unevenly distributed across AMPs, causing some AMPs to process significantly more data than others. This leads to slower query performance. To handle this Data skew , I typically review PI selection and check for unique columns. Sometimes, creating a multicolumn PI can help balance the distribution.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> customer_new</span><br><span class="line"><span class="keyword">PRIMARY</span> INDEX (new_column)</span><br><span class="line"><span class="keyword">AS</span> customer</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">NO</span> DATA;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT INTO</span> customer_new</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> customer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> customer;</span><br><span class="line"></span><br><span class="line">RENAME <span class="keyword">TABLE</span> customer_new <span class="keyword">TO</span> customer;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># orders_stage : 스테이징 테이블</span><br><span class="line"># orders_nopi : NoPI 테이블 (<span class="keyword">Primary</span> Index 없음)</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT INTO</span> orders_nopi</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> orders_stage</span><br><span class="line">HASH <span class="keyword">BY</span> customer_id;</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="teradata-secondary-index">테라데이터의 Secondary Index 이란</h3>A Secondary Index is useful when frequently queried columns are not part of the Primary Index. It accelerates data access without re-distributing data. However, because Secondary Indexes require additional maintenance, I usually add them only when a business-critical query pattern consistently needs optimization.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># SI 추가하기</span><br><span class="line"><span class="keyword">ALTER TABLE</span> your_table_name</span><br><span class="line"><span class="keyword">ADD</span> INDEX (column_name);</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="teradata-partition">Teradata 파티션</h3>Partitioning allows tables to be divided into manageable segments, usually based on date. This improves query performance because Teradata only scans relevant partitions instead of the whole table. I commonly used date-based partitioning.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> sales_daily (</span><br><span class="line">    order_id     <span class="type">INTEGER</span>,</span><br><span class="line">    order_date   <span class="type">DATE</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> RANGE_N(order_date <span class="operator">&gt;=</span> <span class="type">DATE</span> <span class="string">&#x27;2023-01-01&#x27;</span></span><br><span class="line">                     <span class="keyword">EACH</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> sales</span><br><span class="line"><span class="keyword">WHERE</span> order_date <span class="operator">&gt;=</span> <span class="type">DATE</span> <span class="string">&#x27;2023-05-01&#x27;</span></span><br><span class="line">  <span class="keyword">AND</span> order_date <span class="operator">&lt;</span>  <span class="type">DATE</span> <span class="string">&#x27;2023-06-01&#x27;</span>;</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="alteryx-teradata">Alteryx와 Teradata 사용</h3>I think ETL pipelines between Alteryx and Teradata are built using Alteryx’s In-DB tools. Alteryx generates SQL and pushes all heavy transformations to the Teradata MPP engine, which handles large-scale joins and aggregations efficiently. Alteryx simply orchestrates the workflow, while Teradata performs the actual processing. This approach combines the ease of use of Alteryx with the scalability of Teradata.<h3 id="neo4j">Neo4J 관련해서</h3>Although I haven’t used Neo4j in production, But I’m interested in graph databases. I would like to have the opportunity to learn and apply Neo4j in future projects.<p><a href="#top">맨 위로</a></p><h3 id="multi-national-collaboration">여러 민족 같이 근무 - sync 미팅, 미팅요약</h3>Currently, I work on a data integration project with team members from the U.S., India, and Europe. At first, coordination was difficult because of time zone differences and different communication styles. To improve collaboration, I organized short daily sync meetings that overlapped our working hours and encouraged open discussions so everyone could share progress or blockers. I also started sending clear written summaries after each meeting so teammates in different time zones could stay updated. As a result, we reduced misunderstandings and improved task handoffs between regions.<p><a href="#top">맨 위로</a></p><h3 id="cross-team-collaboration">다른 팀 협업 - 용어 통일</h3>In one of my projects, I worked closely with software engineers and business analysts to improve how we tracked and analyzed user behavior. The engineers were responsible for sending user activity data into our database, and my role was to clean and transform that data so it could be used for reporting and analysis. I noticed that each team had slightly different definitions for key metrics, like “active users” or “sessions,” which caused confusion in reports. So, I organized a short meeting to align on clear definitions and updated our data dictionary to make sure everyone used the same terms. After that, the reports became much more consistent, and the business team was able to make decisions faster and with more confidence. It was a great experience showing how clear communication and teamwork can really improve data quality and trust.<p><a href="#top">맨 위로</a></p><h3 id="tight-schedule-pressure">타이트한 스케줄 & 압박 - 일 나누고 대화, 트랙</h3>When I face tight deadlines or high-pressure situations, I stay calm and break the work into smaller parts. For example, in one project, our team had to build a new ETL workflow in less than two weeks because of a last-minute client request. Instead of stressing out, I focused on what was most important, assigned tasks clearly, and set up short daily check-ins to track progress. I also kept open communication with both the team and stakeholders, making sure everyone understood what we could realistically deliver. By staying organized and working together, we completed the project on time with great results. This experience taught me that under pressure, clear priorities, steady communication, and teamwork are the keys to success.<p><a href="#top">맨 위로</a></p><h2 id="품질과-보안-내용"><a href="#품질과-보안-내용" class="headerlink" title="품질과 보안 내용"></a>품질과 보안 내용</h2><h3 id="dq-unit-test">데이터 품질관리 - 유닛테스트</h3>I usually use Pytest for unit testing in Python. It’s simpler and more readable than the built-in unittest module, and it allows to write tests quickly without creating test classes. In Pytest, test functions simply start with test_, and I use the assert statement to verify the results.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"><span class="keyword">from</span> calculator <span class="keyword">import</span> add, divide  <span class="comment"># calculator.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_add</span>():</span><br><span class="line">    <span class="keyword">assert</span> add(<span class="number">2</span>, <span class="number">3</span>) == <span class="number">5</span></span><br><span class="line">    <span class="keyword">assert</span> add(-<span class="number">1</span>, <span class="number">1</span>) == <span class="number">0</span></span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="quality-issue-currency">품질 문제 - 통화단위 에러</h3>During a new ETL release, one of our reports was showing incorrect revenue numbers. After investigating, I found that the issue came from an incorrect currency conversion in the transformation logic. I quickly fixed the script, reprocessed the data, and added the automated checks to compare daily results with historical trends. After that, the data became much more accurate, and the same issue never happened again.This experience reminded me how important it is to validate data thoroughly before going to production.<p><a href="#top">맨 위로</a></p><h3 id="security-compliance">보안과 규정 (AWS, Azure)</h3>In my current role, we have a dedicated security and compliance team that handles overall data governance. So if I need access to certain sensitive databases or tables, I first have to get approval from that team. This helps only the right people can have an access to the data. On the data engineering side, I am responsible for protecting sensitive data during our ETL processes. That means identifying PII data and masking them, so even if someone sees this data, it’s not readable. Actually, We strictly follow the principle of least privilege. We assign only the minimum required permissions.<p><a href="#top">맨 위로</a></p><h2 id="행동-면접-질문"><a href="#행동-면접-질문" class="headerlink" title="행동 면접 질문"></a>행동 면접 질문</h2><h3 id="manager-absence-decision">매니저가 부재시 결정해야할 경우</h3>A few months ago, there is a configuration issue during a production release. And it caused to delay the data. At that time, my manager was not available, but the fix required his approval. I quickly analyzed the impact, documented the issue and solution, and escalated it to the department lead for temporary approval. I clearly communicated the risk and rollback plan, implemented the fix, and monitored results until the system was stable. When my manager returned, I shared a full summary and documentation for review. This taught me how to act responsibly under pressure. And I also learned to make quick but careful decisions and keep communication clear with others.<p><a href="#top">맨 위로</a></p><h3 id="team-disagreement-sla">팀원 의견 다름 - SLA 20분 지연 - 품질해결</h3>Yes, I’ve experienced disagreements within the team. One example was during a near real-time data pipeline project. We were loading Kafka data into Hadoop, and the pipeline often missed our 10-minute SLA — sometimes it took over 20 minutes. some of the team members wanted to focus on improving Spark performance, while others, including me, thought the main issue was data quality because of inconsistent records and schema mismatches. To find the real cause, I checked the logs and monitoring reports and found that about 70% of the delays were due to data validation errors, not Spark processing speed. Based on that insight, I proposed a short proof-of-concept to implement stronger schema validation and fallback rules in QA environment and it worked. After implementing it in production, the number of failures dropped significantly and we regained our SLA. Once the team saw the data and results, everyone agreed to proceed with quality improvements first, then revisit performance tuning. This experience taught me that using measurable data, clear communication, and structured tests is much more effective than letting opinions dominate technical decisions.<p><a href="#top">맨 위로</a></p><h3 id="manager-disagreement-refresh-partition">상사 의견 다름 - refresh only 변경된 파티션만</h3>If I don’t agree with my manager’s opinion, I first make sure I fully understand their reasoning and goals. Then I share my perspective, supported by data or examples, rather than emotion. For instance, in one project, my manager suggested running a full data reload every day to ensure data completeness. I understood his concern but also knew it would be inefficient, since most of the customer master data rarely changed. So I analyzed the update patterns and designed a process to refresh only the partitions containing changed customer records, instead of reloading the entire dataset. After testing it together in staging, we confirmed that this approach maintained accuracy while reducing runtime from over 3 hours to about 40 minutes. That experience taught me that presenting clear evidence and focusing on the common goal — not on who’s right or wrong. It helps turn disagreements into productive discussions.<p><a href="#top">맨 위로</a></p><h3 id="diffent-personality">다른 성향의 사람과 협동</h3>In one of my previous projects, I worked closely with a senior engineer whose personality was quite different from mine. I’m generally organized and prefer to plan tasks carefully before execution, but he preferred to take a more spontaneous (스팬테니어스), “just try it and fix later” approach. At first, this difference caused some tension because I wanted to review the design and test cases before deployment, while he wanted to move fast to meet deadlines. Instead of arguing, I suggested we combine both styles — I would document the structure and validation rules, and he could focus on rapid prototyping. By dividing responsibilities that way, we were able to deliver the pipeline faster and maintain quality. Over time, I also learned to be more flexible and open to trying quick experiments. And he started to appreciate the value of planning and testing as well. That experience taught me that personality differences can actually strengthen a team when you focus on complementary strengths rather than conflicts.<p><a href="#top">맨 위로</a></p><h3 id="helped-teammate">팀동료 성공시키기</h3>In one project, I worked with a junior data engineer who was new to our Spark-based ETL environment. She was struggling to understand how our partitioning and scheduling logic worked, and her job often failed in production. Instead of just fixing it for her, I scheduled a short session to walk her through how the Spark job read data from Oracle, wrote it into Hadoop platform. I also helped her debug one of her failing jobs step by step and showed her how to check logs and handle schema mismatches. Within a few weeks, she became confident enough to manage her own pipelines and even automated some validation scripts. Seeing her grow and succeed made me realize that helping others not only strengthens the team but also improves overall project quality.<p><a href="#top">맨 위로</a></p><h3 id="urgent-vs-important">급한일과 중요한일의 우선순위 - 급한것 먼저</h3>I usually start by understanding the impact of each task. If something is urgent and affects business operations or other teams, I handle it first. But I also make sure not to ignore important long-term work. For example, once our production ETL job failed right before a reporting deadline. I paused my ongoing optimization task, fixed the ETL issue immediately, and restored the pipeline so the business could get their reports on time. After that, I resumed my optimization work. I believe good prioritization means balancing immediate needs with long-term improvements.<p><a href="#top">맨 위로</a></p><h3 id="tech-challenge-1-5tb">기술 문제 도전 - 1.5TB 처리</h3>One of the most challenging technical problems I faced was optimizing a large Spark ETL job that processed about 1.5 TB everyday. The job was taking more than 5 hours to complete, which caused delays in our downstream dashboards and reports. I started by analyzing the Spark UI and noticed heavy shuffling and many small output files. To fix it, I adjusted the partition strategy, used broadcast joins for smaller tables, and combined small files before writing to Hadoop. I also added data filtering early in the pipeline to reduce unnecessary computation. After these changes, the runtime dropped from 5 hours to under 3 hours, and the cluster cost was reduced by almost 30%. That experience taught me how small technical optimizations can have a big business impact when working with large-scale data.<p><a href="#top">맨 위로</a></p><h3 id="improvement-large-data">개선 사례 - 큰 데이터 처리 (위 동일)</h3>I remember that one of our daily ETL jobs was taking more than 5 hours to finish. It processed a large amount of log data from multiple sources, and sometimes it even failed because of memory issues. I reviewed the Spark job and found that it was using too many small files and unnecessary joins. I optimized the job by adjusting the partition size, adding proper filters early in the transformation, and combining small files before loading. After the changes, the job ran in less than 3 hours and became much more stable. This improvement not only saved computing costs but also made our data available earlier for reporting every morning.<p><a href="#top">맨 위로</a></p><h3 id="production-connect-stop">생산 문제 - 커넥트 Stop</h3>When a production issue happens, I stay calm and focus on finding the root cause quickly. For example, one night our Kafka-to-Hadoop pipeline failed, and the business dashboards in Tableau were missing data the next morning. I immediately checked the Kafka Connect logs and found that the sink connector had stopped due to a network issue. I manually restarted the connector and confirmed that the data started flowing again. Afterward, I created a monitoring script using curl commands that checks the connector status every 10 minutes. If it fails, the script automatically creates an incident and sends an alert to our team. This experience taught me the importance of not only fixing issues quickly but also building automation to prevent the same problem from happening again.<p><a href="#top">맨 위로</a></p><h3 id="process-improvement-connect-stop">자발적 프로세스 개선 - 커넥트 Stop</h3>In one project, I noticed that one of our data pipelines sometimes failed overnight, but the team would only find out the next morning. This caused delays in daily reports and frustration for analysts waiting for updated data. Even though it wasn’t part of my assigned tasks, I decided to create an automatic alert system. I built a small Python script that checked job completion logs and create INC if a failure occurred. After testing it for a week, I presented it to the team, and we integrated it into our pipeline. Since then, we’ve been able to respond to failures immediately, reducing downtime and improving data reliability. That experience taught me the value of being proactive — small improvements can make a big difference to the whole team.<p><a href="#top">맨 위로</a></p><h3 id="kafka-realtime-connect-check">Kafka 실시간 데이터에서 고려할 부분 - 커넥트 상태확인</h3>When designing Kafka pipelines, I focus on a few key areas to ensure performance and reliability. First, I choose the right topic partitioning strategy based on data size. And then I make sure that Kafka connectors are properly configured with retry mechanisms in case of failures. For monitoring, I built a script that uses a curl command to check the status of the all Kafka sink connectors every 10 minutes. If the one of the connectors is down or there’s an issue with the Kafka broker, the script automatically generates an incident, triggering an alert to my team. This setup helped us catch issues and significantly reduced downtime.<p><a href="#top">맨 위로</a></p><h3 id="failure-column-validation">실패,실수 - 컬럼검증X, 통화단위 에러</h3>Yes, I made a mistake during a data validation process. I was in charge of checking the output of a new ETL job before it went live. I verified the total record count but forgot to double-check the column-level transformations. After deployment, we found that one column had an incorrect currency conversion rate, and it caused wrong numbers to show up in a few business reports. As soon as I realized the issue, I corrected the transformation logic, reprocessed the data, and updated the reports. After that, I added column-level validation rules and a simple Python script that automatically compares key fields between the source and target tables before deployment. That experience taught me how even a small mistake can affect business reports, so now I always check both the data structure and actual values carefully before sign-off.<p><a href="#top">맨 위로</a></p><h3 id="project-delay-schema-change">프로젝트 지연 - 스키마 변경</h3>Yes, I’ve experienced that before.In one project, our data ingestion pipeline was delayed because the upstream system changed its schema without notice. This caused our ETL jobs to fail and delayed daily reports for the client. As soon as the client raised concerns, I explained the issue clearly, shared the revised delivery plan, and sent daily updates so they could see our progress. Meanwhile, I worked with my team to add automatic schema validation and fallback logic in the pipeline, so future schema changes wouldn’t break the process again. After we implemented the fix, the pipeline became more stable, and the client appreciated our quick communication and the long-term solution we put in place.<p><a href="#top">맨 위로</a></p><h3 id="team-lead-source-missing">팀 리드 & 솔선 - 소스 입력 안됨</h3>In one project, we had a very tight deadline to deliver a new ETL workflow for daily reporting. However, a few tasks were delayed because some external data sources were not delivered on schedule, and that caused downstream jobs in Spark to fail during testing. To get things back on track, I took the initiative to organize short daily stand-up meetings and created a shared progress tracker in Confluence so everyone — including the data and QA teams — could see real-time task status. This helped us identify blockers early, communicate clearly, and reassign tasks based on team availability. Within a week, we recovered the lost time and successfully completed the workflow before the deadline. The reporting system went live as planned, and we avoided last-minute production issues. That experience taught me that strong coordination and clear communication are just as important as technical skills when leading a project under tight timelines.<p><a href="#top">맨 위로</a></p><h3 id="team-lead-datatype-mismatch">리더싶 사례 - data type mismatch</h3>During a data migration project, we faced a serious issue when one of the ETL jobs started failing right before a major release. Everyone was under pressure, and the team was unsure how to proceed. Even though I wasn’t the official lead, I took the initiative to organize an emergency meeting with the data, QA, and infrastructure teams. I divided the investigation into parts — one team checked data source changes, another team looked at schema issues, and I focused on debugging the Spark job logic. After identifying that a data type mismatch in one column was causing the failure, we quickly fixed it and ran validation tests together. The release went smoothly, and my manager later recognized my leadership for coordinating the teams under tight deadlines. That experience taught me that real leadership often means stepping up and guiding the team toward a solution — even without having a formal title.<p><a href="#top">맨 위로</a></p><h3 id="above-and-beyond-responsibility">지시받지않은 일 - add load_date / 상사와 의견이 맞지 안음사례와 동일</h3>A few months ago, I noticed that one of our nightly ETL jobs in production was running slower and occasionally failing, even though it wasn’t part of the pipelines I was directly responsible for. Instead of ignoring it, I decided to investigate on my own time because it was delaying downstream reports for the business team. After checking the logs, I found that the job was performing a full table scan on a very large dataset every night. To fix the issue, I first added a new LOAD_DATE column to the target table to track daily data loads. Then, I rewrote the logic to process only new and updated records based on this column and created partitions on LOAD_DATE to improve query performance and data management efficiency. After validating the logic, I worked with the scheduler team to test and deploy the fix safely. The result was dramatic — runtime dropped from over 3 hour to under forty minutes, and the business team could access their dashboards much earlier every morning. Even though it wasn’t my assigned task, I took ownership because I knew the issue was affecting overall business operations. That experience taught me that going above and beyond means proactively solving problems that impact the team — not just completing my own tickets.<p><a href="#top">맨 위로</a></p><h3 id="customer-request">고객이 마지막에 변경요청시 - 스키마변경</h3>When a customer requests changes right before the final release, I believe it’s important to balance flexibility with stability. First, I listen carefully to understand why the change is needed — whether it’s a business-critical fix or just a nice-to-have improvement. Then I assess the impact on scope, timeline, and quality. If the change is minor and doesn’t risk the release, I coordinate quickly with the team to implement and test it. However, if the change is major or could affect stability, I clearly communicate the risks and propose alternatives — such as including it in the next patch or minor release. The most important thing is to be honest about the situation, and focus on finding solutions. This way, the customer knows you’re listening, and the project stays on track.For example, in one project, a client requested a schema change right before deployment. I analyzed the dependency, explained that it would delay the release by two days, and suggested deploying the current version first and adding the change in the next release. The client agreed, and we delivered on time without compromising quality.<p><a href="#top">맨 위로</a></p><h3 id="customer-request-offen">고객이 자주 변경사항을 요청할때 - 요청을 그룹핑함</h3>When customers often ask for changes, I try to handle it in a clear way. First, I listen carefully to understand why they want the change — maybe their business needs have changed or something wasn’t clear before. Then I explain what the change means for the project — like how it might affect the schedule or workload — so they can decide what’s most important. If there are many small requests, I suggest grouping them together or saving them for the next update.In one project, the customer kept asking for new data checks. I made a simple list to track all the requests and talked with them once a week to decide which ones to do first. That way, they felt listened to, and our team could work in an organized way without confusion.<p><a href="#top">맨 위로</a></p><h3 id="non-technical">비개발자에게 기술적인 내용을 쉽게 설명할 수 있나요?</h3>Yes, I always try to explain technical topics in a simple and clear way, especially when talking to non-technical people. I focus on using everyday language instead of technical terms, and I give real examples that relate to their work or daily life. For example, when explaining data pipelines, I might say it’s like a factory line — data comes in as raw material, goes through cleaning and transformation, and comes out as a finished product ready for analysis. I believe being able to translate complex ideas into simple concepts is an important skill for teamwork and communication.<p><a href="#top">맨 위로</a></p><h3 id="issue-spark-memory">이슈 - Spark memory 문제</h3>One of the most common issues I encounter is out-of-memory (OOM) errors. To address this, First, I review the PySpark code to identify any operational command like collect() or toPandas() that might be pulling too much data into the driver. If I find them, I either remove or replace them. I also use broadcast joins when dealing with small tables to minimize shuffle operations, it can reduce memory usage. Another important step is avoiding Python UDFs if it is possible to use native Spark SQL functions. Additionally, when I need to reuse the intermediate results, I use caching it and also I use MEMORY_AND_DISK storage option to avoid overwhelming the memory. Finally, I adjust partition sizes using coalesce() or repartition() to optimize resource usage during shuffle operations. By applying these techniques, I’ve been able to effectively prevent and troubleshoot memory-related issues in Spark jobs.<p><a href="#top">맨 위로</a></p><h3 id="etl-pipeline-optimization">ETL pipeline 최적화 - SLA 6시로 맞추기</h3>I had a situation where our SLA required the data to be fully available by 6 AM, But one day, the amount of source data suddenly increased — almost three times more than usual. Because of that, our Spark job didn’t finish until 8 AM. So I increased the number of partitions to allow more parallel processing. I also checked our resource settings and made sure the job had enough memory and CPU by adjusting the scheduler pool and YARN resorce manager.After these changes, the job completed before 6 AM the next day, and we were able to meet the SLA again. This experience helped me understand how important it is to tune the Spark jobs and monitor them carefully, especially when data volume suddenly increase.<p><a href="#top">맨 위로</a></p><h3 id="aws-glue-experience">AWS Glue 사용경험 - ETL services</h3>I have experience building ETL workflows using AWS Glue. In one of my project, I built an analytics pipeline to process and analyze mobile user log data. I’ve used Glue to extract data from S3 and then transform and load into Redshift every 10 minutes Also, I’ve utilized Glue Crawlers to automatically detect schema changes and keep the data catalog updated for querying in Athena as well.<p><a href="#top">맨 위로</a></p><h3 id="no-current-glue">현재 glue를 사용하지는 않는다.</h3>We are currently using CA7 mainframe along with PySpark scripts for our ETL processes mainly. CA7 is a mainframe-based job scheduling and workflow automation tool. It's used to manage and schedule batch jobs, ensuring the tasks run in the right order and at the right time. We have not changed this Orchestration tool Because Our data workflows have been integrated into a mainframe-based CA7 scheduling system for a long time and switching would introduce additional operational costs. Lastly Our team continues to manage and monitor all ETL workflows within the CA7 environment.<p><a href="#top">맨 위로</a></p><h3 id="redshift-intro">Redshift 란?</h3>Redshift is designed for high-speed querying using massively parallel processing (MPP). This makes it great for analyzing large datasets quickly. we can start small and scale up by increasing the node size or number of nodes as the data grows. Data is stored in columnar format, which speeds up analytical queries and reduces I/O.<p><a href="#top">맨 위로</a></p><h3 id="redshift-columnar">Redshift Columnar Storage</h3>When we execute queries like SUM(), AVG(), or filtering on specific columns, the database only needs to read the relevant columns, not entire rows. This speeds up reading performance in data warehousing and analytics. Since each column typically contains similar types of data, it compresses more efficiently than row-based data. Also, it only reads the selected columns, the amount of data scanned is reduced.<p><a href="#top">맨 위로</a></p><h3 id="snowflake-advantages">스노우플레이크 장점 (zero-copy cloning and time travel)</h3>I can instantly clone entire databases or tables without duplicating data and it saves cost and time. Also, there is a Time Travel function and it lets us query or restore data from a previous point in time. It is useful for recovering the data.<p><a href="#top">맨 위로</a></p><h3 id="databricks-experience">Databricks 사용경험 - Anomaly detection</h3>On the Databricks side, I primarily work with the Azure-hosted version of Databricks. Recently, I developed an end-to-end scalable pipeline for computer vision anomaly detection. As you can see my portfolio website. You can see its notebook and model. I use the PyTorch and Hugging Face to train and build the model.<p><a href="#top">맨 위로</a></p><h3 id="partition-strategy">파티션 전략 (Spark, Redshift, Snowflake)</h3>Partitioning strategy depends on query patterns and data volume. Regarding the Oracle Exadata, I Used range partitioning by date column to support daily ingestion and quickly delete old data by simply dropping partition.In Spark, I used dynamic partition overwrite with partitionBy("date") when writing Parquet files, and adjusted the number of partitions with coalesce or repartition commands to avoid creating too many small files.In Redshift, I defined DISTKEY and SORTKEY based on the columns that were most frequently used in joins and filters, which helped improve query performance and reduce data movement across nodes.In Snowflake, I rely on its automatic micro-partitioning feature, which breaks data into 16MB blocks and optimizes storage and query performance without any manual intervention. However, for very large tables where queries frequently filter on specific columns—such as date, I define a cluster key to further improve performance and these approach improved query speed as well.<p><a href="#top">맨 위로</a></p><h3 id="oracle-range-partitioning">데이터 모델링 & Architecture - 오라클 range partitioning</h3>I have strong experience in data modeling and architecture, especially in designing data pipelines at PNC. In Oracle Exadata, I designed the tables using range partitioning by day, so that Kafka data was automatically separated into daily partitions. This made it much easier to manage large volumes of data, speed up queries, and improve overall performance. For example, instead of using a traditional delete command, we could simply drop an entire partition when the data was no longer needed.  And This is not only optimized storage space but also kept query performance fast and efficient, since queries only scanned the relevant partitions rather than the entire table.<p><a href="#top">맨 위로</a></p><h3 id="python-sql-spark">Python, SQL, Spark, and PySpark</h3>I am working with Python, SQL, Spark, and PySpark throughout my career as a data engineer. Python has been my primary programming language for building ETL pipelines. I've used it in both production and QA environments, including developing data ingestion frameworks.SQL is a core part of my daily workflow. I’ve written complex analytical queries and optimized SQL for performance on databases like Oracle Exadata.With Spark, I’ve built scalable data processing pipelines for both batch and near real-time use cases. I’ve used Spark in distributed environments, primarily through PySpark, to perform transformations, aggregations, and joins on large datasets.<p><a href="#top">맨 위로</a></p><h3 id="spark-hadoop-ingestion">경험 - 스파크/하둡 데이터 ingestion</h3>On the Hadoop and Spark side, I designed frameworks to handle large-scale data ingestion and transformation. For example, data coming from Oracle first needed to be cleaned before it could be used for reporting. I built PySpark jobs that automatically parse the data and removed duplicate records, handled missing values, and converted the data into optimized formats like Parquet and stored it at hadoop platform. At the same time, I added metadata and validation rules so that we could easily track the data and confirm its accuracy.<p><a href="#top">맨 위로</a></p><h3 id="aws-experience">경험 - AWS 많이 사용했니?</h3>If you take a look at my portfolio website, you’ll see that most of my projects are built using AWS. I actively use AWS to quickly build and experiment with different data architectures. Since data tools are evolving so fast, I use EMR service to easily install and try out big data tools like Spark, Hadoop, and Kafka. For storing data, I normally use RDS or S3. Overall, AWS has been a great platform for me to learn, experiment, and build end-to-end data pipelines.<p><a href="#top">맨 위로</a></p><h3 id="strengths-weaknesses">장점과 단점</h3>My biggest strengths are my flexibility and adaptability. Wherever I work, work environments change daily and throughout the day. And there are certain projects that require individual attention and others that involve a teamwork approach. My flexibility and adaptability have allowed me to meet the expectations and even go beyond them. Also, I get along with people around me. This kind of personality makes the work environment more comfortable and easierAs far as my weaknesses, I sometimes put in too much time on what I like to do. With my mentor’s help, I started using a daily checklist to plan and prioritize my work. Now I make sure I pace myself better and focus on finishing the most important tasks first. It’s helped me become more balanced and efficient.<p><a href="#top">맨 위로</a></p><h3 id="stress">스트레스를 어떻게 풉니까?</h3>When I feel stressed, I try to handle it in a healthy and productive way. First, I take a short break to clear my mind — even a short walk or a few minutes of quiet time helps me refocus. I also like to organize my tasks and set priorities. Once I have a clear plan, the stress usually goes down because I can see what needs to be done first. Outside of work, I relieve stress by exercising and spending time with my family or friends. These activities help me recharge and come back to work with more energy and focus.<p><a href="#top">맨 위로</a></p><h3 id="motto">삶의 모토는?</h3>My life motto is “Stay curious, stay humble, and keep growing.” I believe learning never stops, no matter how much experience you have. Staying curious helps me discover new ideas, staying humble keeps me open to feedback, and continuous growth gives me purpose in both my career and personal life.<p><a href="#top">맨 위로</a></p><h3 id="powertech">파워텍에서 머신러닝 모델 사용</h3>When I worked at Hyundai Powertech, we produced car transmissions. Each transmission needed a small gasket to fill the gap between parts, but the gap size was different for each transmission — sometimes 1 mm, 1.5 mm, or 2 mm. Because of this, the company had to keep all gasket sizes in stock, which wasted storage space and money. I collected the production log from the machines on the floor and analyze them and trained machine learning models to predict which gasket size would be needed for each transmission. Among several models, XGBoost performed the best. By using this prediction model, we reduced inventory levels and saved costs by ordering only the needed gasket sizes.- XGBoost is an efficient and high-performance boosting algorithm that combines many small decision trees to make strong and accurate predictions.<p><a href="#top">맨 위로</a></p><h3 id="final-remarks">마지막으로 하고 싶은 말</h3>May I ask which technologies your team works with most often, and what types of projects are currently the main focus?May I ask what qualities you think are most important to succeed in this position?May I ask which projects are currently the highest priority?<p><a href="#top">맨 위로</a></p><h3 id="python-type">파이썬 타입비교</h3>Strings are  immutable but maintain order and allow duplicate characters.Lists are mutable, ordered, and allow duplicates.Tuples are similar to lists but immutable.Dictionaries are mutable and ordered (as of Python 3.7+), but their keys must be unique.Sets are mutable but unordered and do not allow duplicate elements.]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a id=&quot;top&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
.toc-grid{
  display:grid;
  grid-template-columns:1fr 1fr; /* 화면 반반 */
  gap:6px 20px;
  align-items:start;</summary>
      
    
    
    
    <category term="OTHERS" scheme="https://kish191919.github.io/categories/OTHERS/"/>
    
    
    <category term="INTERVIEW" scheme="https://kish191919.github.io/tags/INTERVIEW/"/>
    
  </entry>
  
  <entry>
    <title>TERADATA(3)-History View</title>
    <link href="https://kish191919.github.io/2025/12/03/TERADATA-3-History-View/"/>
    <id>https://kish191919.github.io/2025/12/03/TERADATA-3-History-View/</id>
    <published>2025-12-03T17:12:57.000Z</published>
    <updated>2025-12-03T17:17:58.173Z</updated>
    
    <content type="html"><![CDATA[<h1 id="📘-Teradata-Studio-–-SQL-History-창-완전-정리"><a href="#📘-Teradata-Studio-–-SQL-History-창-완전-정리" class="headerlink" title="📘 Teradata Studio – SQL History 창 완전 정리"></a>📘 Teradata Studio – SQL History 창 완전 정리</h1><p><em>Teradata SQL History 창 설정, 컬럼 선택, 노트 기능, 복원 방법</em></p><hr><h2 id="📍-1-SQL-History-창-복원하기"><a href="#📍-1-SQL-History-창-복원하기" class="headerlink" title="📍 1. SQL History 창 복원하기"></a>📍 1. SQL History 창 복원하기</h2><p>SQL History 창을 실수로 닫았거나 사라졌다면:</p><p><strong>Window → Reset Perspective</strong></p><p>를 눌러주면 기본 레이아웃으로 복원됩니다.<br>→ History 창, Result Viewer 등 모든 패널이 원래 위치로 돌아옵니다.</p><hr><h2 id="📍-2-SQL-History-창-컬럼-Columns-커스터마이징"><a href="#📍-2-SQL-History-창-컬럼-Columns-커스터마이징" class="headerlink" title="📍 2. SQL History 창 컬럼(Columns) 커스터마이징"></a>📍 2. SQL History 창 컬럼(Columns) 커스터마이징</h2><p>SQL History는 기본적으로 매우 많은 컬럼을 보여줍니다.<br>예: Timestamp, Source, User, Destination, Row Count, Result, SQL Statement 등</p><p>필요 없는 컬럼이 많으면 화면이 복잡해지므로 <strong>원하는 항목만 선택</strong>할 수 있습니다.</p><h3 id="🔧-설정-경로"><a href="#🔧-설정-경로" class="headerlink" title="🔧 설정 경로"></a>🔧 설정 경로</h3><p><strong>Window → Preferences → Teradata Datatools → SQL History</strong></p><p>여기서 다음을 할 수 있습니다:</p><h3 id="🔹-1-표시할-컬럼-선택"><a href="#🔹-1-표시할-컬럼-선택" class="headerlink" title="🔹 1) 표시할 컬럼 선택"></a>🔹 1) 표시할 컬럼 선택</h3><ul><li>Removed Columns → 표시 안 함</li><li>Displayed Columns → 표시됨</li></ul><p>예시: 아래 항목만 선택하여 필요한 정보만 보이게 할 수 있음</p><ul><li>Timestamp</li><li>SQL Statement</li><li>Result (에러 메시지 포함)</li><li>Row Count</li><li>Elapsed Time</li></ul><p>적용 후 History 창에는 선택한 컬럼만 깔끔하게 표시됩니다.</p><hr><h2 id="📍-3-설정-적용-후-결과-확인"><a href="#📍-3-설정-적용-후-결과-확인" class="headerlink" title="📍 3. 설정 적용 후 결과 확인"></a>📍 3. 설정 적용 후 결과 확인</h2><p>예시 쿼리를 실행한 후 History 창에서 다음 정보를 확인할 수 있습니다:</p><ul><li><strong>Row Count</strong>: 조회된 레코드 수</li><li><strong>Result</strong>:<ul><li>성공 시 “Executed as Single Statement”</li><li>2000개 넘으면 “Canceled” (제한 때문에 일부만 조회됨)</li></ul></li><li><strong>Elapsed Time</strong>: 실행 소요 시간</li><li><strong>SQL Statement</strong>: 실행한 SQL 원문</li><li><strong>Timestamp</strong>: 실행한 날짜&#x2F;시간</li></ul><p>👉 불필요한 데이터 없이 핵심 정보만 남아서 훨씬 읽기 쉬워짐</p><hr><h2 id="📍-4-SQL-Notes-기능-사용하기"><a href="#📍-4-SQL-Notes-기능-사용하기" class="headerlink" title="📍 4. SQL Notes 기능 사용하기"></a>📍 4. SQL Notes 기능 사용하기</h2><p>SQL Editor에서 쿼리를 실행할 때 <strong>노트 입력 팝업</strong>이 뜰 수 있습니다.</p><p>예시 팝업 내용:</p><blockquote><p>“Enter a note for this query”</p></blockquote><p>이 기능은 쿼리 실행 시 <strong>개인 메모</strong>를 기록해두는 기능입니다.</p><p>예:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">My first query</span><br></pre></td></tr></table></figure><p>쿼리를 실행하고 History 창에 “Notes” 컬럼을 추가하면 해당 내용이 표시됩니다.</p><hr><h2 id="📍-5-Notes-컬럼-추가하는-방법"><a href="#📍-5-Notes-컬럼-추가하는-방법" class="headerlink" title="📍 5. Notes 컬럼 추가하는 방법"></a>📍 5. Notes 컬럼 추가하는 방법</h2><p>경로:</p><p><strong>Window → Preferences → Teradata Datatools → SQL History</strong></p><ol><li>Available Columns 목록에서 “Notes”를 선택</li><li>Displayed Columns로 이동</li><li>필요하면 <strong>순서 이동(Up&#x2F;Down)</strong> 기능으로 원하는 위치 조정</li><li>Apply and Close</li></ol><p>이제 History 창에 Notes가 표시됩니다:</p><table><thead><tr><th>Timestamp</th><th>SQL Statement</th><th>Result</th><th>Notes</th></tr></thead><tbody><tr><td>2025-02-10</td><td>SELECT * FROM …</td><td>OK</td><td>My first query</td></tr></tbody></table><p>💡 SQL을 많이 작성하는 경우 쿼리 목적이나 특이사항을 기록해두면 유용함.</p><hr><h2 id="📍-6-Notes-기능-비활성화"><a href="#📍-6-Notes-기능-비활성화" class="headerlink" title="📍 6. Notes 기능 비활성화"></a>📍 6. Notes 기능 비활성화</h2><p>Notes 기능을 사용하지 않는다면 다음과 같이 끌 수 있습니다.</p><p>방법 ①</p><ul><li>SQL 실행 후 뜨는 노트 팝업을 “비활성화” 체크</li></ul><p>방법 ②</p><ul><li>Notes 컬럼을 다시 Removed Columns로 이동<br>→ History 창에서 제거됨</li></ul><hr><h2 id="📍-7-History-창-즉시-설정하는-빠른-방법"><a href="#📍-7-History-창-즉시-설정하는-빠른-방법" class="headerlink" title="📍 7. History 창 즉시 설정하는 빠른 방법"></a>📍 7. History 창 즉시 설정하는 빠른 방법</h2><p>History 창 오른쪽 상단의 <strong>🔧 (Wrench) 아이콘</strong>을 클릭하면<br>바로 같은 설정 화면으로 이동할 수 있습니다.</p><p><strong>Window → Preferences</strong> 메뉴를 타고 갈 필요 없음.</p><hr><h2 id="📍-8-언제든-기본값으로-복원-가능"><a href="#📍-8-언제든-기본값으로-복원-가능" class="headerlink" title="📍 8. 언제든 기본값으로 복원 가능"></a>📍 8. 언제든 기본값으로 복원 가능</h2><p>History 설정이 꼬이거나 너무 복잡해졌다면:</p><p><strong>Restore Defaults</strong> 버튼 클릭</p><p>→ Teradata Studio의 기본 설정으로 깔끔하게 되돌아옵니다.</p><hr><h2 id="🎯-최종-요약"><a href="#🎯-최종-요약" class="headerlink" title="🎯 최종 요약"></a>🎯 최종 요약</h2><p>이번 강의에서는 SQL History 창을 다음과 같이 정리했습니다:</p><h3 id="✔-원하는-컬럼만-표시하도록-커스터마이징"><a href="#✔-원하는-컬럼만-표시하도록-커스터마이징" class="headerlink" title="✔ 원하는 컬럼만 표시하도록 커스터마이징"></a>✔ 원하는 컬럼만 표시하도록 커스터마이징</h3><ul><li>Timestamp</li><li>Row Count</li><li>Result</li><li>SQL Statement</li><li>Elapsed Time</li><li>Notes (필요 시)</li></ul><h3 id="✔-Notes-기능"><a href="#✔-Notes-기능" class="headerlink" title="✔ Notes 기능"></a>✔ Notes 기능</h3><ul><li>쿼리에 개인 메모 추가 가능</li><li>History 창에 Notes 컬럼으로 표시됨</li><li>사용하지 않으면 비활성화 가능</li></ul><h3 id="✔-빠른-설정"><a href="#✔-빠른-설정" class="headerlink" title="✔ 빠른 설정"></a>✔ 빠른 설정</h3><ul><li>History창의 🔧 아이콘으로 즉시 이동</li></ul><h3 id="✔-레이아웃-복원"><a href="#✔-레이아웃-복원" class="headerlink" title="✔ 레이아웃 복원"></a>✔ 레이아웃 복원</h3><ul><li>Window → Reset Perspective 로 전체 UI 초기화 가능</li></ul><hr><p>필요하시면 다음 강의도 정리해서 연결된 md 파일로 만들어드릴게요!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;📘-Teradata-Studio-–-SQL-History-창-완전-정리&quot;&gt;&lt;a href=&quot;#📘-Teradata-Studio-–-SQL-History-창-완전-정리&quot; class=&quot;headerlink&quot; title=&quot;📘 Teradata </summary>
      
    
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/categories/TERADATA/"/>
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/tags/TERADATA/"/>
    
  </entry>
  
  <entry>
    <title>TERADATA(2)-Customizing the Interface and Result Set Viewer</title>
    <link href="https://kish191919.github.io/2025/12/03/TERADATA-2-Customizing-the-Interface-and-Result-Set-Viewer/"/>
    <id>https://kish191919.github.io/2025/12/03/TERADATA-2-Customizing-the-Interface-and-Result-Set-Viewer/</id>
    <published>2025-12-03T16:38:31.000Z</published>
    <updated>2025-12-03T16:43:04.692Z</updated>
    
    <content type="html"><![CDATA[<h1 id="📘-Teradata-Studio-환경-설정-및-화면-커스터마이징"><a href="#📘-Teradata-Studio-환경-설정-및-화면-커스터마이징" class="headerlink" title="📘 Teradata Studio 환경 설정 및 화면 커스터마이징"></a>📘 Teradata Studio 환경 설정 및 화면 커스터마이징</h1><p><em>Teradata Studio 뷰 변경 &#x2F; 폰트 수정 &#x2F; Result Viewer 설정 &#x2F; 행 색상 변경 등</em></p><hr><h2 id="📍-1-하단-창-History-등-숨기기-보이기"><a href="#📍-1-하단-창-History-등-숨기기-보이기" class="headerlink" title="📍 1. 하단 창(History 등) 숨기기 &#x2F; 보이기"></a>📍 1. 하단 창(History 등) 숨기기 &#x2F; 보이기</h2><p>Teradata Studio에서 하단의 <strong>SQL History</strong> 또는 기타 창이 공간을 많이 차지할 때:</p><ul><li>창 우측 상단의 <strong>minimize( — )</strong> 버튼 클릭 → 창이 접힘</li><li>다시 보고 싶을 때는 아래 도킹된 탭을 클릭하면 다시 나타남</li></ul><p>✔ 화면을 넓게 쓰고 싶을 때 매우 유용한 기능</p><hr><h2 id="📍-2-폰트-크기-스타일-변경하기"><a href="#📍-2-폰트-크기-스타일-변경하기" class="headerlink" title="📍 2. 폰트 크기 &#x2F; 스타일 변경하기"></a>📍 2. 폰트 크기 &#x2F; 스타일 변경하기</h2><p>경로:<br><strong>Window → Preferences → General → Appearance → Colors and Fonts</strong></p><p>여기서 다양한 UI 요소의 폰트와 색상을 조정할 수 있습니다.</p><h3 id="🔸-폰트-종류-설명"><a href="#🔸-폰트-종류-설명" class="headerlink" title="🔸 폰트 종류 설명"></a>🔸 폰트 종류 설명</h3><ul><li>글자 옆에 <strong>A 아이콘</strong>이 있으면 “폰트 관련”</li><li>A가 없으면 배경색&#x2F;전경색 같은 “색상 관련”</li></ul><h3 id="🔸-Dialog-Font-변경하기"><a href="#🔸-Dialog-Font-변경하기" class="headerlink" title="🔸 Dialog Font 변경하기"></a>🔸 Dialog Font 변경하기</h3><p>경로:<br><code>General → Appearance → Colors and Fonts → Dialog Font</code></p><ol><li>원하는 폰트&#x2F;크기 선택</li><li>Preview에서 확인</li><li>Apply and Close</li></ol><p>📌 “Dialog Font”는 Studio 전반의 팝업창, 버튼, 표(Row) 등의 기본 폰트를 의미함.</p><blockquote><p>일부 변경 사항은 창을 <strong>닫았다 다시 열어야 반영됨</strong></p></blockquote><hr><h2 id="📍-3-폰트-변경-후-마음에-들지-않을-때"><a href="#📍-3-폰트-변경-후-마음에-들지-않을-때" class="headerlink" title="📍 3. 폰트 변경 후 마음에 들지 않을 때"></a>📍 3. 폰트 변경 후 마음에 들지 않을 때</h2><p>언제든지 <strong>Restore Defaults(기본값으로 복원)</strong> → Apply 하면 초기 상태로 돌아갑니다.</p><p>특히 폰트를 과하게 크게&#x2F;작게 설정했을 경우 유용합니다.</p><hr><h2 id="📍-4-SQL-Editor-폰트-변경하기"><a href="#📍-4-SQL-Editor-폰트-변경하기" class="headerlink" title="📍 4. SQL Editor 폰트 변경하기"></a>📍 4. SQL Editor 폰트 변경하기</h2><p>경로:<br><strong>Window → Preferences → General → Appearance → Colors and Fonts → Text Font</strong></p><p>이 폰트는 <strong>SQL Editor(쿼리 작성 창)</strong> 에 적용됩니다.</p><p>예:</p><ul><li>굵게(Bold)</li><li>폰트 변경</li><li>크기 변경</li></ul><p>적용 후 SQL 창 글꼴이 즉시 변경됨.</p><p>복원이 필요하면 <strong>Restore Defaults</strong> 사용.</p><hr><h2 id="📍-5-색상-Custom-Colors-커스터마이징"><a href="#📍-5-색상-Custom-Colors-커스터마이징" class="headerlink" title="📍 5. 색상(Custom Colors) 커스터마이징"></a>📍 5. 색상(Custom Colors) 커스터마이징</h2><p>Colors and Fonts 메뉴에는 수십 가지 색상 옵션이 존재합니다.</p><p>예)</p><ul><li>Error Color (에러 메시지 색)</li><li>View and Editor folders</li><li>Non-Focused Part Color<br>등등</li></ul><p>커스터마이징 시 실시간으로 UI에 반영됨.</p><p>→ 잘못 바꿔도 <strong>Restore Defaults</strong>로 언제든 복구 가능</p><hr><h2 id="📍-6-Result-Set-Viewer-결과-창-설정-변경"><a href="#📍-6-Result-Set-Viewer-결과-창-설정-변경" class="headerlink" title="📍 6. Result Set Viewer(결과 창) 설정 변경"></a>📍 6. Result Set Viewer(결과 창) 설정 변경</h2><p>경로:<br><strong>Window → Preferences → Teradata Datatools → Result Set Viewer</strong></p><p>여기서 결과창의 여러 설정을 조정할 수 있습니다.</p><h3 id="🔸-1-교차-색상-Row-Alternate-Color-켜기"><a href="#🔸-1-교차-색상-Row-Alternate-Color-켜기" class="headerlink" title="🔸 1) 교차 색상(Row Alternate Color) 켜기"></a>🔸 1) 교차 색상(Row Alternate Color) 켜기</h3><ul><li>‘Display alternate result set rows in a different color’ 체크<br>→ 짝수 행&#x2F;홀수 행이 다른 색으로 표시되어 가독성이 좋아짐</li></ul><h3 id="🔸-2-Column-Header-숨기기-표시하기"><a href="#🔸-2-Column-Header-숨기기-표시하기" class="headerlink" title="🔸 2) Column Header 숨기기 &#x2F; 표시하기"></a>🔸 2) Column Header 숨기기 &#x2F; 표시하기</h3><ul><li>Column header 옵션 OFF → 헤더(컬럼명) 숨기기<br>※ 비추천 (컬럼을 알아보기 어려워짐)</li></ul><h3 id="🔸-3-기본으로-불러오는-최대-Row-수-변경"><a href="#🔸-3-기본으로-불러오는-최대-Row-수-변경" class="headerlink" title="🔸 3) 기본으로 불러오는 최대 Row 수 변경"></a>🔸 3) 기본으로 불러오는 최대 Row 수 변경</h3><p>기본값: 2000<br>예: 1000으로 변경 가능</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.TablesV;</span><br></pre></td></tr></table></figure><p>실행 시:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2,000 rows exceed limit. Retrieve first 2,000?</span><br></pre></td></tr></table></figure><p>이 메시지에 나오는 숫자가 해당 설정에 의해 조정됨.</p><hr><h2 id="📍-7-Result-Viewer-옵션에-빠르게-접근하는-법"><a href="#📍-7-Result-Viewer-옵션에-빠르게-접근하는-법" class="headerlink" title="📍 7. Result Viewer 옵션에 빠르게 접근하는 법"></a>📍 7. Result Viewer 옵션에 빠르게 접근하는 법</h2><p>Result Viewer 우측 상단의 <strong>wrench(🔧 아이콘)</strong> 클릭하면<br>바로 Preferences의 동일한 화면으로 이동할 수 있습니다.</p><hr><h2 id="📍-8-세미콜론-사용-습관-들이기"><a href="#📍-8-세미콜론-사용-습관-들이기" class="headerlink" title="📍 8. 세미콜론(;) 사용 습관 들이기"></a>📍 8. 세미콜론(;) 사용 습관 들이기</h2><p>VTEQ나 여러 SQL 에디터에서는 <strong>각 SQL 문장 뒤에 반드시 세미콜론이 필요</strong>합니다.</p><p>예:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.TablesV;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_DATE</span>;</span><br></pre></td></tr></table></figure><p>여러 SQL을 한 번에 실행할 때 오류를 방지하기 위해<br>항상 세미콜론을 붙이는 것이 좋은 습관입니다.</p><hr><h2 id="📍-9-설정-적용-후-테스트-방법"><a href="#📍-9-설정-적용-후-테스트-방법" class="headerlink" title="📍 9. 설정 적용 후 테스트 방법"></a>📍 9. 설정 적용 후 테스트 방법</h2><p>예를 들어 Dialog Font 크기를 조정한 후 결과를 보고 싶다면:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HELP SESSION;</span><br></pre></td></tr></table></figure><p>혹은</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.TablesV;</span><br></pre></td></tr></table></figure><p>을 실행하면 Result Viewer의 폰트&#x2F;색상 적용 상태를 쉽게 확인할 수 있습니다.</p><hr><h2 id="🎯-최종-요약"><a href="#🎯-최종-요약" class="headerlink" title="🎯 최종 요약"></a>🎯 <strong>최종 요약</strong></h2><p>이번 강의에서는 다음을 배웠습니다:</p><h3 id="✔-화면-레이아웃-조절"><a href="#✔-화면-레이아웃-조절" class="headerlink" title="✔ 화면 레이아웃 조절"></a>✔ 화면 레이아웃 조절</h3><ul><li>하단 창 숨기기&#x2F;보이기</li><li>Reset Perspective로 초기화 가능</li></ul><h3 id="✔-UI-커스터마이징"><a href="#✔-UI-커스터마이징" class="headerlink" title="✔ UI 커스터마이징"></a>✔ UI 커스터마이징</h3><ul><li>Dialog Font</li><li>Text Font (SQL Editor)</li><li>색상(Color)</li><li>Row Alternate Color</li></ul><h3 id="✔-Result-Set-Viewer-설정"><a href="#✔-Result-Set-Viewer-설정" class="headerlink" title="✔ Result Set Viewer 설정"></a>✔ Result Set Viewer 설정</h3><ul><li>최대 row 수 변경</li><li>헤더 표시&#x2F;숨김</li><li>행 색상 교차 표시</li></ul><h3 id="✔-실용-팁"><a href="#✔-실용-팁" class="headerlink" title="✔ 실용 팁"></a>✔ 실용 팁</h3><ul><li>🔧 아이콘으로 빠르게 환경설정 이동</li><li>세미콜론(;) 습관화</li><li>HELP SESSION으로 적용 결과 확인</li></ul><hr><p>필요하다면:</p><ul><li>다음 강의 내용도 md 파일로 생성</li><li>이미지 포함 버전</li><li>GitHub README 스타일</li><li>PDF 변환</li></ul><p>언제든 요청해주세요!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;📘-Teradata-Studio-환경-설정-및-화면-커스터마이징&quot;&gt;&lt;a href=&quot;#📘-Teradata-Studio-환경-설정-및-화면-커스터마이징&quot; class=&quot;headerlink&quot; title=&quot;📘 Teradata Studio 환</summary>
      
    
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/categories/TERADATA/"/>
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/tags/TERADATA/"/>
    
  </entry>
  
  <entry>
    <title>TERADATA(1)-Overview of Teradata Studio Modules</title>
    <link href="https://kish191919.github.io/2025/12/03/TERADATA-1-Overview-of-Teradata-Studio-Modules/"/>
    <id>https://kish191919.github.io/2025/12/03/TERADATA-1-Overview-of-Teradata-Studio-Modules/</id>
    <published>2025-12-03T16:27:48.000Z</published>
    <updated>2025-12-03T16:36:04.790Z</updated>
    
    <content type="html"><![CDATA[<h1 id="📘-Teradata-Studio-기본-화면-이해하기"><a href="#📘-Teradata-Studio-기본-화면-이해하기" class="headerlink" title="📘 Teradata Studio 기본 화면 이해하기"></a>📘 Teradata Studio 기본 화면 이해하기</h1><p><em>기초 UI 설명 + SQL 실행 방법 + 트랜잭션 모드 개념</em></p><h2 id="📍-1-Teradata-Studio-첫-화면-구성"><a href="#📍-1-Teradata-Studio-첫-화면-구성" class="headerlink" title="📍 1. Teradata Studio 첫 화면 구성"></a>📍 1. Teradata Studio 첫 화면 구성</h2><p>Teradata Studio를 실행하면 다음과 같은 주요 영역을 보게 됩니다.</p><h3 id="🔹-1-상단-메뉴-File-Edit-Window-등"><a href="#🔹-1-상단-메뉴-File-Edit-Window-등" class="headerlink" title="🔹 1) 상단 메뉴(File &#x2F; Edit &#x2F; Window 등)"></a>🔹 1) 상단 메뉴(File &#x2F; Edit &#x2F; Window 등)</h3><p>일반적인 파일 관리, 보기 설정, 환경설정 변경 등을 수행합니다.</p><h3 id="🔹-2-Studio-Toolbar"><a href="#🔹-2-Studio-Toolbar" class="headerlink" title="🔹 2) Studio Toolbar"></a>🔹 2) Studio Toolbar</h3><p>쿼리 실행, 저장, 새 연결 생성 등 자주 사용하는 기능 아이콘들이 위치합니다.</p><h3 id="🔹-3-Data-Source-Explorer-좌측-영역"><a href="#🔹-3-Data-Source-Explorer-좌측-영역" class="headerlink" title="🔹 3) Data Source Explorer (좌측 영역)"></a>🔹 3) Data Source Explorer (좌측 영역)</h3><p>Teradata에 연결된 <strong>데이터베이스 &#x2F; 유저 &#x2F; 테이블 &#x2F; 뷰</strong> 등을 탐색할 수 있는 가장 중요한 패널입니다.</p><h3 id="🔹-4-Project-Explorer"><a href="#🔹-4-Project-Explorer" class="headerlink" title="🔹 4) Project Explorer"></a>🔹 4) Project Explorer</h3><p>SQL 파일이나 프로젝트를 로컬에 저장하여 관리할 때 사용합니다.</p><h3 id="🔹-5-SQL-Editor-중앙-상단"><a href="#🔹-5-SQL-Editor-중앙-상단" class="headerlink" title="🔹 5) SQL Editor (중앙 상단)"></a>🔹 5) SQL Editor (중앙 상단)</h3><p>SQL을 작성하는 공간입니다.<br>예:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_TIME</span>;</span><br></pre></td></tr></table></figure><h3 id="🔹-6-Result-Set-Viewer-중앙-하단"><a href="#🔹-6-Result-Set-Viewer-중앙-하단" class="headerlink" title="🔹 6) Result Set Viewer (중앙 &#x2F; 하단)"></a>🔹 6) Result Set Viewer (중앙 &#x2F; 하단)</h3><p>쿼리 실행 결과를 표시합니다.</p><h3 id="🔹-7-SQL-History-하단"><a href="#🔹-7-SQL-History-하단" class="headerlink" title="🔹 7) SQL History (하단)"></a>🔹 7) SQL History (하단)</h3><p>실행한 SQL 기록을 확인할 수 있습니다.</p><h3 id="🔹-8-Perspective-메뉴-오른쪽-상단"><a href="#🔹-8-Perspective-메뉴-오른쪽-상단" class="headerlink" title="🔹 8) Perspective 메뉴 (오른쪽 상단)"></a>🔹 8) Perspective 메뉴 (오른쪽 상단)</h3><ul><li>Administration</li><li>Query Development</li><li>Data Transfer</li></ul><hr><h2 id="📍-2-화면이-망가졌을-때-복구-방법"><a href="#📍-2-화면이-망가졌을-때-복구-방법" class="headerlink" title="📍 2. 화면이 망가졌을 때 복구 방법"></a>📍 2. 화면이 망가졌을 때 복구 방법</h2><p><strong>Window → Reset Perspective</strong></p><hr><h2 id="📍-3-SQL-실행하는-방법"><a href="#📍-3-SQL-실행하는-방법" class="headerlink" title="📍 3. SQL 실행하는 방법"></a>📍 3. SQL 실행하는 방법</h2><h3 id="🔸-방법-1-—-F5-가장-많이-사용"><a href="#🔸-방법-1-—-F5-가장-많이-사용" class="headerlink" title="🔸 방법 1 — F5 (가장 많이 사용)"></a>🔸 방법 1 — <code>F5</code> (가장 많이 사용)</h3><ul><li>선택한 SQL만 실행</li><li>선택하지 않으면 전체 실행</li></ul><h3 id="🔸-방법-2-—-Ctrl-Alt-X"><a href="#🔸-방법-2-—-Ctrl-Alt-X" class="headerlink" title="🔸 방법 2 — Ctrl + Alt + X"></a>🔸 방법 2 — <code>Ctrl + Alt + X</code></h3><p>모든 SQL을 하나의 탭으로 실행합니다.</p><h3 id="🔸-방법-3-—-실행-버튼-클릭"><a href="#🔸-방법-3-—-실행-버튼-클릭" class="headerlink" title="🔸 방법 3 — 실행 버튼 클릭"></a>🔸 방법 3 — 실행 버튼 클릭</h3><p>상단의 ▶️ 아이콘 클릭</p><hr><h2 id="📍-4-SQL-실행-테스트"><a href="#📍-4-SQL-실행-테스트" class="headerlink" title="📍 4. SQL 실행 테스트"></a>📍 4. SQL 실행 테스트</h2><h3 id="🔸-현재-시간-확인"><a href="#🔸-현재-시간-확인" class="headerlink" title="🔸 현재 시간 확인"></a>🔸 현재 시간 확인</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_TIME</span>;</span><br></pre></td></tr></table></figure><h3 id="🔸-DBC-데이터베이스-조회"><a href="#🔸-DBC-데이터베이스-조회" class="headerlink" title="🔸 DBC 데이터베이스 조회"></a>🔸 DBC 데이터베이스 조회</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.DatabasesV;</span><br></pre></td></tr></table></figure><p>DBC는 Teradata의 메타데이터를 보관하는 핵심 데이터베이스입니다.</p><hr><h2 id="📍-5-다중-SQL-실행-시-주의점"><a href="#📍-5-다중-SQL-실행-시-주의점" class="headerlink" title="📍 5. 다중 SQL 실행 시 주의점"></a>📍 5. 다중 SQL 실행 시 주의점</h2><p>각 SQL 문 뒤에는 반드시 **세미콜론(;)**이 필요합니다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.DatabasesV;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_DATE</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_TIME</span>;</span><br></pre></td></tr></table></figure><hr><h2 id="📍-6-HELP-SESSION"><a href="#📍-6-HELP-SESSION" class="headerlink" title="📍 6. HELP SESSION"></a>📍 6. HELP SESSION</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HELP SESSION;</span><br></pre></td></tr></table></figure><p>세션 정보(사용자, 로그인 시간, 모드 등)을 확인할 수 있습니다.</p><hr><h2 id="📍-7-트랜잭션-모드-ANSI-vs-Teradata"><a href="#📍-7-트랜잭션-모드-ANSI-vs-Teradata" class="headerlink" title="📍 7. 트랜잭션 모드 (ANSI vs Teradata)"></a>📍 7. 트랜잭션 모드 (ANSI vs Teradata)</h2><h3 id="🔸-Teradata-모드"><a href="#🔸-Teradata-모드" class="headerlink" title="🔸 Teradata 모드"></a>🔸 Teradata 모드</h3><ul><li>대소문자 구분 적음</li><li>기본 추천 모드</li></ul><h3 id="🔸-ANSI-모드"><a href="#🔸-ANSI-모드" class="headerlink" title="🔸 ANSI 모드"></a>🔸 ANSI 모드</h3><ul><li>표준 SQL 준수</li><li>COMMIT 필요</li><li>대소문자 구분 엄격</li></ul><hr><h2 id="📍-8-트랜잭션-UNIT-OF-WORK-개념"><a href="#📍-8-트랜잭션-UNIT-OF-WORK-개념" class="headerlink" title="📍 8. 트랜잭션(UNIT OF WORK) 개념"></a>📍 8. 트랜잭션(UNIT OF WORK) 개념</h2><p>트랜잭션 &#x3D; 작업 단위<br>전부 성공하거나, 실패 시 전체 롤백되는 방식입니다.</p><p>예: INSERT 중 정전 → 일부 컬럼만 저장되지 않고 전체 취소됨</p><hr><h2 id="🎯-최종-정리"><a href="#🎯-최종-정리" class="headerlink" title="🎯 최종 정리"></a>🎯 최종 정리</h2><ul><li>UI 구성 이해</li><li>SQL 실행 단축키(F5 &#x2F; Ctrl+Alt+X)</li><li>DBC 데이터 조회</li><li>트랜잭션 모드 차이</li><li>HELP SESSION 활용</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;📘-Teradata-Studio-기본-화면-이해하기&quot;&gt;&lt;a href=&quot;#📘-Teradata-Studio-기본-화면-이해하기&quot; class=&quot;headerlink&quot; title=&quot;📘 Teradata Studio 기본 화면 이해하기&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/categories/TERADATA/"/>
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/tags/TERADATA/"/>
    
  </entry>
  
  <entry>
    <title>Coding_Quiz_SQL_1</title>
    <link href="https://kish191919.github.io/2025/09/17/Coding-Quiz-SQL-1/"/>
    <id>https://kish191919.github.io/2025/09/17/Coding-Quiz-SQL-1/</id>
    <published>2025-09-18T02:07:37.000Z</published>
    <updated>2025-11-20T13:04:44.634Z</updated>
    
    
    
    
    <category term="DEV" scheme="https://kish191919.github.io/categories/DEV/"/>
    
    <category term="CODING_QUIZ_SQL" scheme="https://kish191919.github.io/categories/DEV/CODING-QUIZ-SQL/"/>
    
    
    <category term="LEETCODE" scheme="https://kish191919.github.io/tags/LEETCODE/"/>
    
    <category term="SQL" scheme="https://kish191919.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Coding_Quiz_Python_1</title>
    <link href="https://kish191919.github.io/2025/09/17/Coding-Quiz-Python-1/"/>
    <id>https://kish191919.github.io/2025/09/17/Coding-Quiz-Python-1/</id>
    <published>2025-09-17T20:29:17.000Z</published>
    <updated>2025-11-20T13:04:44.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Two-Sum"><a href="#1-Two-Sum" class="headerlink" title="1. Two Sum"></a>1. Two Sum</h1><p>Link : <a href="https://leetcode.com/problems/two-sum/description/?difficulty=EASY">https://leetcode.com/problems/two-sum/description/?difficulty=EASY</a></p><h2 id="Hash-Map-딕셔너리-이용"><a href="#Hash-Map-딕셔너리-이용" class="headerlink" title="Hash Map (딕셔너리 이용)"></a>Hash Map (딕셔너리 이용)</h2><p>보조 공간을 이용해 빠르게 차이를 찾는 방식.</p><ul><li>시간 복잡도: <strong>O(n)</strong></li><li>공간 복잡도: <strong>O(n)</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        num_map = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            diff = target - num</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> diff <span class="keyword">in</span> num_map:</span><br><span class="line">                <span class="keyword">return</span> [num_map[diff], index]</span><br><span class="line">            </span><br><span class="line">            num_map[num] = index</span><br></pre></td></tr></table></figure><h2 id="Sorting-Two-Pointers-정렬-투-포인터"><a href="#Sorting-Two-Pointers-정렬-투-포인터" class="headerlink" title="Sorting + Two Pointers (정렬 + 투 포인터)"></a>Sorting + Two Pointers (정렬 + 투 포인터)</h2><p>정렬 후 양 끝에서 합을 비교.</p><ul><li>시간 복잡도: <strong>O(n log n)</strong></li><li>공간 복잡도: <strong>O(n)</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line"></span><br><span class="line">    arr = [(num, i) <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums)]</span><br><span class="line">    arr.sort()</span><br><span class="line"></span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        s = arr[left][<span class="number">0</span>] + arr[right][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> s == target:</span><br><span class="line">            <span class="keyword">return</span> [arr[left][<span class="number">1</span>], arr[right][<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">elif</span> s &lt; target:</span><br><span class="line">            left +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right -=<span class="number">1</span></span><br></pre></td></tr></table></figure><h1 id="2-Add-Two-Numbers"><a href="#2-Add-Two-Numbers" class="headerlink" title="2. Add Two Numbers"></a>2. Add Two Numbers</h1><p>Link : <a href="https://leetcode.com/problems/add-two-numbers/description/?difficulty=EASY">https://leetcode.com/problems/add-two-numbers/description/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1: <span class="type">Optional</span>[ListNode], l2: <span class="type">Optional</span>[ListNode], c=<span class="number">0</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">    sum_ = l1.val + l2.val + c</span><br><span class="line">    remain = sum_ % <span class="number">10</span></span><br><span class="line">    carry = sum_ // <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    new_node = ListNode(remain)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> l1.<span class="built_in">next</span> != <span class="literal">None</span> <span class="keyword">or</span> l2.<span class="built_in">next</span> !=<span class="literal">None</span> <span class="keyword">or</span> carry != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> l1.<span class="built_in">next</span> == <span class="literal">None</span>:</span><br><span class="line">            l1.<span class="built_in">next</span> = ListNode(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> l2.<span class="built_in">next</span> == <span class="literal">None</span>:</span><br><span class="line">            l2.<span class="built_in">next</span> = ListNode(<span class="number">0</span>)</span><br><span class="line">        new_node.<span class="built_in">next</span> = <span class="variable language_">self</span>.addTwoNumbers(l1.<span class="built_in">next</span>, l2.<span class="built_in">next</span>, carry)</span><br><span class="line">    <span class="keyword">return</span> new_node</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1: <span class="type">Optional</span>[ListNode], l2: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">        current = dummy</span><br><span class="line">        carry = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2 <span class="keyword">or</span> carry:</span><br><span class="line">            val1 = l1.val <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            val2 = l2.val <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            total = val1 + val2 + carry</span><br><span class="line">            remain = total % <span class="number">10</span></span><br><span class="line">            carry = total // <span class="number">10</span></span><br><span class="line"></span><br><span class="line">            current.<span class="built_in">next</span> = ListNode(remain)</span><br><span class="line">            current = current.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> l1: l1 = l1.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> l2: l2 = l2.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure><h1 id="3-Longest-Substring-Without-Repeating-Characters"><a href="#3-Longest-Substring-Without-Repeating-Characters" class="headerlink" title="3. Longest Substring Without Repeating Characters"></a>3. Longest Substring Without Repeating Characters</h1><p>Link : <a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/?difficulty=EASY">https://leetcode.com/problems/longest-substring-without-repeating-characters/description/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        max_num = <span class="number">0</span></span><br><span class="line">        temp = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            <span class="keyword">while</span> s[i] <span class="keyword">in</span> temp:</span><br><span class="line">                temp.pop(<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            temp.append(s[i])</span><br><span class="line">            max_num = <span class="built_in">max</span>(<span class="built_in">len</span>(temp), max_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_num</span><br></pre></td></tr></table></figure><h2 id="투-포인터-set"><a href="#투-포인터-set" class="headerlink" title="투 포인터 + set"></a>투 포인터 + set</h2><ul><li>같은 슬라이딩 윈도우인데 set으로 포함 여부를 O(1)에 체크.</li><li>왼쪽 포인터(left)를 움직이며 중복을 제거.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        max_num = <span class="number">0</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        temp = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> right, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> ch <span class="keyword">in</span> temp:</span><br><span class="line">                temp.remove(s[left])</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            temp.add(ch)</span><br><span class="line"></span><br><span class="line">            max_num = <span class="built_in">max</span>(max_num, <span class="built_in">len</span>(temp))</span><br><span class="line">        <span class="keyword">return</span> max_num</span><br></pre></td></tr></table></figure><h1 id="5-Longest-Palindromic-Substring"><a href="#5-Longest-Palindromic-Substring" class="headerlink" title="5. Longest Palindromic Substring"></a>5. Longest Palindromic Substring</h1><p>Link : <a href="https://leetcode.com/problems/longest-palindromic-substring/?difficulty=EASY">https://leetcode.com/problems/longest-palindromic-substring/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right +=<span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> s[left+<span class="number">1</span> : right]</span><br><span class="line">        max_str = s[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)-<span class="number">1</span>):</span><br><span class="line">            odd = expand(i, i)</span><br><span class="line">            even = expand(i, i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(odd) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = odd</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(even) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = even</span><br><span class="line">        <span class="keyword">return</span> max_str</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">            <span class="comment"># valid palindrome is s[left+1 : right] (right is exclusive)</span></span><br><span class="line">            <span class="keyword">return</span> left+<span class="number">1</span>, right</span><br><span class="line">        max_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            l1, r1 = expand(i, i)</span><br><span class="line">            l2, r2 = expand(i, i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            odd_str = s[l1:r1]</span><br><span class="line">            even_str = s[l2:r2]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(odd_str) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = odd_str</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(even_str) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = even_str</span><br><span class="line">        <span class="keyword">return</span> max_str</span><br></pre></td></tr></table></figure><h1 id="6-Zigzag-Conversion"><a href="#6-Zigzag-Conversion" class="headerlink" title="6. Zigzag Conversion"></a>6. Zigzag Conversion</h1><p>Link : <a href="https://leetcode.com/problems/zigzag-conversion/description/?difficulty=EASY">https://leetcode.com/problems/zigzag-conversion/description/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">self, s: <span class="built_in">str</span>, numRows: <span class="built_in">int</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> numRows == <span class="number">1</span> <span class="keyword">or</span> numRows &gt;= <span class="built_in">len</span>(s):</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        </span><br><span class="line">        rows = [[] <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(numRows)]</span><br><span class="line"></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> s:</span><br><span class="line">            rows[index].append(char)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">                step = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> index == numRows-<span class="number">1</span>:</span><br><span class="line">                step = -<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            index += step</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numRows):</span><br><span class="line">            rows[i] = <span class="string">&#x27;&#x27;</span>.join(rows[i])</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(rows)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-Two-Sum&quot;&gt;&lt;a href=&quot;#1-Two-Sum&quot; class=&quot;headerlink&quot; title=&quot;1. Two Sum&quot;&gt;&lt;/a&gt;1. Two Sum&lt;/h1&gt;&lt;p&gt;Link : &lt;a href=&quot;https://leetcode.com/pro</summary>
      
    
    
    
    <category term="DEV" scheme="https://kish191919.github.io/categories/DEV/"/>
    
    <category term="CODING_QUIZ_PYTHON" scheme="https://kish191919.github.io/categories/DEV/CODING-QUIZ-PYTHON/"/>
    
    
    <category term="PYTHON" scheme="https://kish191919.github.io/tags/PYTHON/"/>
    
    <category term="LEETCODE" scheme="https://kish191919.github.io/tags/LEETCODE/"/>
    
  </entry>
  
  <entry>
    <title>Databricks CV Anomaly Detection</title>
    <link href="https://kish191919.github.io/2025/09/15/Databricks-CV-Anomaly-Detection/"/>
    <id>https://kish191919.github.io/2025/09/15/Databricks-CV-Anomaly-Detection/</id>
    <published>2025-09-15T20:40:23.000Z</published>
    <updated>2025-09-16T01:28:50.347Z</updated>
    
    <content type="html"><![CDATA[<h2 id="👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-Deployment"><a href="#👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-Deployment" class="headerlink" title="👁️ Databricks + Computer Vision Anomaly Detection &amp; Model Deployment"></a>👁️ Databricks + Computer Vision Anomaly Detection &amp; Model Deployment</h2><p><em>A complete guide to anomaly detection with Databricks and Apache Spark</em>  </p><blockquote><p>“From data ingestion to real-time serving — build and deploy scalable computer vision anomaly detection models.”</p></blockquote><p>📎 <strong>Full Project</strong>:<br><a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection">👉 View Jupyter Notebooks on GitHub</a></p><p align="center">  <img src="/images/Anomaly.png" width="80%"></p><hr><h3 id="📌-One-Line-Summary"><a href="#📌-One-Line-Summary" class="headerlink" title="📌 One-Line Summary"></a>📌 One-Line Summary</h3><p>This project provides a full pipeline for <strong>computer vision–based anomaly detection</strong>, covering <strong>data ingestion, preprocessing, model training, deployment, and REST API serving</strong> — all within <strong>Databricks</strong> and powered by <strong>Apache Spark</strong>.</p><hr><h2 id="1️⃣-How-It-Was-Built"><a href="#1️⃣-How-It-Was-Built" class="headerlink" title="1️⃣ How It Was Built"></a>1️⃣ How It Was Built</h2><h3 id="1-Utilities-00-utils-ipynb"><a href="#1-Utilities-00-utils-ipynb" class="headerlink" title="1. Utilities (00_utils.ipynb)"></a><strong>1. Utilities (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/00_utils.ipynb">00_utils.ipynb</a>)</strong></h3><ul><li>Common helper functions for preprocessing and visualization  </li><li>Reusable utilities to streamline workflows</li></ul><hr><h3 id="2-Data-Ingestion-ETL-01-Ingestion-ETL-ipynb"><a href="#2-Data-Ingestion-ETL-01-Ingestion-ETL-ipynb" class="headerlink" title="2. Data Ingestion &amp; ETL (01_Ingestion_ETL.ipynb)"></a><strong>2. Data Ingestion &amp; ETL (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/01_Ingestion_ETL.ipynb">01_Ingestion_ETL.ipynb</a>)</strong></h3><ul><li>Ingested large-scale image datasets into Databricks  </li><li>Implemented Spark-based ETL for scalability  </li><li>Optimized storage and partitioning for performance and cost efficiency </li><li>Image Processing Visualization</li></ul><p align="center">  <img src="/images/image_processing_visualization.png" width="80%"></p><hr><h3 id="3-Deep-Learning-Training-02-HF-Deep-Learning-ipynb"><a href="#3-Deep-Learning-Training-02-HF-Deep-Learning-ipynb" class="headerlink" title="3. Deep Learning Training (02_HF_Deep_Learning.ipynb)"></a><strong>3. Deep Learning Training (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/02_HF_Deep_Learning.ipynb">02_HF_Deep_Learning.ipynb</a>)</strong></h3><ul><li>Applied image preprocessing and augmentation  </li><li>Trained models using <strong>PyTorch + Hugging Face</strong>  </li><li>Evaluated performance with metrics like <strong>Accuracy</strong>, <strong>Loss</strong>, and <strong>PR-AUC</strong></li></ul><hr><h3 id="4-Model-Deployment-03-Model-Deployment-ipynb"><a href="#4-Model-Deployment-03-Model-Deployment-ipynb" class="headerlink" title="4. Model Deployment (03_Model_Deployment.ipynb)"></a><strong>4. Model Deployment (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/03_Model_Deployment.ipynb">03_Model_Deployment.ipynb</a>)</strong></h3><ul><li>Registered trained models in <strong>MLflow</strong>  </li><li>Managed versions for reproducibility  </li><li>Optimized inference pipelines for deployment</li></ul><hr><h3 id="5-Model-Serving-04-Model-Serving-ipynb"><a href="#5-Model-Serving-04-Model-Serving-ipynb" class="headerlink" title="5. Model Serving (04_Model_Serving.ipynb)"></a><strong>5. Model Serving (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/04_Model_Serving.ipynb">04_Model_Serving.ipynb</a>)</strong></h3><ul><li>Deployed models with <strong>Databricks Model Serving</strong>  </li><li>Exposed REST API endpoints for real-time predictions  </li><li>Integrated anomaly detection into external systems</li></ul><hr><h2 id="2️⃣-Optimization-Best-Practices"><a href="#2️⃣-Optimization-Best-Practices" class="headerlink" title="2️⃣ Optimization &amp; Best Practices"></a>2️⃣ Optimization &amp; Best Practices</h2><ul><li>Spark optimizations for large-scale image data  </li><li>Databricks cluster configuration for <strong>cost efficiency</strong>  </li><li>Strategies for balancing performance and resource usage</li></ul><hr><h2 id="🛠-Technologies-Used"><a href="#🛠-Technologies-Used" class="headerlink" title="🛠 Technologies Used"></a>🛠 Technologies Used</h2><table><thead><tr><th>Step</th><th>Technology</th></tr></thead><tbody><tr><td>Data Processing</td><td>Apache Spark, Databricks</td></tr><tr><td>Deep Learning</td><td>PyTorch, Hugging Face</td></tr><tr><td>Experiment Mgmt</td><td>MLflow</td></tr><tr><td>Deployment</td><td>Databricks Model Registry</td></tr><tr><td>Serving</td><td>REST API, Databricks Serving</td></tr></tbody></table><hr><h2 id="💡-Key-Learnings"><a href="#💡-Key-Learnings" class="headerlink" title="💡 Key Learnings"></a>💡 Key Learnings</h2><ul><li>Full lifecycle ML on Databricks: ingestion → training → deployment → serving  </li><li>How to optimize Databricks for <strong>low-cost, high-performance workflows</strong>  </li><li>Practical experience with model versioning, reproducibility, and API integration</li></ul><hr><h2 id="🔗-GitHub-Repository"><a href="#🔗-GitHub-Repository" class="headerlink" title="🔗 GitHub Repository"></a>🔗 GitHub Repository</h2><p>📂 <a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection">View Project on GitHub</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-Deployment&quot;&gt;&lt;a href=&quot;#👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-</summary>
      
    
    
    
    <category term="Showcase" scheme="https://kish191919.github.io/categories/Showcase/"/>
    
    
    <category term="Databricks" scheme="https://kish191919.github.io/tags/Databricks/"/>
    
    <category term="Apache Spark" scheme="https://kish191919.github.io/tags/Apache-Spark/"/>
    
    <category term="Computer Vision" scheme="https://kish191919.github.io/tags/Computer-Vision/"/>
    
    <category term="Deep Learning" scheme="https://kish191919.github.io/tags/Deep-Learning/"/>
    
    <category term="MLflow" scheme="https://kish191919.github.io/tags/MLflow/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (6) - Amazon S3 핵심 정리</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-6/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-6/</id>
    <published>2025-09-15T03:49:25.000Z</published>
    <updated>2025-09-15T04:13:20.934Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Amazon-S3-보안-Amazon-S3-Security"><a href="#Amazon-S3-보안-Amazon-S3-Security" class="headerlink" title="Amazon S3 보안 (Amazon S3 Security)"></a>Amazon S3 보안 (Amazon S3 Security)</h1><p>Amazon S3는 단순한 저장소 서비스지만, <strong>보안(Security)</strong> 을 제대로 설정하지 않으면 데이터 유출(Data Leak)과 같은 심각한 문제가 발생할 수 있습니다.  AWS Certified Machine Learning Engineer – Associate 시험에서도 자주 출제되는 주제이므로 꼭 이해해야 합니다.</p><hr><h2 id="1-S3-보안-유형"><a href="#1-S3-보안-유형" class="headerlink" title="1. S3 보안 유형"></a>1. S3 보안 유형</h2><h3 id="🔹-User-Based-IAM-기반"><a href="#🔹-User-Based-IAM-기반" class="headerlink" title="🔹 User-Based (IAM 기반)"></a>🔹 User-Based (IAM 기반)</h3><ul><li><strong>IAM Policies</strong><br>IAM(Identity and Access Management)에서 특정 사용자(User) 또는 그룹(Group)에 대해 어떤 API 호출(API Calls)을 허용할지 정의합니다.<br>→ 예: <code>s3:GetObject</code> 권한 부여.</li></ul><h3 id="🔹-Resource-Based-리소스-기반"><a href="#🔹-Resource-Based-리소스-기반" class="headerlink" title="🔹 Resource-Based (리소스 기반)"></a>🔹 Resource-Based (리소스 기반)</h3><ul><li><p><strong>Bucket Policies</strong>  </p><ul><li>JSON 기반 정책으로, 버킷 전체에 대한 접근 권한을 설정합니다.  </li><li><strong>Cross-Account Access</strong>(계정 간 접근)도 허용 가능.  </li><li>버킷 정책은 <strong>S3 콘솔</strong>에서 직접 작성&#x2F;관리.</li></ul></li><li><p><strong>Object ACL (Access Control List)</strong>  </p><ul><li>객체 단위로 세밀하게 접근 제어.  </li><li>하지만 현재는 <strong>비추천(Deprecated)</strong> → 대부분 <strong>버킷 정책</strong>으로 대체.</li></ul></li><li><p><strong>Bucket ACL</strong>  </p><ul><li>버킷 단위 ACL. 거의 사용하지 않으며 역시 비추천.</li></ul></li></ul><hr><h2 id="2-IAM-권한-평가-규칙"><a href="#2-IAM-권한-평가-규칙" class="headerlink" title="2. IAM 권한 평가 규칙"></a>2. IAM 권한 평가 규칙</h2><p>S3 객체에 접근하려면 다음 조건을 만족해야 합니다:</p><ul><li><strong>IAM 정책이 ALLOW</strong> 이거나 <strong>리소스 정책이 ALLOW</strong>  </li><li>그리고 <strong>명시적 DENY가 없어야 함</strong></li></ul><p>👉 즉, <code>ALLOW OR ALLOW</code> 이면서 동시에 <code>NO DENY</code> 조건이어야 함.</p><hr><h2 id="3-S3-버킷-정책-Bucket-Policies"><a href="#3-S3-버킷-정책-Bucket-Policies" class="headerlink" title="3. S3 버킷 정책 (Bucket Policies)"></a>3. S3 버킷 정책 (Bucket Policies)</h2><ul><li><strong>형식</strong>: JSON 기반 문서  </li><li><strong>구성 요소</strong><ul><li><strong>Resource</strong>: 적용 대상 (버킷&#x2F;객체 ARN)  </li><li><strong>Effect</strong>: <code>Allow</code> 또는 <code>Deny</code>  </li><li><strong>Action</strong>: 허용&#x2F;거부할 API 목록 (<code>s3:GetObject</code>, <code>s3:PutObject</code> 등)  </li><li><strong>Principal</strong>: 정책 적용 대상 (계정, 사용자, 역할, <code>*</code> &#x3D; 모든 사용자)</li></ul></li></ul><h3 id="예시-1-퍼블릭-읽기-Public-Access"><a href="#예시-1-퍼블릭-읽기-Public-Access" class="headerlink" title="예시 1: 퍼블릭 읽기 (Public Access)"></a>예시 1: 퍼블릭 읽기 (Public Access)</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Principal&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;s3:GetObject&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:s3:::my-example-bucket/*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>이 정책은 <code>my-example-bucket</code> 안의 모든 객체(<code>*</code>)를 <strong>누구나 읽기 가능</strong>하도록 허용.</li></ul><hr><h2 id="4-보안-시나리오별-접근-방법"><a href="#4-보안-시나리오별-접근-방법" class="headerlink" title="4. 보안 시나리오별 접근 방법"></a>4. 보안 시나리오별 접근 방법</h2><ul><li><strong>퍼블릭 접근 (Public Access)</strong>  <ul><li>버킷 정책으로 <code>GetObject</code> 권한을 열어줌.  </li><li>단, <strong>Block Public Access 설정</strong>을 해제해야 함.</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-02.png" width="80%"></p><ul><li><strong>내부 사용자 (IAM User)</strong>  <ul><li>IAM 정책으로 권한 부여 (<code>s3:ListBucket</code>, <code>s3:GetObject</code> 등).</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-03.png" width="80%"></p><ul><li><strong>EC2 인스턴스에서 접근</strong>  <ul><li>IAM User 사용 ❌ → <strong>IAM Role</strong>을 EC2에 부여해야 함.</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-04.png" width="80%"></p><ul><li><strong>Cross-Account Access (계정 간 접근)</strong>  <ul><li>다른 AWS 계정의 IAM User가 접근하려면 → <strong>버킷 정책</strong>으로 허용.</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-05.png" width="80%"></p><hr><h2 id="5-Block-Public-Access-설정"><a href="#5-Block-Public-Access-설정" class="headerlink" title="5. Block Public Access 설정"></a>5. Block Public Access 설정</h2><ul><li>AWS가 <strong>데이터 유출 방지(Data Leak Prevention)</strong> 를 위해 도입한 기능.  </li><li>기본적으로 모든 퍼블릭 접근을 막음.  </li><li>✅ 시험 포인트:  <ul><li>버킷을 공개해야 한다면 반드시 <strong>Block Public Access를 해제</strong>해야 함.  </li><li>하지만 <strong>기업 환경에서는 대부분 항상 켜둔다</strong>.  </li><li>계정 레벨에서도 적용 가능 → <strong>모든 버킷이 퍼블릭 차단됨</strong>.</li></ul></li></ul><hr><h2 id="6-암호화-Encryption"><a href="#6-암호화-Encryption" class="headerlink" title="6. 암호화 (Encryption)"></a>6. 암호화 (Encryption)</h2><ul><li><strong>SSE-S3</strong>: S3가 자체적으로 키 관리 (간단, 기본 옵션).  </li><li><strong>SSE-KMS</strong>: AWS KMS(Key Management Service)를 사용해 키 관리 (더 세밀한 보안, 로깅 가능).  </li><li>시험에서는 <strong>SSE-S3와 SSE-KMS 차이점</strong>을 잘 물어봄.</li></ul><hr><h1 id="✅-시험-대비-핵심-정리"><a href="#✅-시험-대비-핵심-정리" class="headerlink" title="✅ 시험 대비 핵심 정리"></a>✅ 시험 대비 핵심 정리</h1><ol><li><p><strong>IAM Policy vs Bucket Policy</strong>  </p><ul><li>IAM Policy: 사용자(User) 관점에서 권한 관리.  </li><li>Bucket Policy: 리소스(Resource) 관점에서 권한 관리.</li></ul></li><li><p><strong>객체 접근 조건</strong>  </p><ul><li>ALLOW(사용자 정책 OR 리소스 정책) + NO DENY.</li></ul></li><li><p><strong>EC2에서 S3 접근</strong> → <strong>IAM Role 사용</strong>.  </p></li><li><p><strong>Cross-Account Access</strong> → <strong>버킷 정책 필요</strong>.  </p></li><li><p><strong>Block Public Access</strong>  </p><ul><li>기본 ON (데이터 유출 방지).  </li><li>시험에서는 “퍼블릭 버킷이 동작하지 않는다 → Block Public Access 때문” 자주 출제.</li></ul></li><li><p><strong>암호화 옵션</strong>  </p><ul><li>SSE-S3 (자동, 간단) vs SSE-KMS (보안·규제 요구사항 대응).</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Amazon-S3-보안-Amazon-S3-Security&quot;&gt;&lt;a href=&quot;#Amazon-S3-보안-Amazon-S3-Security&quot; class=&quot;headerlink&quot; title=&quot;Amazon S3 보안 (Amazon S3 Securi</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (5) - Amazon S3 핵심 정리</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-5/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-5/</id>
    <published>2025-09-15T02:33:24.000Z</published>
    <updated>2025-09-15T03:49:06.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Amazon-S3-핵심-정리"><a href="#Amazon-S3-핵심-정리" class="headerlink" title="Amazon S3 핵심 정리"></a>Amazon S3 핵심 정리</h1><blockquote><p><strong>왜 중요한가?</strong><br>Amazon S3(Simple Storage Service)는 AWS의 핵심 스토리지 서비스로, “사실상 무한대(virtually unlimited)” 확장성을 제공하는 <strong>객체 스토리지(Object Storage)</strong> 입니다. 대부분의 데이터&#x2F;AI 워크로드가 S3를 중심으로 연결되며, 다른 AWS 서비스와의 통합성이 매우 뛰어납니다.</p></blockquote><hr><h2 id="1-S3가-쓰이는-곳-Use-Cases"><a href="#1-S3가-쓰이는-곳-Use-Cases" class="headerlink" title="1) S3가 쓰이는 곳 (Use Cases)"></a>1) S3가 쓰이는 곳 (Use Cases)</h2><ul><li><strong>백업 &amp; 스토리지(Backup &amp; Storage)</strong>: 장기 보관, 스냅샷 저장.</li><li><strong>재해복구(Disaster Recovery, DR)</strong>: 다른 <strong>리전(Region)</strong> 으로 복제해 RTO&#x2F;RPO 개선.</li><li><strong>아카이브(Archive)</strong>: 저비용 보관(<strong>S3 Glacier</strong> 계열) 후 필요 시 복원.</li><li><strong>하이브리드 스토리지(Hybrid Cloud Storage)</strong>: 온프레미스 + 클라우드 연동.</li><li><strong>애플리케이션&#x2F;미디어 호스팅(App&#x2F;Media Hosting)</strong>: 이미지&#x2F;동영상&#x2F;정적 파일 제공.</li><li><strong>데이터 레이크(Data Lake) &amp; 빅데이터 분석(Big Data Analytics)</strong>: 원천 데이터 저장 후 <strong>Athena&#x2F;Glue&#x2F;Lake Formation</strong> 등으로 분석.</li><li><strong>소프트웨어 배포(Software Delivery)</strong>: 설치 파일, 모델 아티팩트 배포.</li><li><strong>정적 웹사이트(Static Website)</strong>: 정적 호스팅 + <strong>CloudFront(CDN)</strong> 로 가속.</li></ul><blockquote><p><strong>MLA-C01 포인트</strong>: S3는 <strong>SageMaker</strong>와 함께 자주 출제됩니다. 학습&#x2F;추론 데이터 저장, 모델 아티팩트 저장, <strong>Athena&#x2F;Glue</strong>와 연계한 피처 추출 파이프라인 등. S3 경로 표기(<strong>S3 URI</strong>)와 권한 모델을 익히세요.</p></blockquote><hr><h2 id="2-버킷-Bucket-—-최상위-네임스페이스"><a href="#2-버킷-Bucket-—-최상위-네임스페이스" class="headerlink" title="2) 버킷(Bucket) — 최상위 네임스페이스"></a>2) 버킷(Bucket) — 최상위 네임스페이스</h2><ul><li><strong>정의</strong>: 객체(파일)를 담는 <strong>컨테이너</strong>. UI가 폴더처럼 보여도, 실제로는 <strong>버킷&#x2F;키(key)</strong> 기반의 평면 구조.</li><li><strong>글로벌 유니크 이름(Global Unique Name)</strong>: <strong>전 세계 모든 계정&#x2F;리전에서 유일</strong>해야 함.</li><li><strong>리전 범위(Region-scoped)</strong>: 버킷은 <strong>특정 리전</strong>에 생성됩니다. (S3가 글로벌처럼 보여도 생성 위치는 리전)</li><li><strong>네이밍 규칙(Naming Rules)</strong> (대표 규칙만 발췌)<ul><li>영문 소문자&#x2F;숫자&#x2F;하이픈만 사용, <strong>대문자&#x2F;언더스코어 불가</strong></li><li>길이 3~63자</li><li>IP 형태 금지(예: <code>192.168.0.1</code>)</li><li><code>xn--</code> 로 시작 금지, <code>-s3alias</code> 로 끝 금지</li></ul></li><li><strong>권장 초기 설정</strong><ul><li><strong>ACL 비활성화(ACLs disabled, Bucket owner enforced)</strong>  </li><li><strong>퍼블릭 접근 차단(Block Public Access)</strong>: 기본 on  </li><li><strong>서버사이드 암호화(Server-Side Encryption, SSE)</strong>: 기본 <strong>SSE-S3</strong> 또는 <strong>SSE-KMS</strong></li></ul></li></ul><blockquote><p><strong>시험 포인트</strong>  </p><ul><li>“버킷 이름 글로벌 유일”과 “버킷은 리전에 종속”을 구분하세요.  </li><li>보안 기본값: <strong>Block Public Access &#x3D; ON</strong>, <strong>ACL 비권장</strong>, <strong>SSE 적용</strong>.</li></ul></blockquote><hr><h2 id="3-객체-Object-키-Key"><a href="#3-객체-Object-키-Key" class="headerlink" title="3) 객체(Object) &amp; 키(Key)"></a>3) 객체(Object) &amp; 키(Key)</h2><ul><li><strong>키(Key) &#x3D; 전체 경로(Full Path)</strong>  <ul><li>예: <code>s3://my-bucket/my_folder1/another_folder/my_file.txt</code>  </li><li><strong>Prefix + Object Name</strong> 조합 (디렉터리 개념은 UI 편의일 뿐, 실제로는 긴 문자열 경로)</li></ul></li><li><strong>크기 제한(Size Limits)</strong><ul><li><strong>최대 5TB</strong></li><li><strong>5GB 초과 업로드는 반드시 <em>멀티파트 업로드(Multi-part Upload)</em></strong> 사용</li></ul></li><li><strong>메타데이터(Metadata)</strong>: 시스템&#x2F;사용자 정의 Key-Value</li><li><strong>태그(Tags)</strong>: 최대 10쌍, 비용&#x2F;수명주기&#x2F;보안 정책에 유용</li><li><strong>버전 ID(Version ID)</strong>: <strong>버전 관리(Versioning)</strong> 활성화 시 부여</li></ul><blockquote><p><strong>시험 포인트</strong>  </p><ul><li><strong>5GB 초과 → Multi-part Upload 필수</strong>  </li><li><strong>Versioning</strong>: 삭제&#x2F;덮어쓰기 보호, <strong>MFA Delete</strong> 와 함께 보안 강화</li></ul></blockquote><hr><h2 id="4-접근-방법-퍼블릭-URL-vs-프리사인드-URL"><a href="#4-접근-방법-퍼블릭-URL-vs-프리사인드-URL" class="headerlink" title="4) 접근 방법: 퍼블릭 URL vs 프리사인드 URL"></a>4) 접근 방법: 퍼블릭 URL vs 프리사인드 URL</h2><ul><li><strong>Public URL</strong>: 객체가 <strong>퍼블릭</strong>이어야 접근 가능(기본은 차단됨).</li><li><strong>Pre-signed URL</strong>: <strong>임시 권한이 서명된 URL</strong>. 비공개 객체도 <strong>서명 만료 시간 동안</strong> 접근 가능. 안전한 1회성 공유에 적합.</li></ul><blockquote><p><strong>시험 포인트</strong>: “객체는 비공개인데 외부와 잠깐 공유하고 싶다” → <strong>Pre-signed URL</strong> 정답.</p></blockquote><hr><h2 id="5-필수-보안-운영-기능-Security-Operations"><a href="#5-필수-보안-운영-기능-Security-Operations" class="headerlink" title="5) 필수 보안&#x2F;운영 기능 (Security &amp; Operations)"></a>5) 필수 보안&#x2F;운영 기능 (Security &amp; Operations)</h2><ul><li><strong>암호화(Encryption)</strong><ul><li><strong>SSE-S3</strong>(Amazon S3 managed keys) — 가장 간단</li><li><strong>SSE-KMS</strong>(AWS KMS keys) — <strong>키 사용 권한&#x2F;감사</strong> 필요 시</li><li><strong>CSE(Client-Side Encryption)</strong> — 클라이언트에서 암호화 후 업로드</li></ul></li><li><strong>버전 관리(Versioning)</strong> + <strong>MFA Delete</strong>: 실수&#x2F;랜섬웨어 대응</li><li><strong>수명주기(Lifecycle) 정책</strong>: <strong>Standard → IA → Glacier</strong> 티어링, 자동 만료&#x2F;아카이브</li><li><strong>복제(Replication)</strong><ul><li><strong>CRR(Cross-Region Replication)</strong>: DR&#x2F;지연단축</li><li><strong>SRR(Same-Region Replication)</strong>: 멀티계정&#x2F;멀티버킷 분리</li></ul></li><li><strong>액세스 제어(Access Control)</strong><ul><li><strong>IAM 정책(IAM Policy)</strong>, <strong>버킷 정책(Bucket Policy)</strong> 중심</li><li><strong>S3 Access Points</strong>&#x2F;**VPC 엔드포인트(Interface&#x2F;Gateway)**로 네트워크 격리</li></ul></li><li><strong>감사&#x2F;로깅(Audit&#x2F;Logging)</strong>: <strong>CloudTrail</strong>, <strong>Server Access Logging</strong>, <strong>S3 Object Ownership</strong></li></ul><blockquote><p><strong>시험 포인트</strong>  </p><ul><li><strong>SSE-KMS</strong> 선택 시 <strong>KMS 키 권한</strong>(Encrypt&#x2F;Decrypt&#x2F;API호출)이 추가로 필요.  </li><li><strong>CRR</strong>은 <strong>버전닝이 양쪽 모두 활성화</strong>되어야 작동. 소유권&#x2F;권한 이슈 자주 출제.</li></ul></blockquote><hr><h2 id="6-정적-웹사이트-CDN"><a href="#6-정적-웹사이트-CDN" class="headerlink" title="6) 정적 웹사이트 &amp; CDN"></a>6) 정적 웹사이트 &amp; CDN</h2><ul><li><strong>Static Website Hosting</strong>: S3 정적 웹 사이트 엔드포인트 사용(퍼블릭 접근 필요).  </li><li><strong>CloudFront</strong> 앞단 배치 권장: OAC(Origin Access Control)로 <strong>S3는 비공개</strong>, <strong>CloudFront만 접근</strong> → 성능&#x2F;보안 모두 향상.</li></ul><blockquote><p><strong>시험 포인트</strong>: “S3를 퍼블릭으로 열지 않고 정적 사이트 제공?” → <strong>CloudFront + OAC</strong> 패턴.</p></blockquote><hr><h2 id="7-데이터-레이크-패턴-for-ML-Analytics"><a href="#7-데이터-레이크-패턴-for-ML-Analytics" class="headerlink" title="7) 데이터 레이크 패턴 (for ML&#x2F;Analytics)"></a>7) 데이터 레이크 패턴 (for ML&#x2F;Analytics)</h2><ul><li><strong>원천 저장</strong>: S3에 원본 데이터 적재(스키마 온 리드, <em>Schema-on-Read</em>).</li><li><strong>카탈로그</strong>: <strong>AWS Glue Data Catalog</strong></li><li><strong>쿼리&#x2F;탐색</strong>: <strong>Amazon Athena</strong>(서버리스 SQL), <strong>Redshift Spectrum</strong></li><li><strong>거버넌스</strong>: <strong>Lake Formation</strong>(권한&#x2F;데이터 액세스 제어)</li><li><strong>형식 최적화</strong>: <strong>Parquet&#x2F;ORC</strong> 등 <strong>컬럼나형(Columnar)</strong> 포맷 권장 (스캔&#x2F;비용 절감)</li></ul><blockquote><p><strong>시험 포인트</strong>: “S3 + Athena 비용 최적화?” → <strong>Parquet + 파티셔닝(Partitioning) + 프리픽스 설계</strong>.</p></blockquote><hr><h2 id="8-콘솔에서-자주-하는-작업-요약"><a href="#8-콘솔에서-자주-하는-작업-요약" class="headerlink" title="8) 콘솔에서 자주 하는 작업 요약"></a>8) 콘솔에서 자주 하는 작업 요약</h2><ol><li><strong>버킷 생성(Create Bucket)</strong>: 리전 선택 → 이름 지정(글로벌 유일) → <strong>Block Public Access ON</strong>, <strong>SSE 설정</strong>  </li><li><strong>객체 업로드(Upload)</strong>: 단일&#x2F;멀티파트 선택(5GB 초과 시 멀티파트)  </li><li><strong>폴더처럼 사용하기</strong>: 접두사(prefix)로 논리적 구분(실제 디렉터리는 아님)  </li><li><strong>프리사인드 URL 생성</strong>: 일시적 외부 공유  </li><li><strong>버킷 정책&#x2F;액세스 포인트</strong>: 세밀 권한&#x2F;네트워크 격리 설정</li></ol><hr><h2 id="9-자주-나오는-함정-Exam-Gotchas"><a href="#9-자주-나오는-함정-Exam-Gotchas" class="headerlink" title="9) 자주 나오는 함정(Exam Gotchas)"></a>9) 자주 나오는 함정(Exam Gotchas)</h2><ul><li><strong>디렉터리 개념 없음</strong>: 키 문자열에 <code>/</code>를 써서 <strong>prefix</strong>를 흉내낼 뿐.</li><li><strong>5GB 업로드 제한</strong>: 초과 시 <strong>Multi-part Upload</strong> 필수.</li><li><strong>CRR 조건</strong>: <strong>양쪽 버킷 Versioning ON</strong> + 권한&#x2F;소유권 고려.</li><li><strong>KMS 사용 시 권한 오류</strong>: KMS 키 정책&#x2F;Grant&#x2F;IAM 권한 누락 체크.</li><li><strong>퍼블릭 차단이 기본</strong>: 정적 사이트&#x2F;퍼블릭 파일 배포는 <strong>CloudFront</strong> 경유가 안전.</li></ul><hr><h2 id="10-ML-엔지니어를-위한-S3-빠른-체크리스트"><a href="#10-ML-엔지니어를-위한-S3-빠른-체크리스트" class="headerlink" title="10) ML 엔지니어를 위한 S3 빠른 체크리스트"></a>10) ML 엔지니어를 위한 S3 빠른 체크리스트</h2><ul><li><input disabled="" type="checkbox"> <strong>데이터 저장 포맷</strong>: CSV → <strong>Parquet</strong> 변환 고려(성능&#x2F;비용)  </li><li><input disabled="" type="checkbox"> <strong>접근 제어</strong>: IAM Role 기반 최소 권한(least privilege)  </li><li><input disabled="" type="checkbox"> <strong>암호화</strong>: SSE-KMS 기본, 키 권한 점검(파이프라인&#x2F;노트북&#x2F;배치)  </li><li><input disabled="" type="checkbox"> <strong>수명주기 정책</strong>: 학습 로그&#x2F;중간 산출물 자동 정리  </li><li><input disabled="" type="checkbox"> <strong>경로 규약</strong>: <code>s3://bucket/project/dataset/partition=.../</code> 일관성  </li><li><input disabled="" type="checkbox"> <strong>프리사인드 URL</strong>: 일시적 데이터 공유&#x2F;검수 자동화에 활용</li></ul><hr><h3 id="핵심-용어-요약-KR-EN"><a href="#핵심-용어-요약-KR-EN" class="headerlink" title="핵심 용어 요약 (KR&#x2F;EN)"></a>핵심 용어 요약 (KR&#x2F;EN)</h3><ul><li>버킷 <strong>Bucket</strong> &#x2F; 객체 <strong>Object</strong> &#x2F; 키 <strong>Key</strong> &#x2F; 접두사 <strong>Prefix</strong> &#x2F; 버전 관리 <strong>Versioning</strong>  </li><li>프리사인드 URL <strong>Pre-signed URL</strong> &#x2F; 수명주기 정책 <strong>Lifecycle Policy</strong>  </li><li>서버사이드 암호화 <strong>Server-Side Encryption (SSE-S3 &#x2F; SSE-KMS)</strong>  </li><li>교차 리전 복제 <strong>Cross-Region Replication (CRR)</strong> &#x2F; 동일 리전 복제 <strong>Same-Region Replication (SRR)</strong>  </li><li>데이터 레이크 <strong>Data Lake</strong> &#x2F; 스키마 온 리드 <strong>Schema-on-Read</strong> &#x2F; 컬럼나 <strong>Columnar</strong></li></ul><hr><h2 id="추가-참고-심화"><a href="#추가-참고-심화" class="headerlink" title="추가 참고(심화)"></a>추가 참고(심화)</h2><ul><li><strong>S3 Storage Classes</strong>: Standard &#x2F; Standard-IA &#x2F; One Zone-IA &#x2F; Intelligent-Tiering &#x2F; Glacier Instant&#x2F;Flx&#x2F;Deep Archive  </li><li><strong>네트워크 최적화</strong>: <strong>S3 Transfer Acceleration</strong>, 멀티파트 병렬 업로드, VPC 엔드포인트  </li><li><strong>비용 관리</strong>: S3 Storage Lens, Lifecycle&#x2F;Intelligent-Tiering, 파티셔닝&#x2F;프리픽스 설계</li></ul><hr><p><strong>요약</strong>: S3는 ML&#x2F;분석 워크로드의 “공용 데이터 허브”입니다. <strong>보안 기본값</strong>, <strong>멀티파트 업로드</strong>, <strong>Versioning&#x2F;CRR</strong>, <strong>SSE-KMS</strong>, <strong>Athena+Parquet</strong> 같은 패턴을 확실히 익히면 MLA-C01에서 고득점할 수 있습니다.</p><h1 id="Amazon-S3-암호화-방식-쉽게-설명하기"><a href="#Amazon-S3-암호화-방식-쉽게-설명하기" class="headerlink" title="Amazon S3 암호화 방식 쉽게 설명하기"></a>Amazon S3 암호화 방식 쉽게 설명하기</h1><p>Amazon S3에서 데이터를 저장할 때 보안을 위해 <strong>서버 사이드 암호화(Server-side encryption, SSE)</strong> 를 사용할 수 있습니다.<br>대표적으로 자주 쓰이는 두 가지 방식이 있습니다.</p><hr><h2 id="1-SSE-S3-Server-side-encryption-with-Amazon-S3-managed-keys"><a href="#1-SSE-S3-Server-side-encryption-with-Amazon-S3-managed-keys" class="headerlink" title="1. SSE-S3 (Server-side encryption with Amazon S3 managed keys)"></a>1. SSE-S3 (Server-side encryption with Amazon S3 managed keys)</h2><ul><li><p><strong>설명</strong><br>S3가 직접 암호화 키를 관리해주는 방식입니다.<br>사용자가 따로 키를 만들거나 관리할 필요가 없습니다.<br>데이터를 업로드하면 S3가 자동으로 암호화하고, 다운로드하면 자동으로 복호화해 줍니다.</p></li><li><p><strong>특징</strong></p><ul><li>사용하기 가장 쉽습니다 (추가 설정 거의 필요 없음).</li><li>암호화 키는 <strong>Amazon S3가 전적으로 관리</strong>합니다.</li><li><code>AES-256</code> 암호화 알고리즘을 사용합니다.</li><li>비용은 추가로 발생하지 않습니다.</li></ul></li><li><p><strong>시험 포인트 (AWS Certified ML Engineer Associate)</strong>  </p><ul><li><strong>SSE-S3는 S3가 키를 관리한다</strong>라는 점이 핵심.  </li><li>옵션을 활성화하기만 하면 끝 (운영 편리성 ↑).  </li><li>보안 규제가 강하지 않거나 단순 저장이 목적일 때 적합.</li></ul></li></ul><hr><h2 id="2-SSE-KMS-Server-side-encryption-with-AWS-Key-Management-Service-keys"><a href="#2-SSE-KMS-Server-side-encryption-with-AWS-Key-Management-Service-keys" class="headerlink" title="2. SSE-KMS (Server-side encryption with AWS Key Management Service keys)"></a>2. SSE-KMS (Server-side encryption with AWS Key Management Service keys)</h2><ul><li><p><strong>설명</strong><br>AWS Key Management Service(KMS)를 통해 암호화 키를 관리하는 방식입니다.<br>즉, 암호화 키를 직접 생성, 관리, 권한 제어할 수 있습니다.<br>더 세밀한 보안 관리가 필요한 경우에 사용됩니다.</p></li><li><p><strong>특징</strong></p><ul><li><strong>고객이 키를 직접 관리 (Customer managed keys)</strong> 가능.  </li><li>키 사용에 대한 <strong>CloudTrail 로그</strong>로 추적 가능 → 누가, 언제, 어떤 키를 사용했는지 알 수 있음.  </li><li>IAM 정책을 통해 세밀하게 접근 제어 가능 (예: 특정 사용자만 복호화 가능).  </li><li>KMS 호출 비용이 발생합니다.</li></ul></li><li><p><strong>시험 포인트</strong></p><ul><li><strong>SSE-KMS는 KMS와 통합되어 있어 세밀한 보안·감사 관리 가능</strong>.  </li><li>규제가 있는 산업(금융, 헬스케어 등)에서는 SSE-KMS가 요구될 수 있음.  </li><li>비용과 성능(추가 API 호출)도 고려해야 함.</li></ul></li></ul><hr><h2 id="비교-요약"><a href="#비교-요약" class="headerlink" title="비교 요약"></a>비교 요약</h2><table><thead><tr><th>항목</th><th>SSE-S3</th><th>SSE-KMS</th></tr></thead><tbody><tr><td>키 관리</td><td>Amazon S3 자동 관리</td><td>AWS KMS에서 직접 관리 가능</td></tr><tr><td>보안 수준</td><td>기본적 (단순 암호화)</td><td>고급 (세밀한 제어, 로깅)</td></tr><tr><td>비용</td><td>추가 비용 없음</td><td>KMS 사용 비용 발생</td></tr><tr><td>주요 특징</td><td>가장 간단, 자동 처리</td><td>IAM·CloudTrail 통합, 규제 준수에 적합</td></tr></tbody></table><hr><p>✅ <strong>시험 대비 핵심</strong>  </p><ul><li>SSE-S3: <strong>S3가 키 관리</strong>, 간단, 저비용  </li><li>SSE-KMS: <strong>KMS와 통합</strong>, 세밀한 제어, 감사 로그, 비용 발생</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Amazon-S3-핵심-정리&quot;&gt;&lt;a href=&quot;#Amazon-S3-핵심-정리&quot; class=&quot;headerlink&quot; title=&quot;Amazon S3 핵심 정리&quot;&gt;&lt;/a&gt;Amazon S3 핵심 정리&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;stro</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (4) - ETL 파이프라인과 데이터 포맷 이해</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-4/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-4/</id>
    <published>2025-09-15T02:06:05.000Z</published>
    <updated>2025-09-15T02:32:58.595Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ETL-파이프라인과-데이터-포맷-이해"><a href="#ETL-파이프라인과-데이터-포맷-이해" class="headerlink" title="ETL 파이프라인과 데이터 포맷 이해"></a>ETL 파이프라인과 데이터 포맷 이해</h1><h2 id="1-ETL-파이프라인이란"><a href="#1-ETL-파이프라인이란" class="headerlink" title="1. ETL 파이프라인이란?"></a>1. ETL 파이프라인이란?</h2><ul><li><strong>ETL</strong>은 <strong>Extract, Transform, Load</strong>의 약자입니다.<br>→ 데이터를 <strong>추출 → 변환 → 적재</strong>하는 일련의 과정.</li><li>주로 **데이터 웨어하우스(DWH)**로 데이터를 옮길 때 사용됩니다.</li><li>데이터 레이크에서는 <strong>ELT</strong>(Extract → Load → Transform) 방식이 더 일반적입니다.</li></ul><hr><h2 id="2-Extract-추출"><a href="#2-Extract-추출" class="headerlink" title="2. Extract (추출)"></a>2. Extract (추출)</h2><ul><li><strong>정의</strong>: 원천 시스템에서 <strong>데이터를 가져오는 단계</strong></li><li><strong>데이터 출처</strong>:<ul><li>데이터베이스 (MySQL, PostgreSQL, Oracle 등)</li><li>CRM (예: Salesforce)</li><li>로그 파일</li><li>API</li><li>스트리밍 데이터 (Kafka, Kinesis 등)</li></ul></li><li><strong>중요 고려사항</strong>:<ul><li>데이터 무결성 보장 (중간에 손실&#x2F;에러 발생 시 재시도 정책 필요)</li><li>처리 방식: <strong>실시간, 근실시간(near real-time), 배치(batch)</strong></li></ul></li></ul><hr><h2 id="3-Transform-변환"><a href="#3-Transform-변환" class="headerlink" title="3. Transform (변환)"></a>3. Transform (변환)</h2><ul><li><strong>정의</strong>: 추출한 데이터를 <strong>분석&#x2F;저장하기 적합한 형태</strong>로 변환</li><li><strong>주요 작업</strong>:<ul><li>데이터 정제 (중복 제거, 오류 수정)</li><li>데이터 보강 (추가 정보 합치기)</li><li>포맷 변경 (문자열 → 날짜 형식 변환 등)</li><li>집계&#x2F;계산 (합계, 평균 등)</li><li>인코딩&#x2F;디코딩 (압축 해제, 암호 해제, 컬럼 포맷 변환 등)</li><li>결측치 처리 (제거, 평균값 대체, null 값 허용 여부 확인)</li></ul></li></ul><p><strong>시험 포인트</strong>:</p><ul><li><strong>결측치 처리 방식</strong>은 머신러닝 모델 품질과 직결 → <strong>평균&#x2F;중앙값 대체, 삭제, 예측 기반 보간(imputation)</strong> 방법 숙지</li><li>SageMaker <strong>Processing Job, Data Wrangler</strong> 같은 서비스 활용법도 시험에 자주 등장</li></ul><hr><h2 id="4-Load-적재"><a href="#4-Load-적재" class="headerlink" title="4. Load (적재)"></a>4. Load (적재)</h2><ul><li><strong>정의</strong>: 변환된 데이터를 **목적지(데이터 웨어하우스, 데이터 레이크 등)**에 저장</li><li><strong>방법</strong>:<ul><li>배치 적재: 일정 주기로 대량 데이터 적재</li><li>스트리밍 적재: 데이터가 들어오는 즉시 적재</li></ul></li><li><strong>중요 고려사항</strong>:<ul><li>적재 시 데이터 무결성 확인</li><li>적재 실패 시 복구 전략 필요</li></ul></li></ul><hr><h2 id="5-ETL-파이프라인-관리"><a href="#5-ETL-파이프라인-관리" class="headerlink" title="5. ETL 파이프라인 관리"></a>5. ETL 파이프라인 관리</h2><p>ETL 과정은 <strong>자동화</strong>와 <strong>오케스트레이션</strong>이 중요합니다.</p><ul><li><strong>AWS Glue</strong> – ETL 작업 자동화 및 스케줄링</li><li><strong>AWS Step Functions</strong> – 워크플로우 관리</li><li><strong>Amazon MWAA (Managed Apache Airflow)</strong> – 복잡한 데이터 파이프라인 관리</li><li><strong>Amazon EventBridge</strong> – 이벤트 기반 트리거</li><li><strong>AWS Lambda</strong> – 서버리스 기반 데이터 처리</li></ul><p><strong>시험 포인트</strong>:</p><ul><li>Glue는 <strong>서버리스 ETL</strong> 서비스, Spark 기반 동작</li><li>Step Functions는 <strong>상태 기반 워크플로우</strong> 관리</li><li>MWAA는 <strong>Apache Airflow 관리형 서비스</strong></li></ul><hr><h2 id="6-주요-데이터-소스-인터페이스"><a href="#6-주요-데이터-소스-인터페이스" class="headerlink" title="6. 주요 데이터 소스 인터페이스"></a>6. 주요 데이터 소스 인터페이스</h2><ul><li><strong>JDBC (Java Database Connectivity)</strong><ul><li>자바 기반, <strong>플랫폼 독립적</strong>, 하지만 <strong>언어(Java) 종속적</strong></li></ul></li><li><strong>ODBC (Open Database Connectivity)</strong><ul><li>드라이버 필요(플랫폼 종속), 하지만 <strong>언어 독립적</strong></li></ul></li><li><strong>API</strong> – 외부 시스템에서 데이터 가져오기</li><li><strong>로그 파일</strong> – 서버 로그, 애플리케이션 로그 등</li><li><strong>스트리밍 데이터</strong> – Kafka, Kinesis 등</li></ul><hr><h2 id="7-데이터-포맷-정리"><a href="#7-데이터-포맷-정리" class="headerlink" title="7. 데이터 포맷 정리"></a>7. 데이터 포맷 정리</h2><h3 id="CSV-Comma-Separated-Values"><a href="#CSV-Comma-Separated-Values" class="headerlink" title="CSV (Comma-Separated Values)"></a>CSV (Comma-Separated Values)</h3><ul><li><strong>특징</strong>: 텍스트 기반, 행 단위 데이터, 구분자는 <code>,</code> 또는 <code>\t</code></li><li><strong>장점</strong>: 사람이 읽기 쉬움, 이식성 높음</li><li><strong>단점</strong>: 대규모 데이터 처리 시 비효율적</li><li><strong>시험 포인트</strong>: Pandas, R, Excel 등에서 손쉽게 처리 가능</li></ul><h3 id="JSON-JavaScript-Object-Notation"><a href="#JSON-JavaScript-Object-Notation" class="headerlink" title="JSON (JavaScript Object Notation)"></a>JSON (JavaScript Object Notation)</h3><ul><li><strong>특징</strong>: 키-값 기반, <strong>반정형(semi-structured) 데이터</strong> 표현 가능</li><li><strong>장점</strong>: 유연한 스키마, 중첩 구조 지원</li><li><strong>활용</strong>: API 응답, 설정 파일, NoSQL DB(MongoDB 등)</li></ul><h3 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h3><ul><li><strong>특징</strong>: 바이너리 포맷, 데이터와 스키마를 함께 저장</li><li><strong>장점</strong>: 효율적인 직렬화(Serialization), <strong>스키마 진화(schema evolution)</strong> 지원</li><li><strong>활용</strong>: Kafka, Spark, Flink, Hadoop</li></ul><h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><ul><li><strong>특징</strong>: <strong>컬럼 지향(columnar)</strong> 저장 포맷</li><li><strong>장점</strong>: 특정 컬럼만 읽기 가능 → 대규모 분석에 최적화</li><li><strong>활용</strong>: Redshift Spectrum, Spark, Hive, Athena</li><li><strong>시험 포인트</strong>: <strong>분석용 최적화 포맷</strong>으로 자주 언급됨</li></ul><hr><h2 id="8-시험-대비-요약"><a href="#8-시험-대비-요약" class="headerlink" title="8. 시험 대비 요약"></a>8. 시험 대비 요약</h2><ul><li><strong>ETL vs ELT</strong>: DWH는 ETL, Data Lake는 ELT</li><li><strong>데이터 포맷 특징 비교</strong>: CSV(단순), JSON(유연), Avro(스키마 포함), Parquet(분석 최적화)</li><li><strong>AWS Glue, Step Functions, MWAA</strong>: ETL 관리 핵심 서비스</li><li><strong>스키마 온 라이트(schema-on-write)</strong> vs <strong>스키마 온 리드(schema-on-read)</strong>: 시험 단골</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;ETL-파이프라인과-데이터-포맷-이해&quot;&gt;&lt;a href=&quot;#ETL-파이프라인과-데이터-포맷-이해&quot; class=&quot;headerlink&quot; title=&quot;ETL 파이프라인과 데이터 포맷 이해&quot;&gt;&lt;/a&gt;ETL 파이프라인과 데이터 포맷 이해&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (3) - 데이터의 세 가지 유형</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-3/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-3/</id>
    <published>2025-09-15T01:55:59.000Z</published>
    <updated>2025-09-15T04:13:20.934Z</updated>
    
    <content type="html"><![CDATA[<h1 id="데이터-웨어하우스-데이터-레이크-데이터-레이크하우스-데이터-메시-정리"><a href="#데이터-웨어하우스-데이터-레이크-데이터-레이크하우스-데이터-메시-정리" class="headerlink" title="데이터 웨어하우스, 데이터 레이크, 데이터 레이크하우스, 데이터 메시 정리"></a>데이터 웨어하우스, 데이터 레이크, 데이터 레이크하우스, 데이터 메시 정리</h1><h2 id="1-데이터-웨어하우스-Data-Warehouse"><a href="#1-데이터-웨어하우스-Data-Warehouse" class="headerlink" title="1. 데이터 웨어하우스 (Data Warehouse)"></a>1. 데이터 웨어하우스 (Data Warehouse)</h2><p><strong>정의</strong><br>데이터 웨어하우스는 <strong>여러 소스에서 수집된 데이터를 정제(ETL)하여 구조화된 형태로 저장</strong>하는 중앙 저장소입니다. 주로 분석과 BI(Business Intelligence)에 최적화되어 있습니다.</p><p><strong>특징</strong></p><ul><li>복잡한 쿼리와 분석 작업에 최적화  </li><li>사전에 스키마(schema)를 정의하고 데이터를 적재 (Schema-on-Write)  </li><li>주로 <strong>Star Schema</strong> 또는 <strong>Snowflake Schema</strong> 사용  </li><li>읽기(Read) 중심의 워크로드에 강함</li></ul><p><strong>예시</strong></p><ul><li>AWS: <strong>Amazon Redshift</strong>  </li><li>(비교) Google BigQuery, Azure Synapse</li></ul><p><strong>시험 포인트</strong></p><ul><li><strong>ETL (Extract → Transform → Load)</strong> 과정이 중요  </li><li>데이터 웨어하우스는 <strong>구조화된 데이터(Structured Data)</strong> 중심이라는 점 기억하기</li></ul><hr><h2 id="2-데이터-레이크-Data-Lake"><a href="#2-데이터-레이크-Data-Lake" class="headerlink" title="2. 데이터 레이크 (Data Lake)"></a>2. 데이터 레이크 (Data Lake)</h2><p><strong>정의</strong><br>데이터 레이크는 <strong>정형, 반정형, 비정형 데이터를 원본 그대로 저장</strong>하는 저장소입니다.  </p><p><strong>특징</strong></p><ul><li>데이터는 <strong>사전 스키마 정의 없이 원본(raw)</strong> 형태로 저장 (Schema-on-Read)  </li><li>대규모 데이터 저장에 유리 (저비용, 확장성)  </li><li>배치, 실시간, 스트리밍 데이터 모두 수용 가능  </li><li>필요할 때 데이터를 변환(ELT)하여 분석</li></ul><p><strong>예시</strong></p><ul><li>AWS: <strong>Amazon S3 (데이터 레이크로 사용)</strong>  </li><li>Azure Data Lake Storage, HDFS</li></ul><p><strong>시험 포인트</strong></p><ul><li>데이터 레이크는 <strong>ELT (Extract → Load → Transform)</strong> 방식  </li><li>로그 데이터, IoT 센서 데이터, 소셜 미디어 데이터 등 <strong>다양한 원천 데이터</strong> 저장 가능</li></ul><hr><h2 id="3-데이터-웨어하우스-vs-데이터-레이크"><a href="#3-데이터-웨어하우스-vs-데이터-레이크" class="headerlink" title="3. 데이터 웨어하우스 vs 데이터 레이크"></a>3. 데이터 웨어하우스 vs 데이터 레이크</h2><table><thead><tr><th>구분</th><th>데이터 웨어하우스</th><th>데이터 레이크</th></tr></thead><tbody><tr><td>스키마</td><td>Schema-on-Write (사전 정의)</td><td>Schema-on-Read (읽을 때 정의)</td></tr><tr><td>처리 방식</td><td>ETL</td><td>ELT 또는 단순 적재</td></tr><tr><td>데이터 형태</td><td>구조화된 데이터 중심</td><td>정형 + 비정형 모두</td></tr><tr><td>민첩성</td><td>낮음 (스키마 변경 어려움)</td><td>높음 (원본 그대로 저장)</td></tr><tr><td>비용</td><td>상대적으로 비쌈</td><td>저비용, 대규모 저장에 적합</td></tr></tbody></table><p><strong>시험에 자주 나오는 포인트</strong>  </p><ul><li>Redshift &#x3D; Data Warehouse  </li><li>S3 &#x3D; Data Lake  </li><li>Schema-on-Write ↔ Schema-on-Read 차이를 꼭 기억</li></ul><hr><h2 id="4-데이터-레이크하우스-Data-Lakehouse"><a href="#4-데이터-레이크하우스-Data-Lakehouse" class="headerlink" title="4. 데이터 레이크하우스 (Data Lakehouse)"></a>4. 데이터 레이크하우스 (Data Lakehouse)</h2><p><strong>정의</strong><br>데이터 웨어하우스와 데이터 레이크의 장점을 결합한 <strong>하이브리드 아키텍처</strong>입니다.  </p><p><strong>특징</strong></p><ul><li>정형 + 비정형 데이터 모두 지원  </li><li>Schema-on-Write, Schema-on-Read 모두 가능  </li><li>고성능 분석 + 머신러닝 활용 가능  </li><li>ACID 트랜잭션 보장 (데이터 정합성 유지)</li></ul><p><strong>예시</strong></p><ul><li>AWS: <strong>Lake Formation + S3 + Redshift Spectrum</strong>  </li><li>Databricks Lakehouse (Delta Lake)  </li><li>Azure Synapse Analytics</li></ul><p><strong>시험 포인트</strong></p><ul><li><strong>Lakehouse &#x3D; 데이터 분석(웨어하우스) + 머신러닝&#x2F;유연성(레이크) 결합</strong>  </li><li>Redshift Spectrum: <strong>S3에 저장된 데이터</strong>를 Redshift에서 직접 쿼리</li></ul><hr><h1 id="ACID-트랜잭션-보장"><a href="#ACID-트랜잭션-보장" class="headerlink" title="ACID 트랜잭션 보장"></a>ACID 트랜잭션 보장</h1><h2 id="1-Atomicity-원자성"><a href="#1-Atomicity-원자성" class="headerlink" title="1. Atomicity (원자성)"></a>1. Atomicity (원자성)</h2><ul><li>하나의 트랜잭션은 <strong>모두 실행되거나 전혀 실행되지 않아야 함</strong>을 의미합니다.</li><li>중간에 실패하면 이전까지 실행된 작업도 모두 롤백(취소)되어야 합니다</li><li>예: A 계좌에서 B 계좌로 10만원 송금 → A에서 출금만 되고 B에 입금이 안 되면 안 됨. 둘 다 실행되거나 둘 다 실행되지 않아야 함.</li></ul><h2 id="2-Consistency-일관성"><a href="#2-Consistency-일관성" class="headerlink" title="2. Consistency (일관성)"></a>2. Consistency (일관성)</h2><ul><li>트랜잭션 실행 전후에 <strong>데이터베이스의 제약조건과 규칙이 항상 지켜져야 함</strong>을 의미합니다.</li><li>잘못된 데이터 상태를 허용하지 않습니다.</li><li>예: 은행 계좌 잔액이 음수가 되면 안 된다는 규칙이 있을 때, 트랜잭션이 끝난 후에도 이 규칙은 항상 지켜져야 함.</li></ul><h2 id="3-Isolation-격리성"><a href="#3-Isolation-격리성" class="headerlink" title="3. Isolation (격리성)"></a>3. Isolation (격리성)</h2><ul><li>동시에 여러 트랜잭션이 실행되더라도 <strong>서로 간섭하지 않아야 함</strong>을 의미합니다.</li><li>마치 트랜잭션이 순차적으로 실행된 것처럼 결과가 나와야 합니다.</li><li>예: 두 사람이 동시에 같은 좌석을 예매할 때, 둘 다 예매 성공이 되면 안 됨. 하나는 성공, 하나는 실패해야 함.</li></ul><h2 id="4-Durability-지속성"><a href="#4-Durability-지속성" class="headerlink" title="4. Durability (지속성)"></a>4. Durability (지속성)</h2><ul><li>트랜잭션이 성공적으로 완료되면, 그 결과는 <strong>시스템 장애가 발생하더라도 영구적으로 보존</strong>되어야 합니다.</li><li>예: 돈을 이체하고 “성공” 메시지를 받았다면, 서버가 다운되더라도 그 결과는 반드시 반영되어 있어야 함.</li></ul><hr><h1 id="ACID와-분산-시스템-시험에-자주-나오는-부분"><a href="#ACID와-분산-시스템-시험에-자주-나오는-부분" class="headerlink" title="ACID와 분산 시스템 (시험에 자주 나오는 부분)"></a>ACID와 분산 시스템 (시험에 자주 나오는 부분)</h1><ul><li>전통적인 관계형 데이터베이스(RDBMS: MySQL, PostgreSQL, Oracle 등)는 ACID를 강하게 보장합니다.</li><li>하지만 **분산 시스템(빅데이터, NoSQL, 클라우드 환경)**에서는 성능과 확장성을 위해 일부 ACID 특성을 완화하기도 합니다.<ul><li>예: DynamoDB, Cassandra 같은 NoSQL DB는 완전한 ACID 대신 <strong>Eventually Consistent (최종 일관성)</strong> 모델을 제공하기도 함.</li></ul></li><li>최근에는 <strong>데이터 레이크&#x2F;레이크하우스 환경</strong>에서도 <strong>Delta Lake, Apache Iceberg, AWS Lake Formation</strong> 같은 기술이 <strong>ACID 트랜잭션 보장</strong>을 지원하면서, 대규모 분석 환경에서도 안정성을 확보할 수 있게 되었습니다.</li></ul><hr><h1 id="시험-포인트-AWS-Certified-ML-Engineer-Associate"><a href="#시험-포인트-AWS-Certified-ML-Engineer-Associate" class="headerlink" title="시험 포인트 (AWS Certified ML Engineer Associate)"></a>시험 포인트 (AWS Certified ML Engineer Associate)</h1><ol><li><strong>데이터 웨어하우스(Redshift)</strong> → ACID 보장 (RDBMS 기반).</li><li><strong>데이터 레이크(S3)</strong> → 원래는 ACID 보장 없음 → <strong>Delta Lake, Lake Formation</strong> 등을 통해 ACID 보장 추가 가능.</li><li><strong>트랜잭션 실패 시 롤백 &#x2F; 시스템 장애 시 지속성 보장</strong> 같은 개념이 시험에서 자주 언급됨.</li></ol><hr><h2 id="5-데이터-메시-Data-Mesh"><a href="#5-데이터-메시-Data-Mesh" class="headerlink" title="5. 데이터 메시 (Data Mesh)"></a>5. 데이터 메시 (Data Mesh)</h2><p><strong>정의</strong><br>데이터 메시(Data Mesh)는 <strong>특정 기술이 아니라 조직의 데이터 관리 방식</strong>을 의미합니다.  </p><p align="center">  <img src="/images/aws-ml-01.png" width="80%"></p><p><strong>핵심 개념</strong></p><ul><li><strong>도메인 기반 데이터 관리</strong>: 각 팀&#x2F;부서가 자기 데이터에 대한 “소유권”과 “책임”을 가짐  </li><li>데이터는 <strong>데이터 제품(Data Product)</strong> 형태로 다른 팀과 공유  </li><li>중앙에서 표준화된 거버넌스 제공 (보안, 품질, 권한 관리)  </li><li>AWS Glue Data Catalog, Lake Formation 등을 이용해 구현 가능</li></ul><p><strong>시험 포인트</strong></p><ul><li>시험에서는 “데이터 메시 &#x3D; 조직적&#x2F;운영적 개념”이라는 점을 구분해야 함  </li><li>“데이터 웨어하우스&#x2F;레이크&#x2F;레이크하우스”는 <strong>기술적 개념</strong>, “데이터 메시”는 <strong>조직적 패러다임</strong></li></ul><hr><h2 id="6-정리-–-언제-무엇을-쓸까"><a href="#6-정리-–-언제-무엇을-쓸까" class="headerlink" title="6. 정리 – 언제 무엇을 쓸까?"></a>6. 정리 – 언제 무엇을 쓸까?</h2><ul><li><p><strong>데이터 웨어하우스</strong>  </p><ul><li>정형 데이터  </li><li>빠르고 복잡한 쿼리  </li><li>BI&#x2F;리포팅 중심</li></ul></li><li><p><strong>데이터 레이크</strong>  </p><ul><li>정형 + 비정형 데이터 혼합  </li><li>머신러닝&#x2F;고급 분석 준비  </li><li>비용 효율적 저장소</li></ul></li><li><p><strong>데이터 레이크하우스</strong>  </p><ul><li>분석 + 머신러닝 모두 지원  </li><li>유연성 + 성능 모두 필요할 때</li></ul></li><li><p><strong>데이터 메시</strong>  </p><ul><li>조직 규모가 크고, 여러 부서가 데이터 관리  </li><li>팀별 데이터 소유권과 중앙 거버넌스 결합</li></ul></li></ul><hr><p>👉 <strong>시험 대비 핵심 키워드</strong>  </p><ul><li><strong>Schema-on-Write (Warehouse)</strong>  </li><li><strong>Schema-on-Read (Lake)</strong>  </li><li><strong>ETL ↔ ELT 차이</strong>  </li><li><strong>Redshift &#x3D; Data Warehouse &#x2F; S3 &#x3D; Data Lake</strong>  </li><li><strong>Redshift Spectrum &#x3D; Lakehouse 활용 예시</strong>  </li><li><strong>Data Mesh &#x3D; 기술이 아닌 조직적 거버넌스 패러다임</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;데이터-웨어하우스-데이터-레이크-데이터-레이크하우스-데이터-메시-정리&quot;&gt;&lt;a href=&quot;#데이터-웨어하우스-데이터-레이크-데이터-레이크하우스-데이터-메시-정리&quot; class=&quot;headerlink&quot; title=&quot;데이터 웨어하우스, 데이터 레</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (2) - 데이터의 세 가지 유형</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-2/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-2/</id>
    <published>2025-09-15T01:41:19.000Z</published>
    <updated>2025-09-15T02:05:30.557Z</updated>
    
    <content type="html"><![CDATA[<h1 id="데이터-엔지니어링-기초"><a href="#데이터-엔지니어링-기초" class="headerlink" title="데이터 엔지니어링 기초"></a>데이터 엔지니어링 기초</h1><p>이번 섹션은 AWS 서비스 자체보다는 <strong>데이터 엔지니어링의 기초 개념</strong>에 초점을 둡니다.<br>시험 가이드에서도 AWS 서비스뿐 아니라 데이터 관련 기본 개념을 알아야 한다고 명시되어 있습니다.  </p><hr><h2 id="1-데이터의-세-가지-유형"><a href="#1-데이터의-세-가지-유형" class="headerlink" title="1. 데이터의 세 가지 유형"></a>1. 데이터의 세 가지 유형</h2><h3 id="①-구조화-데이터-Structured-Data"><a href="#①-구조화-데이터-Structured-Data" class="headerlink" title="① 구조화 데이터 (Structured Data)"></a>① 구조화 데이터 (Structured Data)</h3><ul><li><strong>정의</strong>: 미리 정의된 스키마(열, 자료형 등)에 맞춰 정리된 데이터  </li><li><strong>특징</strong>: SQL로 쉽게 질의 가능, 행&#x2F;열 구조로 일관성 있음  </li><li><strong>예시</strong>:  <ul><li>관계형 데이터베이스 (MySQL, PostgreSQL, Amazon RDS, Amazon Redshift)  </li><li>잘 정리된 CSV 파일  </li><li>전형적인 엑셀 시트</li></ul></li></ul><p>👉 <strong>시험 포인트</strong>: RDB vs 데이터 레이크 차이를 구분해야 함. Redshift(OLAP)와 S3 기반 데이터 레이크 차이를 물을 수 있음.  </p><hr><h3 id="②-비구조화-데이터-Unstructured-Data"><a href="#②-비구조화-데이터-Unstructured-Data" class="headerlink" title="② 비구조화 데이터 (Unstructured Data)"></a>② 비구조화 데이터 (Unstructured Data)</h3><ul><li><strong>정의</strong>: 스키마가 없거나 일정하지 않은 데이터  </li><li><strong>특징</strong>: 바로 질의할 수 없음. 전처리&#x2F;메타데이터 추출 필요  </li><li><strong>예시</strong>:  <ul><li>텍스트 문서 (위키 문서, 전자책 등)  </li><li>오디오, 동영상, 이미지 파일  </li><li>이메일, 워드 문서</li></ul></li></ul><p>👉 <strong>시험 포인트</strong>: 이미지&#x2F;영상&#x2F;텍스트 → Amazon Rekognition, Transcribe, Comprehend 같은 서비스 활용.  </p><hr><h3 id="③-반구조화-데이터-Semi-structured-Data"><a href="#③-반구조화-데이터-Semi-structured-Data" class="headerlink" title="③ 반구조화 데이터 (Semi-structured Data)"></a>③ 반구조화 데이터 (Semi-structured Data)</h3><ul><li><strong>정의</strong>: 완전한 스키마는 없지만, 태그&#x2F;계층 구조 등 일부 구조적 특징 존재  </li><li><strong>예시</strong>:  <ul><li>JSON, XML  </li><li>로그 파일 (웹 서버 로그, 애플리케이션 로그 등)  </li><li>이메일 헤더</li></ul></li></ul><p>👉 <strong>시험 포인트</strong>: JSON&#x2F;로그 → Amazon Athena, Glue, OpenSearch로 쿼리 가능.  </p><hr><h2 id="2-데이터의-특성-–-3V-시험-중요"><a href="#2-데이터의-특성-–-3V-시험-중요" class="headerlink" title="2. 데이터의 특성 – 3V (시험 중요!)"></a>2. 데이터의 특성 – 3V (시험 중요!)</h2><p>AWS 시험에서 자주 등장하는 개념: <strong>데이터의 3V</strong><br>(Volume, Velocity, Variety)  </p><h3 id="①-Volume-데이터-양"><a href="#①-Volume-데이터-양" class="headerlink" title="① Volume (데이터 양)"></a>① Volume (데이터 양)</h3><ul><li><strong>정의</strong>: 데이터의 크기  </li><li><strong>예시</strong>:  <ul><li>SNS → 하루 수 TB 이상  </li><li>대형 리테일러 → 수년간 거래 기록 수 PB</li></ul></li><li><strong>AWS 관련 서비스</strong>:  <ul><li>대용량 마이그레이션 → <strong>Snowball, Snowmobile</strong>  </li><li>스토리지 → <strong>Amazon S3, EFS, FSx</strong></li></ul></li></ul><hr><h3 id="②-Velocity-데이터-생성-처리-속도"><a href="#②-Velocity-데이터-생성-처리-속도" class="headerlink" title="② Velocity (데이터 생성&#x2F;처리 속도)"></a>② Velocity (데이터 생성&#x2F;처리 속도)</h3><ul><li><strong>정의</strong>: 데이터가 생성&#x2F;수집&#x2F;처리되는 속도  </li><li><strong>예시</strong>:  <ul><li>IoT 센서 → 매 ms 단위 데이터 스트리밍  </li><li>주식 고빈도 거래(HFT) → 실시간 처리 필수</li></ul></li><li><strong>AWS 관련 서비스</strong>:  <ul><li>실시간 스트리밍 → <strong>Kinesis Data Streams</strong>  </li><li>근실시간(near real-time) 배치 → <strong>Kinesis Firehose, AWS Glue streaming ETL</strong></li></ul></li></ul><p>👉 <strong>시험 포인트</strong>: “실시간(real-time)” vs “근실시간(near real-time)” 서비스 구분 문제 자주 나옴.  </p><hr><h3 id="③-Variety-데이터-다양성"><a href="#③-Variety-데이터-다양성" class="headerlink" title="③ Variety (데이터 다양성)"></a>③ Variety (데이터 다양성)</h3><ul><li><strong>정의</strong>: 데이터의 형태와 출처의 다양성  </li><li><strong>예시</strong>:  <ul><li>구조화: 관계형 DB (RDS, Redshift)  </li><li>반구조화: JSON 로그 (CloudTrail 로그 등)  </li><li>비구조화: 환자 피드백 텍스트, 의료 영상</li></ul></li><li><strong>AWS 관련 서비스</strong>:  <ul><li>다양한 포맷 저장&#x2F;분석 → <strong>Lake Formation, Glue, Athena</strong></li></ul></li></ul><hr><h2 id="3-추가로-알아두면-좋은-개념"><a href="#3-추가로-알아두면-좋은-개념" class="headerlink" title="3. 추가로 알아두면 좋은 개념"></a>3. 추가로 알아두면 좋은 개념</h2><ul><li><p><strong>Veracity (진실성, 정확성)</strong>  </p><ul><li>공식 시험 가이드엔 없지만, 데이터의 신뢰성과 품질을 뜻함.  </li><li>AWS Glue DataBrew, SageMaker Data Wrangler를 통해 데이터 정제 가능.</li></ul></li><li><p><strong>데이터 품질 관리</strong>  </p><ul><li>Completeness(완전성), Accuracy(정확성), Consistency(일관성) 등은 시험에서 자주 출제.</li></ul></li><li><p><strong>로그와 반구조화 데이터 처리</strong>  </p><ul><li>CloudWatch Logs + Athena로 쿼리  </li><li>OpenSearch로 검색 및 분석</li></ul></li></ul><hr><h2 id="정리-시험-대비-포인트"><a href="#정리-시험-대비-포인트" class="headerlink" title="정리 (시험 대비 포인트)"></a>정리 (시험 대비 포인트)</h2><ul><li><p><strong>세 가지 데이터 유형</strong>  </p><ul><li>Structured (SQL 질의 가능)  </li><li>Semi-structured (JSON&#x2F;로그, 일부 구조)  </li><li>Unstructured (텍스트&#x2F;영상&#x2F;오디오, 전처리 필요)</li></ul></li><li><p><strong>데이터의 3V</strong>  </p><ul><li>Volume → 크기 (S3, Snowball)  </li><li>Velocity → 속도 (Kinesis, Firehose)  </li><li>Variety → 다양성 (RDS, S3, Glue, Athena 등)</li></ul></li><li><p><strong>시험에서 잘 나오는 부분</strong>  </p><ul><li>Kinesis Data Streams vs Firehose 차이  </li><li>Snowball vs Snowmobile 선택 기준  </li><li>데이터 레이크 vs 데이터 웨어하우스 (S3 + Athena vs Redshift)  </li><li>Glue &#x2F; Data Wrangler &#x2F; EMR 비교</li></ul></li></ul><hr><p>👉 이 섹션은 AWS 서비스 자체보다는 <strong>데이터 엔지니어링 기초 개념을 AWS 환경에 어떻게 적용하는지</strong> 묻는 문제가 출제될 가능성이 큽니다.  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;데이터-엔지니어링-기초&quot;&gt;&lt;a href=&quot;#데이터-엔지니어링-기초&quot; class=&quot;headerlink&quot; title=&quot;데이터 엔지니어링 기초&quot;&gt;&lt;/a&gt;데이터 엔지니어링 기초&lt;/h1&gt;&lt;p&gt;이번 섹션은 AWS 서비스 자체보다는 &lt;strong&gt;데</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (1) - AWS ML 엔지니어 어소시에이트(MLA-C01) 한눈에 보기</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-1/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-1/</id>
    <published>2025-09-14T20:54:26.000Z</published>
    <updated>2025-09-14T21:05:52.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AWS-ML-엔지니어-어소시에이트-MLA-C01-한눈에-보기"><a href="#AWS-ML-엔지니어-어소시에이트-MLA-C01-한눈에-보기" class="headerlink" title="AWS ML 엔지니어 어소시에이트(MLA-C01) 한눈에 보기"></a>AWS ML 엔지니어 어소시에이트(MLA-C01) 한눈에 보기</h1><p>이 과정에서는 <strong>데이터 수집→변환&#x2F;특징공학→모델 학습&#x2F;튜닝&#x2F;평가→생성형 AI→MLOps→보안&#x2F;거버넌스</strong>까지 실무 흐름을 따라가며, <strong>SageMaker 중심</strong>으로 AWS 서비스들을 연결해 이해합니다.</p><blockquote><h3 id="시험-포인트"><a href="#시험-포인트" class="headerlink" title="시험 포인트"></a>시험 포인트</h3><ul><li><strong>SageMaker 전반</strong>(Processing&#x2F;Training&#x2F;Inference&#x2F;Deployment)</li><li><strong>Glue, EMR, Kinesis, S3, EFS, EBS</strong> 활용</li><li><strong>데이터 변환·특징공학 기법</strong> (결측치, 이상치, 불균형 데이터 처리)</li><li><strong>기본 ML 알고리즘</strong> (XGBoost, Linear Learner 등 SageMaker 내장 알고리즘)</li><li><strong>성능 측정 지표</strong> (Precision, Recall, F1-score, Accuracy 등)</li><li><strong>하이퍼파라미터 튜닝</strong> (SageMaker Automatic Model Tuning)</li><li><strong>Bedrock, Jumpstart, RAG, Guardrails</strong> 등 생성형 AI 관련 신기능</li><li><strong>MLOps</strong> (CI&#x2F;CD, 파이프라인, 버전 관리, 모니터링, 재학습)</li><li><strong>보안·컴플라이언스</strong> (IAM, KMS, VPC, CloudTrail, Config 등)</li></ul></blockquote><hr><h2 id="1-데이터-수집-저장"><a href="#1-데이터-수집-저장" class="headerlink" title="1. 데이터 수집 &amp; 저장"></a>1. 데이터 수집 &amp; 저장</h2><ul><li><strong>형식</strong>: 정형&#x2F;비정형 데이터 (CSV, JSON, 이미지, 로그 등)</li><li><strong>저장소</strong>: <ul><li><strong>데이터 웨어하우스</strong>: Redshift</li><li><strong>데이터 레이크</strong>: S3</li><li><strong>데이터 레이크하우스</strong>: Lake Formation</li></ul></li><li><strong>스트리밍</strong>: Amazon Kinesis</li><li><strong>파일 스토리지</strong>: EFS, FSx</li><li><strong>블록 스토리지</strong>: EBS</li></ul><hr><h2 id="2-데이터-변환-특징공학"><a href="#2-데이터-변환-특징공학" class="headerlink" title="2. 데이터 변환 &amp; 특징공학"></a>2. 데이터 변환 &amp; 특징공학</h2><ul><li><strong>EMR</strong>: Hadoop, Spark 기반 대규모 데이터 처리</li><li><strong>결측치&#x2F;이상치 처리, 불균형 데이터 처리</strong></li><li><strong>SageMaker Processing, Data Wrangler</strong> 활용</li><li><strong>AWS Glue</strong>: ETL 파이프라인 자동화</li></ul><hr><h2 id="3-모델-학습-튜닝-평가"><a href="#3-모델-학습-튜닝-평가" class="headerlink" title="3. 모델 학습 &amp; 튜닝 &amp; 평가"></a>3. 모델 학습 &amp; 튜닝 &amp; 평가</h2><ul><li><strong>내장 알고리즘</strong>: XGBoost, Linear Learner, K-means, PCA 등</li><li><strong>딥러닝 기초</strong>: 뉴럴 네트워크, 옵티마이저, 학습률, 활성화 함수</li><li><strong>성능 지표</strong>: Precision, Recall, F1-score, Accuracy</li><li><strong>튜닝</strong>: SageMaker Automatic Model Tuning (Hyperparameter Optimization)</li></ul><hr><h2 id="4-생성형-AI"><a href="#4-생성형-AI" class="headerlink" title="4. 생성형 AI"></a>4. 생성형 AI</h2><ul><li><strong>Bedrock</strong>: 여러 파운데이션 모델 API 제공</li><li><strong>RAG (Retrieval Augmented Generation)</strong>: 외부 데이터 결합, 벡터 DB 활용</li><li><strong>Jumpstart</strong>: 사전 학습된 모델 빠른 활용</li><li><strong>Guardrails</strong>: 유해 콘텐츠 차단, PII 보호</li><li><strong>LLM Agent</strong>: 사용자 정의 툴과 코드 연동</li></ul><hr><h2 id="5-MLOps"><a href="#5-MLOps" class="headerlink" title="5. MLOps"></a>5. MLOps</h2><ul><li><strong>버전 관리</strong>: 데이터, 코드, 모델</li><li><strong>자동화</strong>: 데이터 수집, 전처리, 학습, 배포 파이프라인</li><li><strong>CI&#x2F;CD</strong>: CodePipeline, CodeBuild, CodeDeploy</li><li><strong>컨테이너화</strong>: EKS, ECR</li><li><strong>모니터링</strong>: CloudWatch, Model Monitor</li><li><strong>재학습</strong>: 지속적인 데이터 반영</li></ul><hr><h2 id="6-보안-거버넌스"><a href="#6-보안-거버넌스" class="headerlink" title="6. 보안 &amp; 거버넌스"></a>6. 보안 &amp; 거버넌스</h2><ul><li><strong>Shared Responsibility Model</strong><ul><li>AWS: 클라우드 인프라 보안</li><li>고객: 데이터, 접근 제어, 암호화</li></ul></li><li><strong>보안 서비스</strong>: IAM, KMS, Secrets Manager, Macie, WAF, Shield</li><li><strong>네트워크 보안</strong>: VPC, PrivateLink</li><li><strong>거버넌스 &amp; 비용 관리</strong>: CloudTrail, Config, Trusted Advisor, Budgets, Cost Explorer</li><li><strong>Well-Architected ML Lens</strong>: 모범 아키텍처 가이드라인</li></ul><hr><h2 id="시험-준비-팁"><a href="#시험-준비-팁" class="headerlink" title="시험 준비 팁"></a>시험 준비 팁</h2><ul><li>기출 및 유사 시험: <strong>ML Specialty, Data Engineer Associate</strong>와 겹치는 부분 많음</li><li>실제 경험 없어도 **핸즈온 랩(SageMaker Notebooks)**을 활용해 체험 필수</li><li>핵심 서비스: <strong>SageMaker, Bedrock, Glue, EMR, Kinesis, S3</strong></li><li>ML 기본기 (알고리즘, 지표, 전처리 기법) 반드시 숙지</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AWS-ML-엔지니어-어소시에이트-MLA-C01-한눈에-보기&quot;&gt;&lt;a href=&quot;#AWS-ML-엔지니어-어소시에이트-MLA-C01-한눈에-보기&quot; class=&quot;headerlink&quot; title=&quot;AWS ML 엔지니어 어소시에이트(MLA-C01</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS Certified AI Practitioner (41) - 거버넌스 &amp; 컴플라이언스의 중요성</title>
    <link href="https://kish191919.github.io/2025/09/02/KO-AWS-Certified-AI-Practitioner-41/"/>
    <id>https://kish191919.github.io/2025/09/02/KO-AWS-Certified-AI-Practitioner-41/</id>
    <published>2025-09-02T19:29:55.000Z</published>
    <updated>2025-09-02T19:55:57.955Z</updated>
    
    <content type="html"><![CDATA[<h1 id="거버넌스-컴플라이언스의-중요성"><a href="#거버넌스-컴플라이언스의-중요성" class="headerlink" title="거버넌스 &amp; 컴플라이언스의 중요성"></a>거버넌스 &amp; 컴플라이언스의 중요성</h1><ul><li><strong>조직의 AI 이니셔티브를 관리·최적화·확장</strong>하기 위한 기본 토대</li><li><strong>신뢰 구축</strong>: 책임 있는 AI 운영을 통해 내부·외부 이해관계자의 신뢰 확보</li><li><strong>위험 완화</strong>: 편향, 프라이버시 침해, 의도치 않은 결과 등</li><li><strong>정책·가이드·감독 체계</strong>로 법·규제 정합성 확보</li><li><strong>법적·평판 리스크</strong> 예방, <strong>대중 신뢰</strong> 제고</li></ul><blockquote><p>📌 <strong>시험 포인트(AWS&#x2F;클라우드 공통)</strong></p><ul><li>“책임 있는 AI(Responsible AI)”는 <strong>정책·감독·모니터링</strong>을 AI 수명주기 전반(설계→개발→배포→운영)에서 수행하는 것을 뜻함.</li><li><strong>공공·금융·의료</strong> 등은 규제 요건(감사·보관·추적성)이 강화됨.</li></ul></blockquote><hr><h1 id="거버넌스-프레임워크-예시"><a href="#거버넌스-프레임워크-예시" class="headerlink" title="거버넌스 프레임워크(예시)"></a>거버넌스 프레임워크(예시)</h1><ol><li><strong>AI 거버넌스 위원회 구성</strong><ul><li>법무, 컴플라이언스, 보안&#x2F;개인정보, 데이터, AI 개발 <strong>SME</strong>가 참여</li></ul></li><li><strong>역할과 책임 정의</strong><ul><li><strong>정책수립</strong>, <strong>리스크 평가</strong>, <strong>승인&#x2F;결정 절차</strong> 명확화</li></ul></li><li><strong>정책·프로세스 수립</strong><ul><li>데이터 관리 → 모델 개발&#x2F;검증 → <strong>배포&#x2F;모니터링</strong>까지 <strong>전 수명주기 표준화</strong></li></ul></li></ol><blockquote><p>🧩 <strong>AWS에서 도움 되는 서비스 예시</strong></p><ul><li><strong>AWS Config</strong>(설정 준수 추적), <strong>CloudTrail</strong>(감사 로그), <strong>Inspector</strong>(취약점), <strong>Audit Manager</strong>(감사용 증적 수집), <strong>Artifact</strong>(컴플라이언스 자료), <strong>Trusted Advisor</strong>(보안&#x2F;비용 권고).</li></ul></blockquote><p align="center">  <img src="/images/aws_basic_209.png" width="80%"></p><hr><h1 id="거버넌스-실행-전략"><a href="#거버넌스-실행-전략" class="headerlink" title="거버넌스 실행 전략"></a>거버넌스 실행 전략</h1><h2 id="1-정책"><a href="#1-정책" class="headerlink" title="1) 정책"></a>1) 정책</h2><ul><li>데이터 관리, 학습·검증, 출력 검수, 안전·휴먼 오버사이트</li><li><strong>IP&#x2F;저작권</strong>, <strong>편향 완화</strong>, <strong>개인정보 보호</strong> 포함</li></ul><h2 id="2-정기-리뷰-Review-Cadence"><a href="#2-정기-리뷰-Review-Cadence" class="headerlink" title="2) 정기 리뷰(Review Cadence)"></a>2) 정기 리뷰(Review Cadence)</h2><ul><li><strong>기술 리뷰</strong>: 성능, 데이터 품질, 알고리즘 강건성</li><li><strong>비기술 리뷰</strong>: 정책 준수, 책임 있는 AI 원칙, 규제 대응</li><li><strong>주기</strong>: 월간&#x2F;분기&#x2F;연간 + <strong>SME&#x2F;법무&#x2F;사용자</strong> 참여</li><li><strong>출시 전 테스트·검증 절차</strong>와 <strong>의사결정 기준</strong> 문서화</li></ul><h2 id="3-투명성-기준"><a href="#3-투명성-기준" class="headerlink" title="3) 투명성 기준"></a>3) 투명성 기준</h2><ul><li>모델&#x2F;데이터&#x2F;주요 의사결정 공개(가능 범위 내)</li><li><strong>한계·가능·적용사례</strong> 문서화, <strong>피드백 채널</strong> 운영</li></ul><h2 id="4-팀-교육"><a href="#4-팀-교육" class="headerlink" title="4) 팀 교육"></a>4) 팀 교육</h2><ul><li>정책·가이드·모범사례 교육, <strong>편향 완화&#x2F;Responsible AI</strong> 트레이닝</li><li><strong>교차 협업</strong> 장려, 내부 <strong>수료&#x2F;인증</strong> 제도</li></ul><hr><h1 id="데이터-거버넌스-전략"><a href="#데이터-거버넌스-전략" class="headerlink" title="데이터 거버넌스 전략"></a>데이터 거버넌스 전략</h1><ul><li><strong>Responsible AI 프레임워크</strong>: 공정성·투명성·책임성 지표 운영, <strong>GenAI 편향&#x2F;부작용 모니터링</strong></li><li><strong>조직 구조</strong>: 데이터 거버넌스 위원회, <strong>Data Steward&#x2F;Owner&#x2F;Custodian</strong> 역할 정의</li><li><strong>데이터 공유</strong>: 내부 보안 공유협약, <strong>가상화&#x2F;페더레이션</strong>으로 <strong>소유권 유지+접근성 제공</strong></li><li><strong>문화</strong>: 데이터 기반 의사결정, 공동 거버넌스 문화</li></ul><h2 id="📌-Data-Owner"><a href="#📌-Data-Owner" class="headerlink" title="📌 Data Owner"></a>📌 Data Owner</h2><ul><li><strong>정의</strong>: 데이터의 최종 책임자 (business 책임).</li><li><strong>주요 역할</strong>:<ul><li>데이터가 <strong>정확하고 적절히 사용</strong>되는지 보장.</li><li>데이터 사용 목적, 보존 기간, 보안 요구사항 등 <strong>정책적 결정</strong> 담당.</li><li>규제 및 법적 요구사항을 충족하도록 보장.</li></ul></li><li><strong>예시</strong>: 금융회사에서 고객 데이터의 Owner는 <strong>Compliance 팀장</strong> 또는 <strong>데이터 책임 부서장</strong>.</li></ul><hr><h2 id="📌-Data-Steward"><a href="#📌-Data-Steward" class="headerlink" title="📌 Data Steward"></a>📌 Data Steward</h2><ul><li><strong>정의</strong>: Data Owner가 정한 정책을 <strong>실제 관리하고 실행</strong>하는 사람.</li><li><strong>주요 역할</strong>:<ul><li>데이터의 <strong>품질 관리</strong> (정확성, 일관성, 최신성).</li><li>데이터 표준, 정의, 메타데이터 관리.</li><li>사용자들이 데이터를 올바르게 사용할 수 있도록 가이드 제공.</li></ul></li><li><strong>예시</strong>: 데이터 품질팀, 데이터 거버넌스 팀원.</li></ul><hr><h2 id="📌-Data-Custodian"><a href="#📌-Data-Custodian" class="headerlink" title="📌 Data Custodian"></a>📌 Data Custodian</h2><ul><li><strong>정의</strong>: 데이터를 <strong>기술적으로 보관·운영</strong>하는 사람.</li><li><strong>주요 역할</strong>:<ul><li>데이터 저장소(DB, Data Lake, Warehouse) <strong>보안·백업·권한 관리</strong>.</li><li>인프라, 접근 제어, 암호화 등 기술적 관리.</li><li>Data Owner&#x2F;Steward의 정책이 기술적으로 적용되도록 보장.</li></ul></li><li><strong>예시</strong>: DBA(Database Admin), 클라우드 엔지니어, 보안팀.</li></ul><h2 id="✅-세-역할의-차이-요약"><a href="#✅-세-역할의-차이-요약" class="headerlink" title="✅ 세 역할의 차이 요약"></a>✅ 세 역할의 차이 요약</h2><table><thead><tr><th>역할</th><th>책임 영역</th><th>주요 초점</th><th>예시 직무</th></tr></thead><tbody><tr><td><strong>Data Owner</strong></td><td>데이터에 대한 <strong>비즈니스적 책임</strong></td><td>법적&#x2F;규제 준수, 정책 수립</td><td>Compliance 책임자</td></tr><tr><td><strong>Data Steward</strong></td><td>데이터의 <strong>운영적 관리</strong></td><td>품질, 표준, 정의 관리</td><td>데이터 거버넌스 팀</td></tr><tr><td><strong>Data Custodian</strong></td><td>데이터의 <strong>기술적 관리</strong></td><td>보안, 저장, 접근 제어</td><td>DBA, 클라우드 엔지니어</td></tr></tbody></table><p>👉 쉽게 말하면:  </p><ul><li><strong>Owner</strong> &#x3D; “이 데이터의 주인은 누구인가?”  </li><li><strong>Steward</strong> &#x3D; “데이터를 잘 관리하고 있는가?”  </li><li><strong>Custodian</strong> &#x3D; “데이터를 안전하게 보관하고 있는가?”</li></ul><hr><h1 id="핵심-데이터-관리-개념"><a href="#핵심-데이터-관리-개념" class="headerlink" title="핵심 데이터 관리 개념"></a>핵심 데이터 관리 개념</h1><ul><li><strong>수명주기</strong>: 수집 → 처리 → 저장 → 소비 → 보관</li><li><strong>로그</strong>: 입력&#x2F;출력, 성능, 시스템 이벤트 추적</li><li><strong>데이터 레지던시</strong>: 저장&#x2F;처리 위치(법·프라이버시, <strong>데이터-연산 근접성</strong>)</li><li><strong>모니터링</strong>: 품질, 이상·드리프트 탐지</li><li><strong>분석</strong>: 통계&#x2F;시각화&#x2F;탐색</li><li><strong>보존</strong>: 규제, 재학습 히스토리, 비용 고려</li></ul><h3 id="데이터-라인리지-출처·이력"><a href="#데이터-라인리지-출처·이력" class="headerlink" title="데이터 라인리지(출처·이력)"></a>데이터 라인리지(출처·이력)</h3><ul><li><strong>출처 표시</strong>(데이터셋&#x2F;DB&#x2F;기타, 라이선스·이용약관)</li><li><strong>수집·정제·전처리 과정</strong> 문서화, <strong>카탈로그화</strong>로 추적성·책임성 강화</li></ul><p align="center">  <img src="/images/aws_basic_210.png" width="80%"></p>------------------------------------------------------------------------<h1 id="AI-시스템-보안·프라이버시"><a href="#AI-시스템-보안·프라이버시" class="headerlink" title="AI 시스템 보안·프라이버시"></a>AI 시스템 보안·프라이버시</h1><h2 id="위협-탐지"><a href="#위협-탐지" class="headerlink" title="위협 탐지"></a>위협 탐지</h2><ul><li>가짜 콘텐츠, 조작 데이터, 자동화 공격 탐지</li><li>네트워크 트래픽&#x2F;사용자 행태 등 <strong>AI 기반 탐지</strong> 적용</li></ul><h2 id="취약점-관리"><a href="#취약점-관리" class="headerlink" title="취약점 관리"></a>취약점 관리</h2><ul><li>소프트웨어 버그&#x2F;모델 약점 점검</li><li><strong>보안 점검·침투 테스트·코드 리뷰</strong>, <strong>패치&#x2F;업데이트</strong> 절차</li></ul><h2 id="인프라-보호"><a href="#인프라-보호" class="headerlink" title="인프라 보호"></a>인프라 보호</h2><ul><li>클라우드&#x2F;엣지&#x2F;데이터 저장소 보안</li><li><strong>접근통제</strong>, <strong>네트워크 분리</strong>, <strong>암호화</strong>, <strong>장애 내성</strong></li></ul><h2 id="프롬프트-인젝션-대응"><a href="#프롬프트-인젝션-대응" class="headerlink" title="프롬프트 인젝션 대응"></a>프롬프트 인젝션 대응</h2><ul><li><strong>필터링&#x2F;정화&#x2F;검증</strong> 가드레일</li><li><strong>정책 우회 시나리오</strong> 테스트(레드팀), 안전 출력 정책</li></ul><p align="center">  <img src="/images/aws_basic_211.png" width="80%"></p><h2 id="암호화·키관리"><a href="#암호화·키관리" class="headerlink" title="암호화·키관리"></a>암호화·키관리</h2><ul><li>저장&#x2F;전송 <strong>암호화</strong>, <strong>KMS 등 키보호</strong> 엄격 운영</li></ul><hr><h1 id="운영-모니터링-모델-인프라"><a href="#운영-모니터링-모델-인프라" class="headerlink" title="운영 모니터링(모델 &amp; 인프라)"></a>운영 모니터링(모델 &amp; 인프라)</h1><ul><li><strong>정확도(Accuracy)</strong>, <strong>정밀도(Precision)</strong>, <strong>재현율(Recall)</strong>, <strong>F1</strong></li><li><strong>지연시간</strong>(응답), <strong>CPU&#x2F;GPU&#x2F;네트워크&#x2F;스토리지</strong> 지표</li><li><strong>시스템 로그</strong>, <strong>편향&#x2F;공정성</strong>, <strong>규제·정책 준수</strong></li></ul><blockquote><p>📝 <strong>시험 포인트</strong></p><ul><li><strong>정밀도 vs 재현율</strong>: 불균형 데이터(사기탐지)에서 <strong>F1</strong>이 균형 지표로 자주 쓰임.</li><li>운영 중 <strong>데이터&#x2F;모델 드리프트</strong> → 재학습 또는 피처&#x2F;정책 재점검.</li></ul></blockquote><hr><h1 id="AWS-공유책임모델-Shared-Responsibility"><a href="#AWS-공유책임모델-Shared-Responsibility" class="headerlink" title="AWS 공유책임모델(Shared Responsibility)"></a>AWS 공유책임모델(Shared Responsibility)</h1><ul><li><strong>AWS(클라우드의 보안)</strong>: 인프라&#x2F;하이퍼바이저&#x2F;시설&#x2F;네트워크 및 <strong>관리형 서비스</strong>의 보안</li><li><strong>고객(클라우드 내 보안)</strong>: <strong>데이터 관리, 접근제어, 가드레일, 암호화</strong> 등 애플리케이션 측</li><li><strong>공유 통제</strong>: 패치&#x2F;구성&#x2F;보안 인식·교육</li></ul><blockquote><p>📌 <strong>시험 포인트</strong></p><ul><li><strong>Bedrock&#x2F;SageMaker 같은 관리형 서비스</strong>라도 <strong>데이터·접근·가드레일</strong>은 고객 책임.</li><li><strong>KMS, IAM, CloudTrail</strong>과의 연계 책임 구분 이해.</li></ul></blockquote><p align="center">  <img src="/images/aws_basic_212.png" width="80%"></p>------------------------------------------------------------------------<h1 id="보안형-데이터-엔지니어링-모범사례"><a href="#보안형-데이터-엔지니어링-모범사례" class="headerlink" title="보안형 데이터 엔지니어링 모범사례"></a>보안형 데이터 엔지니어링 모범사례</h1><ul><li><strong>데이터 품질</strong>: 완전성·정확성·적시성·일관성 <strong>프로파일링·모니터링</strong></li><li><strong>라인리지</strong>와 <strong>감사 추적</strong> 유지</li><li><strong>PETs(Privacy-Enhancing Tech)</strong>: 마스킹&#x2F;난독화, <strong>암호화&#x2F;토큰화</strong></li><li><strong>접근통제</strong>: 명확한 정책, <strong>RBAC&#x2F;세분권한</strong>, <strong>SSO&#x2F;MFA&#x2F;IAM</strong>, 접근 로깅·주기 점검(<strong>최소권한</strong>)</li><li><strong>무결성</strong>: 백업&#x2F;복구 전략, 통제 점검·테스트</li></ul><hr><h1 id="생성형-AI-보안-스코핑-매트릭스-요약"><a href="#생성형-AI-보안-스코핑-매트릭스-요약" class="headerlink" title="생성형 AI 보안 스코핑 매트릭스(요약)"></a>생성형 AI 보안 스코핑 매트릭스(요약)</h1><ul><li>GenAI 앱을 <strong>소유·책임 수준</strong>에 따라 5단계로 분류:<ol><li><strong>소비자 앱</strong>(공개 GenAI 사용) → 소유 낮음</li><li><strong>엔터프라이즈 SaaS 기능 활용</strong>(Einstein GPT 등)</li><li><strong>사전학습 모델 활용</strong>(Bedrock BM)</li><li><strong>파인튜닝 모델</strong>(Bedrock 커스텀, JumpStart)</li><li><strong>직접 학습 모델</strong>(SageMaker 훈련) → 소유 높음</li></ol></li><li>단계가 올라갈수록 <strong>거버넌스&#x2F;법·프라이버시&#x2F;리스크 통제</strong> 책임이 커짐.</li></ul><blockquote><p>📝 <strong>시험 포인트</strong></p><ul><li><strong>파인튜닝 도입 시</strong> 데이터 거버넌스·보안·규제 부담 <strong>상승</strong>.</li><li><strong>Self-host&#x2F;Training</strong>은 책임·비용·리스크 <strong>최대</strong>.</li></ul></blockquote><p align="center">  <img src="/images/aws_basic_213.png" width="80%"></p>------------------------------------------------------------------------<h1 id="MLOps-머신러닝-운영"><a href="#MLOps-머신러닝-운영" class="headerlink" title="MLOps(머신러닝 운영)"></a>MLOps(머신러닝 운영)</h1><ul><li><strong>개발→배포→감시→재학습</strong>을 <strong>자동·반복</strong></li><li><strong>핵심 원칙</strong><ul><li><strong>버전관리</strong>: 데이터&#x2F;코드&#x2F;모델 롤백 가능</li><li><strong>자동화</strong>: 수집·전처리·학습·검증·배포 파이프라인</li><li><strong>CI</strong>: 모델 테스트 자동화</li><li><strong>CD</strong>: 프로덕션 배포 자동화</li><li><strong>지속 재학습·모니터링</strong>: 드리프트·품질 감시</li></ul></li></ul><h3 id="전형적인-파이프라인"><a href="#전형적인-파이프라인" class="headerlink" title="전형적인 파이프라인"></a>전형적인 파이프라인</h3><ol><li><strong>데이터 준비</strong>(ETL&#x2F;Feature)</li><li><strong>모델 빌드&#x2F;학습</strong></li><li><strong>평가&#x2F;선정</strong></li><li><strong>배포(승인·승급)</strong></li><li><strong>모니터링&#x2F;경보 → 재학습 루프</strong></li></ol><p align="center">  <img src="/images/aws_basic_215.png" width="80%"></p><blockquote><p>🧪 <strong>AWS 연계 예시</strong></p><ul><li><strong>SageMaker Pipelines&#x2F;Model Registry&#x2F;Model Monitor</strong>, <strong>EventBridge + CodePipeline&#x2F;CodeBuild</strong>, <strong>CloudWatch</strong>, <strong>Step Functions</strong></li></ul></blockquote><h2 id="Phases-of-Machine-Learning-Project"><a href="#Phases-of-Machine-Learning-Project" class="headerlink" title="Phases of Machine Learning Project"></a>Phases of Machine Learning Project</h2><p align="center">  <img src="/images/aws_basic_214.png" width="80%"></p><hr><h2 id="요약-체크리스트-시험-대비"><a href="#요약-체크리스트-시험-대비" class="headerlink" title="요약 체크리스트(시험 대비)"></a>요약 체크리스트(시험 대비)</h2><ul><li><strong>Responsible AI</strong>: 공정성·설명가능성·투명성·안전·통제 가능성</li><li><strong>거버넌스 체계</strong>: 위원회, R&amp;R, 정책, 리뷰&#x2F;승인, 투명성, 교육</li><li><strong>데이터 거버넌스</strong>: 라인리지, 레지던시, 품질&#x2F;보존, 공유&#x2F;페더레이션</li><li><strong>보안</strong>: 프롬프트 인젝션 가드레일, 암호화·키관리, 취약점·패치, 인프라 보호</li><li><strong>모니터링 지표</strong>: Accuracy&#x2F;Precision&#x2F;Recall&#x2F;F1&#x2F;Latency + 인프라</li><li><strong>공유책임</strong>: 클라우드 <strong>of</strong> vs <strong>in</strong> 보안 구분</li><li><strong>MLOps</strong>: 버전·자동화·CI&#x2F;CD·재학습·모니터링</li><li><strong>GenAI 스코프</strong>: Pre-trained ↔ Fine-tuned ↔ Self-trained에 따른 <strong>책임 증가</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;거버넌스-컴플라이언스의-중요성&quot;&gt;&lt;a href=&quot;#거버넌스-컴플라이언스의-중요성&quot; class=&quot;headerlink&quot; title=&quot;거버넌스 &amp;amp; 컴플라이언스의 중요성&quot;&gt;&lt;/a&gt;거버넌스 &amp;amp; 컴플라이언스의 중요성&lt;/h1&gt;&lt;ul&gt;
</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_AI_PRACTITIONER_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-AI-PRACTITIONER-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="AWS_AI_PRACTITIONER" scheme="https://kish191919.github.io/tags/AWS-AI-PRACTITIONER/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified AI Practitioner(41) - Governance &amp; Compliance in AI</title>
    <link href="https://kish191919.github.io/2025/09/02/AWS-Certified-AI-Practitioner-41/"/>
    <id>https://kish191919.github.io/2025/09/02/AWS-Certified-AI-Practitioner-41/</id>
    <published>2025-09-02T19:29:51.000Z</published>
    <updated>2025-12-12T14:53:32.597Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Governance-Compliance-in-AI"><a href="#Governance-Compliance-in-AI" class="headerlink" title="Governance &amp; Compliance in AI"></a>Governance &amp; Compliance in AI</h1><h2 id="Why-Governance-and-Compliance-Matter"><a href="#Why-Governance-and-Compliance-Matter" class="headerlink" title="Why Governance and Compliance Matter"></a>Why Governance and Compliance Matter</h2><p>Governance is about managing, optimizing, and scaling AI initiatives inside an organization.  </p><ul><li>It builds <strong>trust</strong> in AI systems.  </li><li>Ensures <strong>responsible and trustworthy practices</strong>.  </li><li>Mitigates risks such as bias, privacy violations, or unintended outcomes.  </li><li>Aligns AI systems with <strong>legal and regulatory requirements</strong>.  </li><li>Protects against <strong>legal and reputational risks</strong>.  </li><li>Fosters <strong>public trust and confidence</strong> in AI deployment.</li></ul><p>📌 <strong>Exam tip</strong>: Expect questions that connect governance with <em>trust, compliance, and risk management</em>. AWS often tests your understanding of <em>why</em> governance is necessary, not just <em>how</em>.</p><hr><h2 id="Governance-Framework"><a href="#Governance-Framework" class="headerlink" title="Governance Framework"></a>Governance Framework</h2><p>A typical governance approach includes:</p><ol><li><strong>AI Governance Board &#x2F; Committee</strong>  <ul><li>Cross-functional: legal, compliance, data privacy, and AI experts.</li></ul></li><li><strong>Defined Roles and Responsibilities</strong>  <ul><li>Oversight, policy-making, risk assessments, decision-making.</li></ul></li><li><strong>Policies &amp; Procedures</strong>  <ul><li>Covering the full AI lifecycle: data management → training → deployment → monitoring.</li></ul></li></ol><h3 id="AWS-Governance-Tools-likely-on-exam"><a href="#AWS-Governance-Tools-likely-on-exam" class="headerlink" title="AWS Governance Tools (likely on exam):"></a>AWS Governance Tools (likely on exam):</h3><ul><li><strong>AWS Config</strong> – continuous monitoring and compliance tracking.  </li><li><strong>Amazon Inspector</strong> – automated vulnerability management.  </li><li><strong>AWS CloudTrail</strong> – records API calls for auditing.  </li><li><strong>AWS Audit Manager</strong> – helps with compliance evidence collection.  </li><li><strong>AWS Trusted Advisor</strong> – best practice checks (cost, security, performance).</li></ul><p align="center">  <img src="/images/aws_basic_209.png" width="80%"></p><hr><h2 id="Governance-Strategies"><a href="#Governance-Strategies" class="headerlink" title="Governance Strategies"></a>Governance Strategies</h2><ul><li><strong>Policies</strong>: Responsible AI guidelines (data handling, training, bias mitigation, IP protection).  </li><li><strong>Review Cadence</strong>: Reviews monthly, quarterly, or annually, with technical + legal experts.  </li><li><strong>Review Types</strong>:  <ul><li><em>Technical</em>: model performance, data quality, robustness.  </li><li><em>Non-technical</em>: legal, compliance, ethical considerations.</li></ul></li><li><strong>Transparency</strong>: Publish model details, training data sources, decisions made, limitations.  </li><li><strong>Team Training</strong>: Policies, responsible AI, bias mitigation, cross-functional collaboration.</li></ul><hr><h2 id="Data-Governance"><a href="#Data-Governance" class="headerlink" title="Data Governance"></a>Data Governance</h2><ul><li><strong>Responsible AI Principles</strong>: fairness, accountability, transparency, bias monitoring.  </li><li><strong>Governance Roles</strong>:  <ul><li><em>Data Owner</em>: accountable for data.  </li><li><em>Data Steward</em>: ensures quality, compliance.  </li><li><em>Data Custodian</em>: manages technical storage&#x2F;security.</li></ul></li><li><strong>Data Sharing</strong>: secure sharing agreements, virtualization, federation.  </li><li><strong>Data Culture</strong>: encourage data-driven decision-making.</li></ul><h3 id="Data-Management-Concepts"><a href="#Data-Management-Concepts" class="headerlink" title="Data Management Concepts"></a>Data Management Concepts</h3><ul><li><strong>Lifecycle</strong>: collection → processing → storage → use → archival.  </li><li><strong>Logging</strong>: track inputs, outputs, metrics, events.  </li><li><strong>Residency</strong>: where data is stored&#x2F;processed (important for GDPR &amp; HIPAA).  </li><li><strong>Monitoring</strong>: quality, anomalies, drift.  </li><li><strong>Retention</strong>: meet regulations and manage storage costs.</li></ul><h3 id="Data-Lineage"><a href="#Data-Lineage" class="headerlink" title="Data Lineage"></a>Data Lineage</h3><ul><li><strong>Source citation</strong>: datasets, licenses, permissions.  </li><li><strong>Origins</strong>: collection, cleaning, transformations.  </li><li><strong>Cataloging</strong>: organize &amp; document datasets.  </li><li>Provides <strong>traceability &amp; accountability</strong>.</li></ul><p align="center">  <img src="/images/aws_basic_210.png" width="80%"></p><hr><h2 id="Security-Privacy-for-AI"><a href="#Security-Privacy-for-AI" class="headerlink" title="Security &amp; Privacy for AI"></a>Security &amp; Privacy for AI</h2><ul><li><strong>Threat Detection</strong>: fake content, manipulated data, automated attacks.  </li><li><strong>Vulnerability Management</strong>: penetration tests, code reviews, patching.  </li><li><strong>Infrastructure Protection</strong>: secure cloud platforms, access controls, encryption, redundancy.  </li><li><strong>Prompt Injection Defense</strong>: input sanitization, guardrails.  </li><li><strong>Encryption</strong>: always encrypt data at rest &amp; in transit; manage keys securely.</li></ul><p align="center">  <img src="/images/aws_basic_211.png" width="80%"></p><hr><h2 id="Monitoring-AI-Systems"><a href="#Monitoring-AI-Systems" class="headerlink" title="Monitoring AI Systems"></a>Monitoring AI Systems</h2><ul><li><strong>Model Metrics</strong>:  <ul><li>Accuracy  </li><li>Precision (true positives &#x2F; predicted positives)  </li><li>Recall (true positives &#x2F; actual positives)  </li><li>F1-score (balance between precision &amp; recall)  </li><li>Latency (response time)</li></ul></li><li><strong>Infrastructure Monitoring</strong>: CPU&#x2F;GPU, network, storage, logs.  </li><li><strong>Bias &amp; Fairness Monitoring</strong>: required for compliance.</li></ul><hr><h2 id="AWS-Shared-Responsibility-Model"><a href="#AWS-Shared-Responsibility-Model" class="headerlink" title="AWS Shared Responsibility Model"></a>AWS Shared Responsibility Model</h2><ul><li><strong>AWS responsibility – Security <em>of</em> the Cloud</strong><br>Infrastructure: hardware, networking, managed services like S3, SageMaker, Bedrock.  </li><li><strong>Customer responsibility – Security <em>in</em> the Cloud</strong><br>Data management, encryption, access controls, guardrails.  </li><li><strong>Shared controls</strong>: patch management, configuration management, training.</li></ul><p>📌 <strong>Exam tip</strong>: Always remember the <em>“of the cloud” vs. “in the cloud”</em> split.</p><p align="center">  <img src="/images/aws_basic_212.png" width="80%"></p><hr><h2 id="Secure-Data-Engineering-Best-Practices"><a href="#Secure-Data-Engineering-Best-Practices" class="headerlink" title="Secure Data Engineering Best Practices"></a>Secure Data Engineering Best Practices</h2><ul><li><strong>Data Quality</strong>: complete, accurate, timely, consistent.  </li><li><strong>Privacy Enhancements</strong>: masking, obfuscation, encryption, tokenization.  </li><li><strong>Access Control</strong>: RBAC (role-based access), fine-grained permissions, SSO, MFA.  </li><li><strong>Data Integrity</strong>: error-free, backed up, lineage maintained, audit trails in place.</li></ul><hr><h2 id="Generative-AI-Security-Scoping-Matrix"><a href="#Generative-AI-Security-Scoping-Matrix" class="headerlink" title="Generative AI Security Scoping Matrix"></a>Generative AI Security Scoping Matrix</h2><p>Levels of ownership and security responsibility:  </p><ol><li><strong>Consumer App</strong> – very low ownership (e.g., using ChatGPT directly).  </li><li><strong>Enterprise App</strong> – SaaS with GenAI features (e.g., Salesforce GPT).  </li><li><strong>Pre-trained Models</strong> – use Bedrock base models without training.  </li><li><strong>Fine-tuned Models</strong> – customize models with your data.  </li><li><strong>Self-trained Models</strong> – full ownership, trained from scratch.</li></ol><p>📌 <strong>Exam tip</strong>: The more control you have → the more <strong>security and compliance responsibility</strong> you carry.  </p><p align="center">  <img src="/images/aws_basic_213.png" width="80%"></p><hr><h2 id="MLOps-Machine-Learning-Operations"><a href="#MLOps-Machine-Learning-Operations" class="headerlink" title="MLOps (Machine Learning Operations)"></a>MLOps (Machine Learning Operations)</h2><p>Extension of DevOps for ML:  </p><ul><li><strong>Version Control</strong>: data, code, models.  </li><li><strong>Automation</strong>: pipelines for ingestion, preprocessing, training.  </li><li><strong>CI&#x2F;CD</strong>: continuous testing and delivery of models.  </li><li><strong>Retraining</strong>: incorporate new data.  </li><li><strong>Monitoring</strong>: catch drift, ensure fairness and performance.</li></ul><p>Example ML pipeline:  </p><ol><li>Data prep  </li><li>Build model  </li><li>Evaluate model  </li><li>Select best candidate  </li><li>Deploy to production  </li><li>Monitor + retrain</li></ol><p>📌 <strong>Exam tip</strong>: AWS may test your knowledge of <strong>SageMaker pipelines, model registry, and monitoring tools</strong> as part of MLOps.  </p><p align="center">  <img src="/images/aws_basic_215.png" width="80%"></p><h2 id="Phases-of-Machine-Learning-Project"><a href="#Phases-of-Machine-Learning-Project" class="headerlink" title="Phases of Machine Learning Project"></a>Phases of Machine Learning Project</h2><p align="center">  <img src="/images/aws_basic_214.png" width="80%"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Governance-Compliance-in-AI&quot;&gt;&lt;a href=&quot;#Governance-Compliance-in-AI&quot; class=&quot;headerlink&quot; title=&quot;Governance &amp;amp; Compliance in AI&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_AI_PRACTITIONER" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-AI-PRACTITIONER/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="AWS_AI_PRACTITIONER" scheme="https://kish191919.github.io/tags/AWS-AI-PRACTITIONER/"/>
    
  </entry>
  
</feed>
