<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Danny&#39;s Blog</title>
  
  <subtitle>Data, AI, and Tech Insight</subtitle>
  <link href="https://kish191919.github.io/atom.xml" rel="self"/>
  
  <link href="https://kish191919.github.io/"/>
  <updated>2025-12-18T16:10:23.876Z</updated>
  <id>https://kish191919.github.io/</id>
  
  <author>
    <name>Danny Ki</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DATABRICKS-Certified-Developer-6</title>
    <link href="https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-6/"/>
    <id>https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-6/</id>
    <published>2025-12-18T15:48:12.000Z</published>
    <updated>2025-12-18T16:10:23.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DataFrame-Schema-정의-방법-이해하기"><a href="#DataFrame-Schema-정의-방법-이해하기" class="headerlink" title="DataFrame Schema 정의 방법 이해하기"></a>DataFrame Schema 정의 방법 이해하기</h1><p>DataFrame은 각 컬럼의 <strong>이름(Name)</strong> 과 <strong>데이터 타입(Data Type)</strong> 을 정의하는<br><strong>Schema</strong>를 가지고 있습니다.</p><p>DataFrame의 Schema를 확인하려면 printSchema 메서드를 사용합니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.printSchema()</span><br></pre></td></tr></table></figure><p>이 명령을 실행하면 DataFrame의 Schema가<br>사람이 읽기 쉬운 형태로 출력됩니다.</p><hr><h2 id="1-왜-Schema를-직접-정의해야-할까"><a href="#1-왜-Schema를-직접-정의해야-할까" class="headerlink" title="1. 왜 Schema를 직접 정의해야 할까?"></a>1. 왜 Schema를 직접 정의해야 할까?</h2><p>예를 들어, 데이터 파일을 보면 address_id 컬럼의 값은 크지 않습니다.<br>하지만 Spark는 자동으로 이를 <strong>Long 타입</strong>으로 인식할 수 있습니다.</p><p>이 경우:</p><ul><li>Long 타입은 불필요하게 큰 타입이고</li><li>Integer 타입으로 충분한 상황입니다</li></ul><p>따라서 <strong>Schema를 직접 정의하여 데이터 타입을 정확히 지정</strong>하는 것이 좋습니다.</p><hr><h2 id="2-Schema를-정의하는-첫-번째-방법-DDL-문자열-방식"><a href="#2-Schema를-정의하는-첫-번째-방법-DDL-문자열-방식" class="headerlink" title="2. Schema를 정의하는 첫 번째 방법: DDL 문자열 방식"></a>2. Schema를 정의하는 첫 번째 방법: DDL 문자열 방식</h2><p>DataFrame Schema를 정의하는 첫 번째 방법은<br><strong>DDL(Data Definition Language) 형식의 문자열</strong>을 사용하는 것입니다.</p><p>DDL 문자열은:</p><ul><li>컬럼 이름</li><li>컬럼 데이터 타입</li></ul><p>을 문자열 형태로 나열한 것입니다.</p><hr><h3 id="DDL-Schema-정의-절차"><a href="#DDL-Schema-정의-절차" class="headerlink" title="DDL Schema 정의 절차"></a>DDL Schema 정의 절차</h3><ol><li><p>Schema 정보를 담을 변수를 생성합니다<br>변수 이름 예시: customerDFSchemaDDL</p></li><li><p>문자열 안에 각 컬럼의 이름과 데이터 타입을 순서대로 작성합니다</p></li></ol><p>예를 들어:</p><ul><li>address_id → Integer</li><li>birth_country → String</li><li>birth_date → Date</li></ul><hr><h3 id="데이터-타입-지정-시-주의-사항"><a href="#데이터-타입-지정-시-주의-사항" class="headerlink" title="데이터 타입 지정 시 주의 사항"></a>데이터 타입 지정 시 주의 사항</h3><p>예를 들어 birth_date 컬럼은:</p><ul><li>원본 JSON 파일에서는 문자열(String) 형태로 저장되어 있지만</li><li>실제 의미는 날짜(Date)입니다</li></ul><p>따라서 <strong>Schema를 수동으로 정의할 때는</strong><br>실제 의미에 맞는 데이터 타입(Date)을 지정해야 합니다.</p><hr><h3 id="DDL-Schema-적용"><a href="#DDL-Schema-적용" class="headerlink" title="DDL Schema 적용"></a>DDL Schema 적용</h3><p>DDL 문자열을 정의한 후에는<br>DataFrame을 생성할 때 schema 옵션으로 적용합니다.</p><p>이렇게 하면 Spark는:</p><ul><li>자동 추론이 아니라</li><li><strong>우리가 지정한 Schema를 그대로 사용</strong>합니다</li></ul><hr><h3 id="Schema-변경-결과-확인"><a href="#Schema-변경-결과-확인" class="headerlink" title="Schema 변경 결과 확인"></a>Schema 변경 결과 확인</h3><p>다시 customerDF.printSchema()를 실행하면:</p><ul><li>이전에는 address_id가 Long 타입</li><li>이제는 Integer 타입</li><li>birth_date는 String → Date</li></ul><p>로 변경된 것을 확인할 수 있습니다.</p><hr><h2 id="3-Schema를-정의하는-두-번째-방법-StructType-사용"><a href="#3-Schema를-정의하는-두-번째-방법-StructType-사용" class="headerlink" title="3. Schema를 정의하는 두 번째 방법: StructType 사용"></a>3. Schema를 정의하는 두 번째 방법: StructType 사용</h2><p>두 번째 방법은<br><strong>StructType과 StructField를 직접 사용하는 방식</strong>입니다.</p><hr><h3 id="기존-DataFrame의-Schema-확인"><a href="#기존-DataFrame의-Schema-확인" class="headerlink" title="기존 DataFrame의 Schema 확인"></a>기존 DataFrame의 Schema 확인</h3><p>DataFrame에는 schema라는 속성이 있습니다.</p><p>customerDF.schema</p><p>이 값을 출력해 보면:</p><ul><li>문자열이 아니라</li><li>Apache Spark의 StructType 객체 형태로 Schema가 반환됩니다</li></ul><p>StructType은:</p><ul><li>여러 개의 StructField로 구성되고</li><li>각 StructField는 다음 정보를 포함합니다<ul><li>컬럼 이름</li><li>데이터 타입</li><li>nullable 여부</li></ul></li></ul><p>nullable 플래그는:</p><ul><li>Spark 내부 최적화를 위한 힌트일 뿐이며</li><li>false라고 해서 null 값이 실제로 없는 것은 아닙니다</li></ul><hr><h3 id="StructType-Schema-직접-정의하기"><a href="#StructType-Schema-직접-정의하기" class="headerlink" title="StructType Schema 직접 정의하기"></a>StructType Schema 직접 정의하기</h3><p>StructType은:</p><ul><li>StructField들의 컬렉션이며</li><li>복합 타입(Struct 안에 Struct, Array 등)도 표현할 수 있습니다</li></ul><p>demographics 컬럼은:</p><ul><li>StructType 안에</li><li>Array 타입 필드를 포함하는</li><li>복합 컬럼의 예시입니다</li></ul><hr><h3 id="필요한-패키지-Import"><a href="#필요한-패키지-Import" class="headerlink" title="필요한 패키지 Import"></a>필요한 패키지 Import</h3><p>StructType을 사용하려면 다음 패키지를 import 해야 합니다.</p><p>import org.apache.spark.sql.types._</p><hr><h3 id="StructType-Schema-적용"><a href="#StructType-Schema-적용" class="headerlink" title="StructType Schema 적용"></a>StructType Schema 적용</h3><p>StructType Schema 역시<br>DataFrameReader의 schema 메서드에 그대로 전달할 수 있습니다.</p><p>DDL 문자열과 StructType은<br><strong>동일한 결과</strong>를 만들어 냅니다.</p><hr><h2 id="4-Schema를-정의하는-세-번째-방법-Infer-Schema"><a href="#4-Schema를-정의하는-세-번째-방법-Infer-Schema" class="headerlink" title="4. Schema를 정의하는 세 번째 방법: Infer Schema"></a>4. Schema를 정의하는 세 번째 방법: Infer Schema</h2><p>세 번째 방법은<br><strong>Apache Spark에게 Schema 추론을 맡기는 방식</strong>입니다.</p><p>이를 위해 DataFrameReader에 옵션을 추가합니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br></pre></td></tr></table></figure><p>Spark는:</p><ul><li>일부 데이터를 먼저 읽고</li><li>데이터 타입을 추론한 뒤</li><li>전체 데이터를 다시 읽습니다</li></ul><hr><h3 id="Infer-Schema의-단점"><a href="#Infer-Schema의-단점" class="headerlink" title="Infer Schema의 단점"></a>Infer Schema의 단점</h3><p>Infer Schema 방식은:</p><ul><li>데이터를 두 번 읽기 때문에 성능 비용이 발생하고</li><li>Integer와 Long을 정확히 구분하지 못할 수 있습니다</li></ul><p>따라서 <strong>운영 환경에서는 Schema를 직접 정의하는 것이 권장</strong>됩니다.</p><hr><h2 id="5-Apache-Spark-문서-활용-방법-시험-대비-핵심"><a href="#5-Apache-Spark-문서-활용-방법-시험-대비-핵심" class="headerlink" title="5. Apache Spark 문서 활용 방법 (시험 대비 핵심)"></a>5. Apache Spark 문서 활용 방법 (시험 대비 핵심)</h2><p>시험 준비 시 매우 중요한 역량은<br><strong>Apache Spark 공식 문서를 빠르게 찾고 읽는 능력</strong>입니다.</p><hr><h3 id="DataFrameReader-문서-확인"><a href="#DataFrameReader-문서-확인" class="headerlink" title="DataFrameReader 문서 확인"></a>DataFrameReader 문서 확인</h3><p>Apache Spark 공식 문서에서:</p><ul><li>최신 버전 선택</li><li>Scala API 문서 선택</li><li>DataFrameReader 검색</li></ul><p>이를 통해 다음을 확인할 수 있습니다.</p><ul><li>format 메서드의 정확한 시그니처</li><li>json 메서드 사용법</li><li>option, load 등 모든 사용 가능한 API</li></ul><hr><h3 id="문서-활용의-중요성"><a href="#문서-활용의-중요성" class="headerlink" title="문서 활용의 중요성"></a>문서 활용의 중요성</h3><p>시험 문제 예시:</p><ul><li>format 메서드는 몇 개의 인자를 받는가?</li><li>inferSchema 옵션은 어떻게 사용하는가?</li></ul><p>이런 문제는:</p><ul><li>문서를 읽을 수 있다면</li><li>바로 답을 찾을 수 있습니다</li></ul><hr><h2 id="6-정리"><a href="#6-정리" class="headerlink" title="6. 정리"></a>6. 정리</h2><p>이번 강의에서 배운 내용:</p><ul><li>DataFrame Schema 확인 방법</li><li>Schema 정의 3가지 방법<ul><li>DDL 문자열</li><li>StructType</li><li>Infer Schema</li></ul></li><li>운영 환경에서는 Schema 수동 정의 권장</li><li>Apache Spark 문서 활용 방법</li></ul><p>다음 강의에서는:</p><ul><li>DataFrameReader의 다양한 옵션</li><li>JSON 외 다른 데이터 소스 읽기</li></ul><p>를 살펴보겠습니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DataFrame-Schema-정의-방법-이해하기&quot;&gt;&lt;a href=&quot;#DataFrame-Schema-정의-방법-이해하기&quot; class=&quot;headerlink&quot; title=&quot;DataFrame Schema 정의 방법 이해하기&quot;&gt;&lt;/a&gt;DataF</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/tags/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Certified-Developer-5</title>
    <link href="https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-5/"/>
    <id>https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-5/</id>
    <published>2025-12-18T15:38:46.000Z</published>
    <updated>2025-12-18T16:05:03.717Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-DataFrame-기초-실습-시작하기"><a href="#Spark-DataFrame-기초-실습-시작하기" class="headerlink" title="Spark DataFrame 기초 실습 시작하기"></a>Spark DataFrame 기초 실습 시작하기</h1><h2 id="1-실습-시작-전-확인-사항-매우-중요"><a href="#1-실습-시작-전-확인-사항-매우-중요" class="headerlink" title="1. 실습 시작 전 확인 사항 (매우 중요)"></a>1. 실습 시작 전 확인 사항 (매우 중요)</h2><p>본격적으로 실습을 시작하기 전에, <strong>두 가지를 반드시 확인</strong>해야 합니다.</p><h3 id="✅-1-데이터셋이-정상적으로-설치되었는지-확인"><a href="#✅-1-데이터셋이-정상적으로-설치되었는지-확인" class="headerlink" title="✅ 1) 데이터셋이 정상적으로 설치되었는지 확인"></a>✅ 1) 데이터셋이 정상적으로 설치되었는지 확인</h3><p>Databricks 환경의 왼쪽 메뉴에서 다음 순서로 이동합니다.</p><ul><li><strong>Data</strong> 클릭</li><li><strong>Add Data</strong></li><li><strong>DBFS (Databricks File System)</strong></li><li><strong>FileStore</strong></li><li><strong>Tables</strong></li><li>이전 강의에서 생성한 데이터 폴더</li></ul><p>이 경로에서:</p><ul><li>이전 강의에서 업로드한 모든 데이터 파일이 보인다면<br>👉 데이터셋은 정상적으로 설치된 것입니다.</li></ul><hr><h3 id="✅-2-클러스터가-실행-중인지-확인"><a href="#✅-2-클러스터가-실행-중인지-확인" class="headerlink" title="✅ 2) 클러스터가 실행 중인지 확인"></a>✅ 2) 클러스터가 실행 중인지 확인</h3><p>Apache Spark는 <strong>클러스터 없이는 아무 작업도 할 수 없습니다</strong>.</p><p>Community Edition에서는:</p><ul><li>동시에 <strong>하나의 클러스터만 실행 가능</strong>합니다.</li></ul><p>클러스터가 없다면:</p><ul><li><strong>Create Cluster</strong> 클릭</li><li>클러스터 이름 입력</li><li><strong>Create Cluster</strong> 클릭</li></ul><p>자세한 클러스터 생성 방법은<br>👉 이전 강의 영상을 참고하시면 됩니다.</p><hr><h2 id="2-Workspace-및-노트북-생성"><a href="#2-Workspace-및-노트북-생성" class="headerlink" title="2. Workspace 및 노트북 생성"></a>2. Workspace 및 노트북 생성</h2><p>이제 Spark 코드를 작성할 <strong>노트북(Notebook)</strong> 을 생성하겠습니다.</p><h3 id="2-1-Workspace-폴더-생성"><a href="#2-1-Workspace-폴더-생성" class="headerlink" title="2-1. Workspace 폴더 생성"></a>2-1. Workspace 폴더 생성</h3><p>왼쪽 메뉴에서:</p><ul><li><strong>Workspace</strong> 클릭</li><li>Workspace 옆 <strong>화살표(▶)</strong> 클릭</li><li><strong>Create → Folder</strong> 선택</li></ul><p>폴더 이름 예시:</p><ul><li><code>Databricks Certified Associate Developer Training</code></li></ul><hr><h3 id="2-2-Notebook-생성"><a href="#2-2-Notebook-생성" class="headerlink" title="2-2. Notebook 생성"></a>2-2. Notebook 생성</h3><p>생성한 폴더 안에서:</p><ul><li><strong>Create → Notebook</strong> 클릭</li></ul><p>다음 정보를 입력합니다.</p><ul><li><strong>Notebook Name</strong><ul><li><code>DataFrame_Basics</code></li></ul></li><li><strong>Language</strong><ul><li><code>Scala</code></li></ul></li><li><strong>Cluster</strong><ul><li>이미 생성해 둔 클러스터 선택</li></ul></li></ul><p>모두 입력한 후:</p><ul><li><strong>Create</strong> 클릭</li></ul><p>이제 Spark 클러스터에 연결된 노트북이 생성되었습니다.</p><hr><h2 id="3-Spark-Session-이해하기"><a href="#3-Spark-Session-이해하기" class="headerlink" title="3. Spark Session 이해하기"></a>3. Spark Session 이해하기</h2><p>Databricks 노트북에서는 이미 <strong>Spark Session</strong>이 자동으로 생성되어 있습니다.</p><p>Spark Session은:</p><ul><li>Spark 애플리케이션의 진입점(entry point)</li><li>클러스터 매니저 및 Executor와 통신하는 창구</li></ul><p>Databricks에서는 기본적으로<br>👉 <code>spark</code> 라는 변수명으로 제공됩니다.</p><h3 id="Spark-Session-확인"><a href="#Spark-Session-확인" class="headerlink" title="Spark Session 확인"></a>Spark Session 확인</h3><p>노트북 셀에 다음을 입력하고 실행해 보세요.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark</span><br></pre></td></tr></table></figure><p>정상적으로 실행되면<br>👉 <strong>현재 사용 중인 Spark Session 정보가 출력됩니다.</strong></p><hr><h2 id="4-Spark-DataFrame-생성하기-JSON-파일"><a href="#4-Spark-DataFrame-생성하기-JSON-파일" class="headerlink" title="4. Spark DataFrame 생성하기 (JSON 파일)"></a>4. Spark DataFrame 생성하기 (JSON 파일)</h2><p>이제 실제 데이터를 읽어서 <strong>DataFrame</strong>을 만들어 보겠습니다.</p><hr><h3 id="4-1-DataFrameReader-사용"><a href="#4-1-DataFrameReader-사용" class="headerlink" title="4-1. DataFrameReader 사용"></a>4-1. DataFrameReader 사용</h3><p>Spark Session의 <code>read</code> 메서드는<br>👉 <strong>DataFrameReader</strong>를 반환합니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br></pre></td></tr></table></figure><p>이를 이용해 외부 데이터를 DataFrame으로 읽을 수 있습니다.</p><h3 id="4-2-customer-json-파일로-DataFrame-생성"><a href="#4-2-customer-json-파일로-DataFrame-생성" class="headerlink" title="4-2. customer.json 파일로 DataFrame 생성"></a>4-2. customer.json 파일로 DataFrame 생성</h3><p>먼저 DataFrame 변수를 하나 정의합니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> customerDF = spark.read</span><br><span class="line">  .format(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/FileStore/tables/Databricks/Certified/Associate/Developer/Data/customer.json&quot;</span>)</span><br></pre></td></tr></table></figure><p>이 코드는 다음 작업을 수행합니다.</p><ul><li>JSON 파일을 읽고</li><li>그 내용을 기반으로 customerDF라는 DataFrame을 생성합니다</li></ul><p>📌 중요</p><ul><li>데이터 경로는 반드시 이전에 업로드한 위치와 정확히 일치해야 합니다.</li><li>경로가 다르면 코드가 실행되지 않습니다.</li></ul><h1 id="5-원본-JSON-파일-내용-확인하기"><a href="#5-원본-JSON-파일-내용-확인하기" class="headerlink" title="5. 원본 JSON 파일 내용 확인하기"></a>5. 원본 JSON 파일 내용 확인하기</h1><p>DataFrame을 만들기 전에,<br>실제 JSON 파일 안에 어떤 데이터가 있는지 확인해 보겠습니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbutils.fs.head(<span class="string">&quot;/FileStore/tables/Databricks/Certified/Associate/Developer/Data/customer.json&quot;</span>)</span><br></pre></td></tr></table></figure><p>이 명령은 다음과 같은 역할을 합니다.</p><ul><li>JSON 파일의 앞부분을 출력</li><li>데이터 구조를 빠르게 확인하는 데 매우 유용</li></ul><hr><h1 id="6-DataFrame-데이터-확인하기"><a href="#6-DataFrame-데이터-확인하기" class="headerlink" title="6. DataFrame 데이터 확인하기"></a>6. DataFrame 데이터 확인하기</h1><p>이제 생성된 DataFrame을 직접 확인해 보겠습니다.</p><p>Databricks 환경에서는 display() 함수를 사용할 수 있습니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display(customerDF)</span><br></pre></td></tr></table></figure><p>출력 결과를 보면 다음과 같은 컬럼을 확인할 수 있습니다.</p><ul><li>address_id</li><li>bad</li><li>country</li><li>birth_date</li><li>demographics</li></ul><p>📌 <strong>demographics 컬럼은 struct 타입의 복합 컬럼입니다.</strong></p><hr><h2 id="DataFrame-스키마-Schema-확인"><a href="#DataFrame-스키마-Schema-확인" class="headerlink" title="DataFrame 스키마(Schema) 확인"></a>DataFrame 스키마(Schema) 확인</h2><p>컬럼 이름과 데이터 타입을 확인하려면 다음 명령을 사용합니다.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.printSchema()</span><br></pre></td></tr></table></figure><p>이 결과에서 확인할 수 있는 정보는 다음과 같습니다.</p><ul><li>컬럼 이름</li><li>데이터 타입</li><li>nullable 여부 (null 가능 여부)</li></ul><p>📌 <strong>nullable 정보는</strong></p><ul><li>데이터 최적화를 위한 힌트이며</li><li>실제로 null 값이 반드시 없다는 의미는 아닙니다</li></ul><hr><h1 id="7-DataFrame과-Spark-아키텍처-연결해서-이해하기"><a href="#7-DataFrame과-Spark-아키텍처-연결해서-이해하기" class="headerlink" title="7. DataFrame과 Spark 아키텍처 연결해서 이해하기"></a>7. DataFrame과 Spark 아키텍처 연결해서 이해하기</h1><p>이제 DataFrame이 <strong>어디에, 어떻게 저장되는지</strong>를 이해해 보겠습니다.</p><hr><h2 id="Apache-Spark-전체-구조-요약"><a href="#Apache-Spark-전체-구조-요약" class="headerlink" title="Apache Spark 전체 구조 요약"></a>Apache Spark 전체 구조 요약</h2><ul><li><p><strong>Cluster Manager</strong></p><ul><li>클러스터 전체 관리</li></ul></li><li><p><strong>Node Manager</strong></p><ul><li>각 워커 노드 관리</li></ul></li><li><p><strong>Spark Driver</strong></p><ul><li>작업 계획 및 제어</li></ul></li><li><p><strong>Spark Executors</strong></p><ul><li>실제 연산 수행</li><li>데이터를 메모리에 보관</li></ul></li></ul><hr><h2 id="Executor와-메모리"><a href="#Executor와-메모리" class="headerlink" title="Executor와 메모리"></a>Executor와 메모리</h2><p>각 Executor는 다음과 같은 리소스를 할당받습니다.</p><ul><li>일정 개수의 CPU Core</li><li>일정 크기의 Memory(RAM)</li></ul><p>Spark가 데이터를 읽으면:</p><ul><li>데이터는 Executor의 메모리에 저장되고</li><li>이 데이터는 DataFrame 형태로 관리됩니다</li></ul><hr><h1 id="8-Partition-개념-이해하기"><a href="#8-Partition-개념-이해하기" class="headerlink" title="8. Partition 개념 이해하기"></a>8. Partition 개념 이해하기</h1><p>Apache Spark는 데이터를 자동으로 나눕니다.</p><p>이 나뉜 데이터 조각을 <strong>Partition</strong>이라고 합니다.</p><ul><li>하나의 Partition &#x3D; 여러 Row의 묶음</li><li>하나의 Partition은 하나의 머신(Executor)에 위치</li></ul><p>👉 DataFrame의 Partition 구조는<br>👉 데이터가 클러스터에 어떻게 분산 저장되는지를 결정합니다.</p><hr><h1 id="9-DataFrame이란-무엇인가"><a href="#9-DataFrame이란-무엇인가" class="headerlink" title="9. DataFrame이란 무엇인가?"></a>9. DataFrame이란 무엇인가?</h1><p>DataFrame은 다음과 같은 특징을 가집니다.</p><ul><li>분산된 데이터 컬렉션</li><li>구조화된 형태 (Schema 존재)</li><li>관계형 데이터베이스의 테이블과 매우 유사</li></ul><hr><h2 id="DataFrame의-특징"><a href="#DataFrame의-특징" class="headerlink" title="DataFrame의 특징"></a>DataFrame의 특징</h2><ul><li>행(Row)과 열(Column)로 구성</li><li>모든 Row는 동일한 컬럼 구조를 가짐</li><li>각 컬럼은 다음 정보를 가짐<ul><li>이름(Name)</li><li>데이터 타입(Data Type)</li><li>nullable 정보</li></ul></li></ul><p>이 컬럼 정의 전체를 <strong>Schema</strong>라고 부릅니다.</p><hr><h1 id="10-DataFrame-Schema의-중요성"><a href="#10-DataFrame-Schema의-중요성" class="headerlink" title="10. DataFrame Schema의 중요성"></a>10. DataFrame Schema의 중요성</h1><p>Schema는 매우 중요합니다.</p><p>이유는 다음과 같습니다.</p><ul><li>문자열을 숫자로 잘못 처리하는 오류 방지</li><li>연산 최적화</li><li>안정적인 데이터 처리</li></ul><p>Schema를 확인하려면 다음 메서드를 사용합니다.</p><p>printSchema()</p><p>Apache Spark는:</p><ul><li>DataFrame이 어떤 Partition에 있는지</li><li>데이터가 어디에 분산되어 있는지</li></ul><p>를 내부적으로 모두 관리해 주기 때문에<br>👉 개발자는 분산 처리에 대해 직접 신경 쓰지 않아도 됩니다.</p><hr><h1 id="11-정리-및-다음-강의-예고"><a href="#11-정리-및-다음-강의-예고" class="headerlink" title="11. 정리 및 다음 강의 예고"></a>11. 정리 및 다음 강의 예고</h1><p>이번 강의에서 배운 내용은 다음과 같습니다.</p><ul><li>✅ 데이터셋 및 클러스터 확인</li><li>✅ Spark Session 이해</li><li>✅ JSON 파일로 DataFrame 생성</li><li>✅ DataFrame 구조와 Schema 이해</li><li>✅ Partition 개념 이해</li></ul><p>다음 강의에서는:</p><ul><li>DataFrame Schema를 직접 정의하는 방법</li><li>데이터 타입을 명확히 지정하는 방법</li></ul><p>을 살펴보겠습니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Spark-DataFrame-기초-실습-시작하기&quot;&gt;&lt;a href=&quot;#Spark-DataFrame-기초-실습-시작하기&quot; class=&quot;headerlink&quot; title=&quot;Spark DataFrame 기초 실습 시작하기&quot;&gt;&lt;/a&gt;Spark Da</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/tags/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Certified-Developer-4</title>
    <link href="https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-4/"/>
    <id>https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-4/</id>
    <published>2025-12-18T15:32:28.000Z</published>
    <updated>2025-12-18T15:34:54.217Z</updated>
    
    <content type="html"><![CDATA[<h1 id="실습-데이터셋-및-Databricks-노트북-설치하기"><a href="#실습-데이터셋-및-Databricks-노트북-설치하기" class="headerlink" title="실습 데이터셋 및 Databricks 노트북 설치하기"></a>실습 데이터셋 및 Databricks 노트북 설치하기</h1><h2 id="1-이-강의에서-할-일"><a href="#1-이-강의에서-할-일" class="headerlink" title="1. 이 강의에서 할 일"></a>1. 이 강의에서 할 일</h2><p>이 강의에서는 앞으로의 모든 실습을 위해 필요한:</p><ul><li>📁 <strong>데이터셋(Data files)</strong></li><li>📓 <strong>Databricks 노트북(Notebooks)</strong></li></ul><p>을 Databricks 환경에 설치합니다.</p><p>이 과정이 제대로 되지 않으면<br>👉 이후 강의에서 제공하는 <strong>소스 코드가 정상적으로 실행되지 않기 때문에</strong><br>반드시 차근차근 따라와 주세요.</p><hr><h2 id="2-강의-자료-다운로드-Zip-파일"><a href="#2-강의-자료-다운로드-Zip-파일" class="headerlink" title="2. 강의 자료 다운로드 (Zip 파일)"></a>2. 강의 자료 다운로드 (Zip 파일)</h2><p>먼저 이 강의의 <strong>Resources(자료)</strong> 섹션으로 이동합니다.</p><p>여기에서 <strong>두 개의 zip 파일</strong>을 다운로드합니다.</p><ol><li>📓 <strong>Notebooks zip 파일</strong><ul><li>강의에서 사용할 Databricks 노트북들</li></ul></li><li>📁 <strong>Dataset zip 파일</strong><ul><li>실습에 사용할 데이터 파일들 (JSON, CSV 등)</li></ul></li></ol><p>두 파일 모두 <strong>로컬 PC에 다운로드 후 압축을 해제</strong>해 주세요.</p><hr><h2 id="3-Databricks에-데이터셋-업로드하기"><a href="#3-Databricks에-데이터셋-업로드하기" class="headerlink" title="3. Databricks에 데이터셋 업로드하기"></a>3. Databricks에 데이터셋 업로드하기</h2><p>이제 Databricks 환경으로 돌아가서<br>먼저 <strong>데이터 파일</strong>을 업로드하겠습니다.</p><h3 id="3-1-Data-메뉴로-이동"><a href="#3-1-Data-메뉴로-이동" class="headerlink" title="3-1. Data 메뉴로 이동"></a>3-1. Data 메뉴로 이동</h3><p>Databricks 화면의 왼쪽 메뉴에서:</p><ul><li><strong>Data</strong> 클릭</li><li><strong>Add Data</strong> 클릭</li></ul><hr><h3 id="3-2-업로드-폴더-생성-중요"><a href="#3-2-업로드-폴더-생성-중요" class="headerlink" title="3-2. 업로드 폴더 생성 (중요)"></a>3-2. 업로드 폴더 생성 (중요)</h3><p>파일을 업로드하기 전에<br>👉 <strong>반드시 새로운 폴더를 하나 생성</strong>해야 합니다.</p><p>이유:</p><ul><li>강의에서 제공하는 모든 노트북 코드는</li><li>특정 경로를 기준으로 데이터를 읽도록 작성되어 있기 때문입니다</li></ul><p>예시 폴더 이름:</p><ul><li><code>datasets</code></li><li><code>spark-data</code></li><li><code>course-data</code></li></ul><p>📌 이 폴더 이름은 <strong>절대 변경하지 않는 것을 추천</strong>합니다.</p><hr><h3 id="3-3-데이터-파일-업로드"><a href="#3-3-데이터-파일-업로드" class="headerlink" title="3-3. 데이터 파일 업로드"></a>3-3. 데이터 파일 업로드</h3><p>이제:</p><ul><li>앞에서 압축 해제한 <strong>데이터 파일 폴더</strong>로 이동</li><li>모든 파일을 선택</li><li>Databricks 업로드 화면으로 <strong>드래그 앤 드롭</strong></li></ul><p>⚠️ 주의 사항:</p><ul><li>파일 업로드가 완료되기 전에 <strong>페이지를 닫지 마세요</strong></li><li>파일 크기에 따라 시간이 조금 걸릴 수 있습니다</li></ul><p>업로드가 완료되면:</p><ul><li>각 파일 옆에 <strong>초록색 체크(✔)</strong> 표시가 나타납니다</li><li>하단에 업로드 성공 메시지가 표시됩니다</li></ul><p>이제 데이터 업로드는 완료되었습니다.</p><hr><h2 id="4-Databricks-노트북-업로드하기"><a href="#4-Databricks-노트북-업로드하기" class="headerlink" title="4. Databricks 노트북 업로드하기"></a>4. Databricks 노트북 업로드하기</h2><p>다음으로 <strong>Databricks 노트북</strong>을 업로드하겠습니다.</p><h3 id="4-1-Workspace-메뉴로-이동"><a href="#4-1-Workspace-메뉴로-이동" class="headerlink" title="4-1. Workspace 메뉴로 이동"></a>4-1. Workspace 메뉴로 이동</h3><p>왼쪽 메뉴에서:</p><ul><li><strong>Workspace</strong> 클릭</li></ul><hr><h3 id="4-2-노트북-Import"><a href="#4-2-노트북-Import" class="headerlink" title="4-2. 노트북 Import"></a>4-2. 노트북 Import</h3><p>Workspace 화면에서:</p><ul><li>Workspace 옆의 <strong>화살표(▶)</strong> 클릭</li><li><strong>Import</strong> 클릭</li></ul><hr><h3 id="4-3-노트북-파일-업로드"><a href="#4-3-노트북-파일-업로드" class="headerlink" title="4-3. 노트북 파일 업로드"></a>4-3. 노트북 파일 업로드</h3><p>이제:</p><ul><li>압축 해제한 <strong>Notebooks 폴더</strong>에서</li><li>노트북 파일을 선택</li><li>Databricks 화면으로 <strong>드래그 앤 드롭</strong></li><li><strong>Import</strong> 클릭</li></ul><p>이 작업을 하면:</p><ul><li>Workspace 안에 <strong>새 폴더가 생성</strong></li><li>이 폴더 안에 강의에서 사용할 모든 노트북이 저장됩니다</li></ul><hr><h2 id="5-첫-번째-노트북-실행-및-확인"><a href="#5-첫-번째-노트북-실행-및-확인" class="headerlink" title="5. 첫 번째 노트북 실행 및 확인"></a>5. 첫 번째 노트북 실행 및 확인</h2><p>이제 모든 것이 제대로 설치되었는지 확인해 보겠습니다.</p><h3 id="5-1-DataFrame-Basic-노트북-열기"><a href="#5-1-DataFrame-Basic-노트북-열기" class="headerlink" title="5-1. DataFrame Basic 노트북 열기"></a>5-1. DataFrame Basic 노트북 열기</h3><p>업로드된 노트북 중:</p><ul><li><strong>DataFrame Basic</strong> 노트북을 엽니다</li></ul><p>이 노트북은:</p><ul><li>이후 강의에서 사용할 데이터의 기본 구조를 확인하는 용도입니다</li></ul><hr><h3 id="5-2-클러스터-연결"><a href="#5-2-클러스터-연결" class="headerlink" title="5-2. 클러스터 연결"></a>5-2. 클러스터 연결</h3><p>노트북 상단에서:</p><ul><li><strong>Attach to Cluster</strong></li><li>앞에서 생성한 클러스터 선택</li></ul><hr><h3 id="5-3-코드-실행-테스트"><a href="#5-3-코드-실행-테스트" class="headerlink" title="5-3. 코드 실행 테스트"></a>5-3. 코드 실행 테스트</h3><p>이제 노트북의 첫 번째 셀을 실행합니다.</p><p>이 코드는:</p><ul><li>앞에서 업로드한 <strong>customer.json 파일</strong>을 읽어서</li><li>Spark DataFrame을 생성합니다</li></ul><p>만약:</p><ul><li>DataFrame이 정상적으로 생성되고</li><li>데이터가 출력된다면</li></ul><p>👉 데이터와 노트북이 <strong>정상적으로 설치된 것</strong>입니다 🎉</p><hr><h2 id="6-매우-중요한-주의-사항-꼭-확인"><a href="#6-매우-중요한-주의-사항-꼭-확인" class="headerlink" title="6. 매우 중요한 주의 사항 (꼭 확인)"></a>6. 매우 중요한 주의 사항 (꼭 확인)</h2><p>📌 <strong>데이터 경로는 반드시 동일해야 합니다</strong></p><ul><li>데이터 파일을 다른 폴더에 업로드하면</li><li>노트북에 작성된 경로와 달라져서</li><li>코드가 실행되지 않습니다</li></ul><p>따라서:</p><ul><li>강의에서 안내한 <strong>폴더 위치</strong></li><li><strong>폴더 이름</strong></li><li><strong>경로 구조</strong></li></ul><p>를 그대로 유지해 주세요.</p><hr><h2 id="7-마무리"><a href="#7-마무리" class="headerlink" title="7. 마무리"></a>7. 마무리</h2><p>이제 다음이 모두 완료되었습니다.</p><p>✅ 데이터셋 업로드<br>✅ Databricks 노트북 업로드<br>✅ Spark DataFrame 정상 생성 확인</p><p>이제 실습을 위한 모든 준비가 끝났습니다.</p><p>다음 강의에서는:</p><ul><li>Spark DataFrame을 이용한</li><li>실제 데이터 처리와 변환 작업을 시작하겠습니다.</li></ul><p>다음 강의에서 뵙겠습니다 😊</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;실습-데이터셋-및-Databricks-노트북-설치하기&quot;&gt;&lt;a href=&quot;#실습-데이터셋-및-Databricks-노트북-설치하기&quot; class=&quot;headerlink&quot; title=&quot;실습 데이터셋 및 Databricks 노트북 설치하기&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/tags/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Certified-Developer-3</title>
    <link href="https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-3/"/>
    <id>https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-3/</id>
    <published>2025-12-18T15:27:05.000Z</published>
    <updated>2025-12-18T15:32:23.163Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Databricks-Community-Edition-계정-생성-및-클러스터-만들기"><a href="#Databricks-Community-Edition-계정-생성-및-클러스터-만들기" class="headerlink" title="Databricks Community Edition 계정 생성 및 클러스터 만들기"></a>Databricks Community Edition 계정 생성 및 클러스터 만들기</h1><h2 id="1-이-강의에서-필요한-준비-사항"><a href="#1-이-강의에서-필요한-준비-사항" class="headerlink" title="1. 이 강의에서 필요한 준비 사항"></a>1. 이 강의에서 필요한 준비 사항</h2><p>이 강의에 포함된 모든 예제를 따라 하기 위해서는<br><strong>Databricks Community Edition 계정</strong>이 필요합니다.</p><p>Databricks Community Edition은:</p><ul><li><strong>무료</strong>로 제공되는 Databricks 계정이며</li><li>Apache Spark를 학습하기에 충분한 환경을 제공합니다</li></ul><p>실무에서 사용하는 대규모 클러스터는 아니지만,<br><strong>Spark의 핵심 개념과 동작 방식</strong>을 이해하기에는 충분합니다.</p><hr><h2 id="2-Databricks-Community-Edition-회원-가입"><a href="#2-Databricks-Community-Edition-회원-가입" class="headerlink" title="2. Databricks Community Edition 회원 가입"></a>2. Databricks Community Edition 회원 가입</h2><p>먼저 웹 브라우저를 열고 다음 사이트로 이동합니다.</p><p>👉 <strong><a href="https://databricks.com/">https://databricks.com</a></strong></p><p>메인 페이지에서:</p><ul><li><strong>Try Databricks</strong> 버튼을 클릭합니다</li></ul><p>그러면 회원 가입(Sign Up) 페이지가 나타납니다.</p><hr><h3 id="회원-가입-정보-입력"><a href="#회원-가입-정보-입력" class="headerlink" title="회원 가입 정보 입력"></a>회원 가입 정보 입력</h3><p>다음과 같은 기본 정보를 입력합니다.</p><ul><li>Company Name (회사명)<ul><li>예: <code>Personal</code>, <code>Self-Study</code>, <code>Learning</code></li></ul></li><li>Email Address<ul><li>개인 이메일 주소 사용 가능</li></ul></li><li>Role<ul><li>예: Data Engineer, Data Analyst, Student 등</li></ul></li><li>Intended Use<ul><li>학습(Learning) 또는 개인 프로젝트</li></ul></li></ul><p>모든 정보를 입력한 후:</p><ul><li><strong>Sign Up</strong> 버튼을 클릭합니다</li></ul><hr><h2 id="3-Databricks-Platform-vs-Community-Edition"><a href="#3-Databricks-Platform-vs-Community-Edition" class="headerlink" title="3. Databricks Platform vs Community Edition"></a>3. Databricks Platform vs Community Edition</h2><p>회원 가입 과정 중에 두 가지 옵션이 나타납니다.</p><h3 id="Databricks-Platform-유료"><a href="#Databricks-Platform-유료" class="headerlink" title="Databricks Platform (유료)"></a>Databricks Platform (유료)</h3><ul><li>Microsoft Azure 또는 AWS 기반</li><li>멀티 노드 클러스터 사용 가능</li><li>실무 및 대규모 데이터 처리에 적합</li></ul><h3 id="Databricks-Community-Edition-무료"><a href="#Databricks-Community-Edition-무료" class="headerlink" title="Databricks Community Edition (무료)"></a>Databricks Community Edition (무료)</h3><ul><li><strong>Single-node 클러스터</strong></li><li>제한된 리소스 제공</li><li>학습 및 실습용으로 적합</li></ul><p>이 강의에서는:</p><ul><li>✅ <strong>Community Edition</strong>을 사용합니다</li></ul><p>따라서 <strong>Get Started</strong> 버튼을 클릭합니다.</p><hr><h2 id="4-이메일-인증-및-비밀번호-설정"><a href="#4-이메일-인증-및-비밀번호-설정" class="headerlink" title="4. 이메일 인증 및 비밀번호 설정"></a>4. 이메일 인증 및 비밀번호 설정</h2><p>회원 가입이 완료되면 Databricks에서<br><strong>이메일 인증 메일</strong>을 발송합니다.</p><p>다음 사항을 꼭 확인하세요:</p><ul><li>받은 편지함(Inbox)</li><li>스팸 메일함(Spam)</li><li>정크 메일함(Junk)</li></ul><p>메일 안에 있는 링크를 클릭하면<br>비밀번호 설정 페이지로 이동합니다.</p><hr><h3 id="비밀번호-설정"><a href="#비밀번호-설정" class="headerlink" title="비밀번호 설정"></a>비밀번호 설정</h3><p>비밀번호 설정 페이지에서:</p><ul><li>비밀번호 입력</li><li>비밀번호 확인 입력</li><li><strong>Reset Password</strong> 클릭</li></ul><p>이제 Databricks 계정이 활성화됩니다.</p><hr><h2 id="5-Databricks-Community-Edition-환경-살펴보기"><a href="#5-Databricks-Community-Edition-환경-살펴보기" class="headerlink" title="5. Databricks Community Edition 환경 살펴보기"></a>5. Databricks Community Edition 환경 살펴보기</h2><p>비밀번호 설정 후 로그인하면<br><strong>Databricks 웹 기반 프로그래밍 환경</strong>에 접속하게 됩니다.</p><p>이 환경에서 할 수 있는 일:</p><ul><li>Apache Spark 코드 작성</li><li>Notebook 생성</li><li>클러스터 생성 및 관리</li></ul><p>다만 Community Edition은 무료 서비스이기 때문에<br><strong>강력한 클러스터를 생성할 수는 없습니다</strong>.</p><hr><h2 id="6-Community-Edition에서-클러스터-생성하기"><a href="#6-Community-Edition에서-클러스터-생성하기" class="headerlink" title="6. Community Edition에서 클러스터 생성하기"></a>6. Community Edition에서 클러스터 생성하기</h2><p>이제 Community Edition에서<br>Spark 클러스터를 직접 만들어 보겠습니다.</p><h3 id="클러스터-생성-방법"><a href="#클러스터-생성-방법" class="headerlink" title="클러스터 생성 방법"></a>클러스터 생성 방법</h3><ul><li>왼쪽 메뉴에서 <strong>Create Cluster</strong> 클릭</li></ul><hr><h3 id="클러스터-이름-설정"><a href="#클러스터-이름-설정" class="headerlink" title="클러스터 이름 설정"></a>클러스터 이름 설정</h3><p>클러스터를 구분하기 위한 이름을 입력합니다.</p><p>예시:</p><ul><li><code>spark-community-cluster</code></li></ul><hr><h3 id="Databricks-Runtime-Version-선택"><a href="#Databricks-Runtime-Version-선택" class="headerlink" title="Databricks Runtime Version 선택"></a>Databricks Runtime Version 선택</h3><p>다음으로 <strong>Databricks Runtime Version</strong>을 선택합니다.</p><p>기본 설정:</p><ul><li>Apache Spark 3.x</li><li>Python 3.x</li></ul><p>이 기본 설정은:</p><ul><li>이 강의의 모든 실습에 충분합니다</li><li>따로 변경하지 않아도 됩니다</li></ul><hr><h3 id="Community-Edition의-리소스-제한"><a href="#Community-Edition의-리소스-제한" class="headerlink" title="Community Edition의 리소스 제한"></a>Community Edition의 리소스 제한</h3><p>Azure Databricks와 비교하면<br>Community Edition은 설정 옵션이 매우 제한적입니다.</p><p>제공되는 리소스:</p><ul><li>메모리: 약 <strong>15GB</strong></li><li>CPU: <strong>2 Core</strong></li><li>Worker Node: ❌ 없음</li></ul><p>즉,</p><ul><li>Spark Driver</li><li>Spark Executors</li></ul><p>가 <strong>모두 하나의 머신에서 실행</strong>됩니다.</p><p>이 구조를 <strong>Single-node Cluster</strong>라고 합니다.</p><hr><h2 id="7-클러스터-생성"><a href="#7-클러스터-생성" class="headerlink" title="7. 클러스터 생성"></a>7. 클러스터 생성</h2><p>모든 설정을 확인한 후:</p><ul><li><strong>Create Cluster</strong> 버튼 클릭</li></ul><p>잠시 후 클러스터가 생성되고 실행됩니다.</p><hr><h2 id="8-클러스터-상태-및-Spark-UI-확인"><a href="#8-클러스터-상태-및-Spark-UI-확인" class="headerlink" title="8. 클러스터 상태 및 Spark UI 확인"></a>8. 클러스터 상태 및 Spark UI 확인</h2><p>생성된 클러스터를 클릭한 후:</p><ul><li><strong>Spark Cluster UI</strong>로 이동</li><li><strong>Executors</strong> 탭 클릭</li></ul><p>여기서 확인할 수 있는 내용:</p><ul><li>실행 중인 머신: <strong>1대</strong></li><li>Driver와 Executor가 동일한 인스턴스에서 실행 중</li></ul><p>이는 Community Edition의 특징이며,<br>실제 멀티 노드 클러스터와의 가장 큰 차이점입니다.</p><hr><h2 id="9-Community-Edition의-특징-정리"><a href="#9-Community-Edition의-특징-정리" class="headerlink" title="9. Community Edition의 특징 정리"></a>9. Community Edition의 특징 정리</h2><p>Databricks Community Edition의 특징을 정리하면 다음과 같습니다.</p><ul><li>무료 사용 가능</li><li>Single-node 클러스터</li><li>제한된 메모리와 CPU</li><li>학습 및 실습용으로 매우 적합</li><li>실무 환경과는 구조적으로 차이가 있음</li></ul><hr><h2 id="10-마무리-및-다음-강의-예고"><a href="#10-마무리-및-다음-강의-예고" class="headerlink" title="10. 마무리 및 다음 강의 예고"></a>10. 마무리 및 다음 강의 예고</h2><p>이제 Databricks Community Edition에서:</p><ul><li>계정을 생성하고</li><li>Spark 클러스터를 만드는 방법을 배웠습니다</li></ul><p>다음 강의에서는:</p><ul><li>이 클러스터를 이용해</li><li><strong>Apache Spark 코드를 직접 작성하고 실행</strong>해 보겠습니다</li></ul><p>다음 영상에서 뵙겠습니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Databricks-Community-Edition-계정-생성-및-클러스터-만들기&quot;&gt;&lt;a href=&quot;#Databricks-Community-Edition-계정-생성-및-클러스터-만들기&quot; class=&quot;headerlink&quot; title=&quot;Da</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/tags/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Certified-Developer-2</title>
    <link href="https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-2/"/>
    <id>https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-2/</id>
    <published>2025-12-18T15:13:19.000Z</published>
    <updated>2025-12-18T15:22:08.584Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Azure-Databricks에서-Apache-Spark-클러스터-생성하기"><a href="#Azure-Databricks에서-Apache-Spark-클러스터-생성하기" class="headerlink" title="Azure Databricks에서 Apache Spark 클러스터 생성하기"></a>Azure Databricks에서 Apache Spark 클러스터 생성하기</h1><h2 id="1-이-강의에서-배울-내용"><a href="#1-이-강의에서-배울-내용" class="headerlink" title="1. 이 강의에서 배울 내용"></a>1. 이 강의에서 배울 내용</h2><p>이 영상에서는 <strong>Apache Spark를 실행하기 위한 Databricks 클러스터를 생성하는 방법</strong>을 살펴보겠습니다.<br>구체적으로는 다음 내용을 다룹니다.</p><ul><li>Azure Databricks 워크스페이스 생성</li><li>Databricks 환경 실행</li><li>Spark 클러스터 생성</li><li>클러스터 주요 설정 옵션 이해</li><li>클러스터 상태 및 Spark UI 확인</li></ul><p>Spark는 <strong>클러스터 없이 실행될 수 없기 때문에</strong>,<br>이 단계는 이후 모든 실습의 기초가 되는 매우 중요한 과정입니다.</p><hr><h2 id="2-Azure-Databricks-워크스페이스-생성"><a href="#2-Azure-Databricks-워크스페이스-생성" class="headerlink" title="2. Azure Databricks 워크스페이스 생성"></a>2. Azure Databricks 워크스페이스 생성</h2><p>먼저 <strong>Azure Portal</strong>에서 시작합니다.</p><p>이미 Azure 계정이 있다면:</p><ul><li>Azure Portal에 로그인</li><li>상단 검색창에서 <strong>Databricks</strong> 검색</li><li><strong>Azure Databricks</strong> 선택</li></ul><p>만약 바로 보이지 않으면 검색창에<br><code>Databricks</code> 라고 입력하면 쉽게 찾을 수 있습니다.</p><p>Azure Databricks를 선택한 후 <strong>Add(추가)</strong> 또는 <strong>Create(생성)</strong> 를 클릭합니다.</p><hr><h3 id="필수-설정-항목"><a href="#필수-설정-항목" class="headerlink" title="필수 설정 항목"></a>필수 설정 항목</h3><p>이제 몇 가지 필수 정보를 입력해야 합니다.</p><h4 id="1-Resource-Group"><a href="#1-Resource-Group" class="headerlink" title="1) Resource Group"></a>1) Resource Group</h4><ul><li>새로운 Resource Group 생성</li><li>예시 이름:<ul><li><code>spark-certification-resources</code></li></ul></li></ul><p>Resource Group은 관련된 Azure 리소스들을<br><strong>하나의 묶음으로 관리하기 위한 컨테이너</strong>라고 생각하시면 됩니다.</p><hr><h4 id="2-Workspace-Name"><a href="#2-Workspace-Name" class="headerlink" title="2) Workspace Name"></a>2) Workspace Name</h4><ul><li>Databricks 워크스페이스 이름 지정</li><li>예시:<ul><li><code>spark-certification-training</code></li></ul></li></ul><p>이 이름은 Databricks 환경의 식별자 역할을 합니다.</p><hr><h4 id="3-Location"><a href="#3-Location" class="headerlink" title="3) Location"></a>3) Location</h4><ul><li>Databricks를 생성할 리전 선택</li><li>예시:<ul><li><code>US EAST</code></li></ul></li></ul><p>일반적으로는 <strong>본인과 가까운 지역</strong>을 선택하는 것이 좋습니다.</p><hr><h3 id="선택-Tags-설정"><a href="#선택-Tags-설정" class="headerlink" title="(선택) Tags 설정"></a>(선택) Tags 설정</h3><p>Tags는 필수는 아니지만 <strong>실무에서는 매우 유용</strong>합니다.</p><p>예시:</p><ul><li>Key: <code>purpose</code></li><li>Value: <code>spark-certification</code></li></ul><p>Tags를 사용하면:</p><ul><li>리소스 목적 구분</li><li>비용 추적</li><li>리소스 관리</li></ul><p>에 도움이 됩니다.</p><hr><h3 id="Review-Create"><a href="#Review-Create" class="headerlink" title="Review &amp; Create"></a>Review &amp; Create</h3><ul><li><strong>Review + Create</strong> 클릭</li><li>설정을 확인한 뒤 <strong>Create</strong> 클릭</li></ul><p>Databricks 워크스페이스 생성에는<br>보통 몇 분 정도가 소요됩니다.</p><p>이 시점에서는 영상을 잠시 멈추고,<br>배포가 완료될 때까지 기다립니다.</p><hr><h2 id="3-Databricks-워크스페이스-실행"><a href="#3-Databricks-워크스페이스-실행" class="headerlink" title="3. Databricks 워크스페이스 실행"></a>3. Databricks 워크스페이스 실행</h2><p>배포가 완료되면:</p><ul><li><strong>Go to resource</strong> 클릭</li><li>또는 Azure 홈 화면에서 해당 리소스로 이동</li></ul><p>리소스 화면에서 <strong>Launch Workspace</strong> 를 클릭합니다.</p><p>이제 브라우저에서 <strong>Azure Databricks 환경</strong>이 열립니다.</p><hr><h2 id="4-Databricks-클러스터-생성"><a href="#4-Databricks-클러스터-생성" class="headerlink" title="4. Databricks 클러스터 생성"></a>4. Databricks 클러스터 생성</h2><p>Databricks 환경에 접속한 후:</p><ul><li>왼쪽 메뉴에서 <strong>Clusters</strong> 클릭</li><li><strong>Create Cluster</strong> 클릭</li></ul><p>이제 Spark가 실행될 <strong>클러스터 설정 단계</strong>로 들어갑니다.</p><hr><h3 id="클러스터-이름"><a href="#클러스터-이름" class="headerlink" title="클러스터 이름"></a>클러스터 이름</h3><p>클러스터를 구분할 수 있도록 이름을 지정합니다.</p><p>예시:</p><ul><li><code>spark-training-cluster</code></li></ul><hr><h3 id="Cluster-Mode"><a href="#Cluster-Mode" class="headerlink" title="Cluster Mode"></a>Cluster Mode</h3><p>두 가지 모드가 있습니다.</p><ul><li><strong>Standard</strong></li><li><strong>High Concurrency</strong></li></ul><p>이 강의에서는:</p><ul><li>✅ <strong>Standard 모드</strong>를 사용합니다.</li></ul><p>High Concurrency 모드는<br>여러 사용자가 동시에 하나의 클러스터를 사용할 때 주로 사용됩니다.</p><hr><h2 id="5-Databricks-Runtime-Version"><a href="#5-Databricks-Runtime-Version" class="headerlink" title="5. Databricks Runtime Version"></a>5. Databricks Runtime Version</h2><p>다음은 <strong>Databricks Runtime Version</strong> 선택입니다.</p><p>Databricks Runtime은 다음을 포함한 <strong>통합 실행 환경</strong>입니다.</p><ul><li>Apache Spark</li><li>Scala</li><li>Python</li><li>다양한 최적화 라이브러리</li></ul><p>이 강의에서는:</p><ul><li><strong>Spark 3.x Runtime</strong>을 사용합니다.</li></ul><p>처음에는 기본값을 사용해도 전혀 문제 없습니다.</p><hr><h2 id="6-Driver와-Worker-설정-중요"><a href="#6-Driver와-Worker-설정-중요" class="headerlink" title="6. Driver와 Worker 설정 (중요)"></a>6. Driver와 Worker 설정 (중요)</h2><p>이제 클러스터의 <strong>하드웨어 사양</strong>을 설정합니다.</p><h3 id="Worker-Type"><a href="#Worker-Type" class="headerlink" title="Worker Type"></a>Worker Type</h3><p>Worker 노드는 <strong>Spark Executor가 실행되는 머신</strong>입니다.</p><p>예시:</p><ul><li>Memory: 14GB</li><li>CPU: 4 cores</li></ul><p>드롭다운을 열면:</p><ul><li>다양한 VM 타입</li><li>메모리&#x2F;CPU 조합</li><li>비용 차이</li></ul><p>를 확인할 수 있습니다.</p><p>지금은 기본 설정을 그대로 사용하겠습니다.</p><hr><h3 id="Driver-Type"><a href="#Driver-Type" class="headerlink" title="Driver Type"></a>Driver Type</h3><p>Driver 노드는 <strong>Spark Driver 프로세스</strong>가 실행되는 머신입니다.</p><ul><li>Worker와 동일한 사양을 사용할 수도 있고</li><li>Driver에 더 많은 메모리를 줄 수도 있습니다</li></ul><p>예를 들어:</p><ul><li>Driver를 Worker보다 <strong>메모리 2배</strong>로 설정</li></ul><p>Driver는 전체 작업을 조율하기 때문에<br>대규모 작업에서는 더 많은 리소스를 주는 것이 일반적입니다.</p><hr><h2 id="7-Worker-개수와-Auto-Scaling"><a href="#7-Worker-개수와-Auto-Scaling" class="headerlink" title="7. Worker 개수와 Auto Scaling"></a>7. Worker 개수와 Auto Scaling</h2><p>여기서 다음을 설정할 수 있습니다.</p><ul><li>최소 Worker 수</li><li>최대 Worker 수</li></ul><p>예시:</p><ul><li>최소: 2</li><li>최대: 8</li></ul><p>이 의미는:</p><ul><li>항상 최소 2개의 Worker는 유지</li><li>작업량이 증가하면 자동으로 Worker 추가</li></ul><p>이 기능을 <strong>Auto Scaling</strong>이라고 합니다.</p><hr><h3 id="Auto-Scaling의-장점"><a href="#Auto-Scaling의-장점" class="headerlink" title="Auto Scaling의 장점"></a>Auto Scaling의 장점</h3><ul><li>Worker 수 증가 → 병렬 처리 증가</li><li>병렬 처리 증가 → 처리 속도 향상</li></ul><p>예시:</p><ul><li>4 Workers × 14GB &#x3D; <strong>56GB 메모리</strong></li><li>8 Workers × 14GB &#x3D; <strong>112GB 메모리</strong></li></ul><p>CPU 코어도 동일한 방식으로 증가합니다.</p><hr><h2 id="8-클러스터-생성"><a href="#8-클러스터-생성" class="headerlink" title="8. 클러스터 생성"></a>8. 클러스터 생성</h2><p>모든 설정이 완료되면:</p><ul><li><strong>Create Cluster</strong> 클릭</li></ul><p>클러스터 생성에는 몇 분 정도가 소요됩니다.</p><p>이 단계에서도 잠시 영상을 멈추고 기다립니다.</p><hr><h2 id="9-클러스터-상태-확인-Spark-UI"><a href="#9-클러스터-상태-확인-Spark-UI" class="headerlink" title="9. 클러스터 상태 확인 (Spark UI)"></a>9. 클러스터 상태 확인 (Spark UI)</h2><p>클러스터 생성이 완료되면:</p><ul><li>클러스터 이름 클릭</li><li><strong>Spark UI</strong> 확인</li></ul><p>여기서 다음 정보를 볼 수 있습니다.</p><ul><li>Worker 개수</li><li>각 Worker의 메모리 및 CPU</li><li>실행 중인 애플리케이션</li><li>각 노드의 IP 주소</li></ul><p>중요한 포인트 하나:</p><ul><li><strong>실제 멀티 노드 클러스터</strong>에서는<br>각 Worker가 <strong>서로 다른 IP 주소</strong>를 가집니다.</li><li><strong>Databricks Community Edition</strong>에서는<br>모든 것이 한 머신에서 실행되기 때문에 IP가 동일합니다.</li></ul><p>이를 통해:</p><ul><li>실제 분산 환경</li><li>단일 노드 환경</li></ul><p>의 차이를 명확히 확인할 수 있습니다.</p><hr><h2 id="10-다음-강의-예고"><a href="#10-다음-강의-예고" class="headerlink" title="10. 다음 강의 예고"></a>10. 다음 강의 예고</h2><p>이제 Azure Databricks에서<br><strong>실제 Spark 클러스터를 생성하는 방법</strong>을 배웠습니다.</p><p>다음 강의에서는:</p><ul><li><strong>Databricks Community Edition</strong>에서 클러스터 생성</li><li>Single-node 환경에서 Spark가 어떻게 동작하는지를 살펴보겠습니다.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Azure-Databricks에서-Apache-Spark-클러스터-생성하기&quot;&gt;&lt;a href=&quot;#Azure-Databricks에서-Apache-Spark-클러스터-생성하기&quot; class=&quot;headerlink&quot; title=&quot;Azure Data</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/tags/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Certified-Developer-1</title>
    <link href="https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-1/"/>
    <id>https://kish191919.github.io/2025/12/18/DATABRICKS-Certified-Developer-1/</id>
    <published>2025-12-18T15:10:28.000Z</published>
    <updated>2025-12-18T15:12:54.951Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Apache-Spark-Cluster-Architecture-–-Easy-Explanation"><a href="#Apache-Spark-Cluster-Architecture-–-Easy-Explanation" class="headerlink" title="Apache Spark Cluster Architecture – Easy Explanation"></a>Apache Spark Cluster Architecture – Easy Explanation</h1><h2 id="1-Computing-on-a-Single-Computer"><a href="#1-Computing-on-a-Single-Computer" class="headerlink" title="1. Computing on a Single Computer"></a>1. Computing on a Single Computer</h2><p>먼저, 하나의 컴퓨터에서 컴퓨팅이 어떻게 이루어지는지 살펴보겠습니다.</p><p>하나의 컴퓨터에는 다음과 같은 컴퓨팅 자원이 있습니다.</p><ul><li><strong>CPU</strong>: 연산을 수행</li><li><strong>Memory (RAM)</strong>: 실행 중인 데이터 저장</li><li><strong>GPU</strong>: 대규모 병렬 연산 (필요한 경우)</li></ul><p>이 모든 자원은 <strong>운영체제(OS)</strong> 가 관리합니다.</p><h3 id="Operating-System의-역할"><a href="#Operating-System의-역할" class="headerlink" title="Operating System의 역할"></a>Operating System의 역할</h3><p>운영체제는 여러 애플리케이션이 동시에 실행될 때:</p><ul><li>CPU와 메모리를 어떻게 나눠 쓸지 결정하고</li><li>각 애플리케이션의 자원 사용을 스케줄링합니다</li></ul><p>덕분에 여러 프로그램이 동시에 실행되어도 시스템이 안정적으로 동작합니다.</p><hr><h2 id="2-Why-Single-Computer-Is-Not-Enough-for-Big-Data"><a href="#2-Why-Single-Computer-Is-Not-Enough-for-Big-Data" class="headerlink" title="2. Why Single Computer Is Not Enough for Big Data"></a>2. Why Single Computer Is Not Enough for Big Data</h2><p>빅데이터를 처리하려면 다음과 같은 문제가 발생합니다.</p><ul><li>데이터 크기가 너무 큼</li><li>연산량이 많음</li><li>처리 시간이 오래 걸림</li></ul><p>그래서 하나의 컴퓨터가 아닌 <strong>여러 대의 컴퓨터를 함께 사용하는 방식</strong>이 필요합니다.</p><p>이것이 바로 <strong>Computer Cluster</strong>입니다.</p><hr><h2 id="3-What-Is-a-Computer-Cluster"><a href="#3-What-Is-a-Computer-Cluster" class="headerlink" title="3. What Is a Computer Cluster?"></a>3. What Is a Computer Cluster?</h2><p><strong>컴퓨터 클러스터</strong>란 여러 대의 컴퓨터를 네트워크로 연결하여<br>마치 하나의 거대한 컴퓨터처럼 사용하는 구조입니다.</p><ul><li>각 컴퓨터를 <strong>Node</strong>라고 부름</li><li>노드들은 고속 네트워크로 연결됨</li><li>각 노드에는 독립적인 운영체제가 실행됨</li></ul><p>하지만 여기서 새로운 문제가 생깁니다.</p><blockquote><p>누가 이 많은 컴퓨터의 자원을 관리할까?</p></blockquote><hr><h2 id="4-Cluster-Manager-Node-Manager"><a href="#4-Cluster-Manager-Node-Manager" class="headerlink" title="4. Cluster Manager &amp; Node Manager"></a>4. Cluster Manager &amp; Node Manager</h2><p>Single Computer에서는 OS가 자원을 관리했습니다.<br>Cluster 환경에서는 이를 대신하는 <strong>전용 자원 관리자</strong>가 필요합니다.</p><h3 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager"></a>Cluster Manager</h3><p>Cluster Manager는 클러스터 전체를 관리합니다.</p><p>주요 역할:</p><ul><li>전체 자원(CPU, Memory) 관리</li><li>여러 애플리케이션 간 자원 할당</li><li>각 노드의 상태 및 헬스 체크</li></ul><p>👉 클러스터에서 애플리케이션을 실행하려면 반드시 Cluster Manager를 거쳐야 합니다.</p><h3 id="Node-Manager"><a href="#Node-Manager" class="headerlink" title="Node Manager"></a>Node Manager</h3><p>각 노드에는 <strong>Node Manager</strong>가 실행됩니다.</p><p>역할:</p><ul><li>해당 노드의 자원 상태 모니터링</li><li>실행 중인 프로세스 관리</li><li>서버 장애 및 애플리케이션 실패 감지</li><li>모든 정보를 Cluster Manager에게 보고</li></ul><p>📌<br><strong>Cluster Manager + Node Manager &#x3D; 클러스터의 운영체제</strong></p><hr><h2 id="5-Apache-Spark-Application-Components"><a href="#5-Apache-Spark-Application-Components" class="headerlink" title="5. Apache Spark Application Components"></a>5. Apache Spark Application Components</h2><p>Apache Spark 애플리케이션은 항상 두 가지 구성 요소를 가집니다.</p><h3 id="Spark-Driver"><a href="#Spark-Driver" class="headerlink" title="Spark Driver"></a>Spark Driver</h3><ul><li>애플리케이션의 두뇌</li><li>전체 작업 계획 및 제어</li><li>자원 요청 및 Executor 관리</li><li>작업(Task) 분배</li></ul><h3 id="Spark-Executors"><a href="#Spark-Executors" class="headerlink" title="Spark Executors"></a>Spark Executors</h3><ul><li>실제 연산 수행</li><li>Driver로부터 Task를 받아 실행</li><li>결과를 Driver에게 반환</li></ul><hr><h2 id="6-Spark-Application-Execution-Flow"><a href="#6-Spark-Application-Execution-Flow" class="headerlink" title="6. Spark Application Execution Flow"></a>6. Spark Application Execution Flow</h2><p>Spark 애플리케이션이 실행되는 과정을 단계별로 살펴보겠습니다.</p><h3 id="Step-1-spark-submit"><a href="#Step-1-spark-submit" class="headerlink" title="Step 1: spark-submit"></a>Step 1: spark-submit</h3><p>사용자는 <code>spark-submit</code> 명령으로 애플리케이션을 제출합니다.</p><p>이때 다음 설정을 전달합니다.</p><ul><li>Driver memory (예: 20GB)</li><li>Executor memory (예: 20GB)</li><li>CPU cores (예: 4)</li><li>Number of executors (예: 3)</li></ul><hr><h3 id="Step-2-Driver-Creation"><a href="#Step-2-Driver-Creation" class="headerlink" title="Step 2: Driver Creation"></a>Step 2: Driver Creation</h3><ul><li>Cluster Manager가 Node Manager에게 요청</li><li>자원이 충분한 노드에서 Driver 프로세스 생성</li></ul><hr><h3 id="Step-3-Executor-Request"><a href="#Step-3-Executor-Request" class="headerlink" title="Step 3: Executor Request"></a>Step 3: Executor Request</h3><ul><li>Driver가 Cluster Manager에게 Executor 요청</li><li>설정된 개수와 자원 정보 전달</li></ul><hr><h3 id="Step-4-Executor-Creation"><a href="#Step-4-Executor-Creation" class="headerlink" title="Step 4: Executor Creation"></a>Step 4: Executor Creation</h3><ul><li>Cluster Manager → Node Manager</li><li>자원이 충분한 노드에서 Executor 생성</li><li>Executor 위치 정보를 Cluster Manager에게 보고</li></ul><hr><h3 id="Step-5-Driver–Executor-Connection"><a href="#Step-5-Driver–Executor-Connection" class="headerlink" title="Step 5: Driver–Executor Connection"></a>Step 5: Driver–Executor Connection</h3><ul><li>Cluster Manager가 Executor 위치를 Driver에게 전달</li><li>Spark 애플리케이션 실행 완료</li></ul><hr><h2 id="7-Parallel-Processing-in-Spark"><a href="#7-Parallel-Processing-in-Spark" class="headerlink" title="7. Parallel Processing in Spark"></a>7. Parallel Processing in Spark</h2><p>Driver는 작업을 여러 <strong>Task</strong>로 나누어 Executors에 전달합니다.</p><p>예를 들어 큰 파일을 읽을 때:</p><ul><li>파일을 여러 조각으로 분할</li><li>여러 Executor가 동시에 처리</li></ul><p>👉 이것이 Spark가 빠른 이유이며, 높은 병렬성을 제공합니다.</p><hr><h2 id="8-Spark-Deployment-Modes"><a href="#8-Spark-Deployment-Modes" class="headerlink" title="8. Spark Deployment Modes"></a>8. Spark Deployment Modes</h2><p>Spark Driver가 어디에서 실행되느냐에 따라 Deployment Mode가 결정됩니다.</p><h3 id="Cluster-Mode"><a href="#Cluster-Mode" class="headerlink" title="Cluster Mode"></a>Cluster Mode</h3><ul><li>Driver와 Executor 모두 클러스터 내부</li><li>Cluster Manager가 전체 관리</li><li><strong>Production 환경에 적합</strong></li></ul><hr><h3 id="Client-Mode"><a href="#Client-Mode" class="headerlink" title="Client Mode"></a>Client Mode</h3><ul><li>Driver는 클러스터 외부 (사용자 머신)</li><li>Executor는 클러스터 내부</li><li>Driver가 종료되면 애플리케이션도 종료됨</li></ul><p>👉 개발 및 테스트에 주로 사용</p><hr><h3 id="Local-Mode"><a href="#Local-Mode" class="headerlink" title="Local Mode"></a>Local Mode</h3><ul><li>Driver와 Executor가 한 대의 머신에서 실행</li><li>로컬 테스트 및 학습용</li></ul><hr><h2 id="9-Responsibilities-Summary"><a href="#9-Responsibilities-Summary" class="headerlink" title="9. Responsibilities Summary"></a>9. Responsibilities Summary</h2><h3 id="Spark-Driver-1"><a href="#Spark-Driver-1" class="headerlink" title="Spark Driver"></a>Spark Driver</h3><ul><li>자원 요청 및 관리</li><li>작업 스케줄링</li><li>Executor 상태 모니터링</li><li>장애 발생 시 재요청</li><li>사용자 입력 처리</li></ul><h3 id="Spark-Executor"><a href="#Spark-Executor" class="headerlink" title="Spark Executor"></a>Spark Executor</h3><ul><li>Task 실행</li><li>결과 반환</li><li>단순하고 명확한 역할</li></ul><hr><h2 id="10-Databricks-and-Spark"><a href="#10-Databricks-and-Spark" class="headerlink" title="10. Databricks and Spark"></a>10. Databricks and Spark</h2><p>Databricks는 Spark를 쉽게 사용할 수 있는 플랫폼입니다.</p><h3 id="Databricks-Editions"><a href="#Databricks-Editions" class="headerlink" title="Databricks Editions"></a>Databricks Editions</h3><ul><li><strong>Community Edition (무료)</strong><ul><li>Single-node cluster</li><li>학습용으로 충분</li></ul></li><li><strong>Paid Edition (AWS &#x2F; Azure)</strong><ul><li>Multi-node cluster</li><li>실제 분산 처리 구조 학습 가능</li></ul></li></ul><p>이 강의에서는:</p><ul><li>Community Edition으로 기본 개념 학습</li><li>Azure Databricks로 실제 분산 구조 시연</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Apache-Spark-Cluster-Architecture-–-Easy-Explanation&quot;&gt;&lt;a href=&quot;#Apache-Spark-Cluster-Architecture-–-Easy-Explanation&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_CERTIFIED_DEVELOPER" scheme="https://kish191919.github.io/tags/DATABRICKS-CERTIFIED-DEVELOPER/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-5</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-5/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-5/</id>
    <published>2025-12-12T17:01:18.000Z</published>
    <updated>2025-12-12T17:10:47.449Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-오늘-강의에서-다룰-질문-3가지"><a href="#1-오늘-강의에서-다룰-질문-3가지" class="headerlink" title="1. 오늘 강의에서 다룰 질문 3가지"></a>1. 오늘 강의에서 다룰 질문 3가지</h2><p>여러분, Databricks 워크스페이스를 만들면 “웹에서 접속하는 화면”은 바로 보이죠.<br>그런데 수업을 하거나 실무에서 설명할 때는 <strong>아래 3가지 질문을 꼭 답할 수 있어야 합니다.</strong></p><ol><li><strong>Databricks는 Azure 어디에 설치되나요?</strong></li><li><strong>노트북&#x2F;잡&#x2F;클러스터&#x2F;데이터는 각각 어디에 있나요?</strong></li><li><strong>왜 굳이 Control Plane &#x2F; Data Plane으로 나누나요?</strong></li></ol><p>이 3가지를 이해하면, Databricks가 “그냥 웹앱”이 아니라 <strong>클라우드 네이티브 데이터 플랫폼</strong>이라는 게 보입니다.</p><hr><h2 id="2-큰-그림-Control-Plane-vs-Data-Plane"><a href="#2-큰-그림-Control-Plane-vs-Data-Plane" class="headerlink" title="2. 큰 그림: Control Plane vs Data Plane"></a>2. 큰 그림: Control Plane vs Data Plane</h2><h3 id="2-1-한-문장-요약"><a href="#2-1-한-문장-요약" class="headerlink" title="2.1 한 문장 요약"></a>2.1 한 문장 요약</h3><ul><li><strong>Control Plane(컨트롤 플레인)</strong>: Databricks가 제공하는 “관리&#x2F;제어” 영역 (UI, 노트북&#x2F;잡&#x2F;클러스터 설정 등)</li><li><strong>Data Plane(데이터 플레인)</strong>: 고객(Azure 구독) 안에서 실제로 돈이 나가는 “실행&#x2F;데이터” 영역 (클러스터 VM, 스토리지 등)</li></ul><blockquote><p>강의 멘트(핵심):<br>“**설정&#x2F;관리(컨트롤)**는 Databricks가, **실행&#x2F;데이터(데이터)**는 고객 구독에서 돌아간다.”</p></blockquote><hr><h2 id="3-Control-Plane-컨트롤-플레인-—-“Databricks의-두뇌”"><a href="#3-Control-Plane-컨트롤-플레인-—-“Databricks의-두뇌”" class="headerlink" title="3. Control Plane(컨트롤 플레인) — “Databricks의 두뇌”"></a>3. Control Plane(컨트롤 플레인) — “Databricks의 두뇌”</h2><h3 id="3-1-Control-Plane에-있는-것들-왜-중요한가"><a href="#3-1-Control-Plane에-있는-것들-왜-중요한가" class="headerlink" title="3.1 Control Plane에 있는 것들 (왜 중요한가?)"></a>3.1 Control Plane에 있는 것들 (왜 중요한가?)</h3><p>여기에는 Databricks가 제공하는 <strong>웹 애플리케이션(Workspace UI)</strong> 과 “메타데이터&#x2F;설정 정보”가 들어있습니다.</p><ul><li>Databricks Web Application (브라우저에서 보는 Workspace UI)</li><li>Notebooks (노트북 자체, 또는 노트북 메타&#x2F;버전&#x2F;권한 등)</li><li>Jobs &#x2F; Workflows 설정</li><li>Queries &#x2F; Alerts &#x2F; Dashboards 설정(특히 SQL 기능)</li><li>Cluster 설정(클러스터 생성 옵션, 노드 타입, 런타임 등)</li><li>사용자&#x2F;권한&#x2F;기본 정책(일부는 Account&#x2F;Workspace 레벨로 나뉨)</li></ul><p><strong>왜 컨트롤 플레인이 필요한가?</strong><br>Databricks는 사용자가 “직접 VM을 만들어서 Spark 설치하고 튜닝”하는 대신,<br><strong>UI&#x2F;REST API로 ‘요청’만 하면</strong> Databricks가 적절한 리소스를 띄우고 관리해줍니다.<br>즉, 컨트롤 플레인은 <strong>클라우드 인프라를 추상화해주는 제어 계층</strong>이에요.</p><h3 id="3-2-Control-Plane에-“직접-접속”-못하는-이유"><a href="#3-2-Control-Plane에-“직접-접속”-못하는-이유" class="headerlink" title="3.2 Control Plane에 “직접 접속” 못하는 이유"></a>3.2 Control Plane에 “직접 접속” 못하는 이유</h3><ul><li>사용자는 컨트롤 플레인 내부 서버에 SSH 같은 방식으로 들어가는 게 아니라,<ul><li><strong>브라우저(UI)</strong> 또는</li><li><strong>REST API&#x2F;CLI</strong><br>로만 기능을 사용합니다.</li></ul></li></ul><p><strong>이렇게 막아두는 이유</strong></p><ul><li>보안&#x2F;컴플라이언스: 관리 계층에 대한 직접 접근을 줄여 공격면을 최소화</li><li>운영 안정성: Databricks가 서비스 품질(SLA)을 보장하려면 관리 영역은 일관된 방식으로 운영되어야 함</li></ul><hr><h2 id="4-Data-Plane-데이터-플레인-—-“실제-실행과-비용이-발생하는-곳”"><a href="#4-Data-Plane-데이터-플레인-—-“실제-실행과-비용이-발생하는-곳”" class="headerlink" title="4. Data Plane(데이터 플레인) — “실제 실행과 비용이 발생하는 곳”"></a>4. Data Plane(데이터 플레인) — “실제 실행과 비용이 발생하는 곳”</h2><h3 id="4-1-Data-Plane에-있는-핵심-요소"><a href="#4-1-Data-Plane에-있는-핵심-요소" class="headerlink" title="4.1 Data Plane에 있는 핵심 요소"></a>4.1 Data Plane에 있는 핵심 요소</h3><p>Data Plane은 <strong>고객의 Azure Subscription</strong> 안에 있습니다. 그래서 비용도 여기서 발생합니다.</p><ul><li><strong>Compute(클러스터&#x2F;SQL Warehouse 실행 리소스)</strong><ul><li>Spark 작업을 돌리는 VM&#x2F;노드들(드라이버&#x2F;워커)</li></ul></li><li><strong>Storage(데이터 저장소)</strong><ul><li>고객 데이터: 보통 ADLS Gen2 &#x2F; Blob Storage</li><li>워크스페이스 기본 스토리지(Workspace Root Storage &#x2F; Root DBFS)</li></ul></li></ul><blockquote><p>강의 멘트(핵심):<br>“컨트롤 플레인은 ‘설정과 명령’, 데이터 플레인은 ‘실제 실행과 데이터’입니다.<br>비용이 큰 건 대부분 데이터 플레인에서 나옵니다.”</p></blockquote><h3 id="4-2-“클러스터는-왜-고객-구독에-만들까-”"><a href="#4-2-“클러스터는-왜-고객-구독에-만들까-”" class="headerlink" title="4.2 “클러스터는 왜 고객 구독에 만들까?”"></a>4.2 “클러스터는 왜 고객 구독에 만들까?”</h3><ul><li><strong>비용&#x2F;과금 주체가 고객</strong>이기 때문</li><li><strong>네트워크&#x2F;보안(예: VNet, Private Link, 방화벽, NSG) 정책을 고객이 통제</strong>하기 때문</li><li>기업 환경에서는 데이터&#x2F;네트워크 통제가 매우 중요 → 실행 인프라가 고객 구독에 있어야 함</li></ul><hr><h2 id="5-DBFS와-Workspace-Root-Storage-—-“기본-저장소지만-메인-데이터레이크는-아니다”"><a href="#5-DBFS와-Workspace-Root-Storage-—-“기본-저장소지만-메인-데이터레이크는-아니다”" class="headerlink" title="5. DBFS와 Workspace Root Storage — “기본 저장소지만, 메인 데이터레이크는 아니다”"></a>5. DBFS와 Workspace Root Storage — “기본 저장소지만, 메인 데이터레이크는 아니다”</h2><h3 id="5-1-DBFS-Databricks-File-System-가-뭔가요"><a href="#5-1-DBFS-Databricks-File-System-가-뭔가요" class="headerlink" title="5.1 DBFS( Databricks File System )가 뭔가요?"></a>5.1 DBFS( Databricks File System )가 뭔가요?</h3><p>초보자 기준으로는 이렇게 이해하면 됩니다.</p><ul><li>DBFS는 Databricks에서 파일을 다루기 위한 <strong>가상 파일시스템 인터페이스</strong></li><li>내부적으로는 Azure Storage(ADLS&#x2F;Blob) 같은 곳을 기반으로 동작할 수 있음</li><li>예: 라이브러리 업로드, 임시 파일, 일부 로그&#x2F;결과 등</li></ul><h3 id="5-2-Workspace-Root-Storage-Root-DBFS-주의사항"><a href="#5-2-Workspace-Root-Storage-Root-DBFS-주의사항" class="headerlink" title="5.2 Workspace Root Storage(&#x3D; Root DBFS) 주의사항"></a>5.2 Workspace Root Storage(&#x3D; Root DBFS) 주의사항</h3><p>워크스페이스를 만들면 자동으로 “기본 저장소”가 하나 생기는데, 이것이 Root DBFS입니다.</p><ul><li>워크스페이스 시스템 용도(일부 로그&#x2F;시스템 데이터)에 적합</li><li>하지만 <strong>실무에서 데이터레이크(원천&#x2F;정제&#x2F;골드 데이터)</strong> 를 여기에 넣는 건 권장되지 않음</li></ul><p><strong>왜 별도 Storage Account가 필요할까?</strong></p><ul><li>데이터 거버넌스&#x2F;보안: “데이터는 고객이 직접 통제하는 저장소에”</li><li>운영&#x2F;백업&#x2F;수명주기(Lifecycle): 워크스페이스와 데이터의 생명주기를 분리</li><li>멀티 워크스페이스 공유: 하나의 데이터레이크를 여러 워크스페이스에서 접근</li><li>비용&#x2F;정책&#x2F;권한 모델을 더 명확하게 설계 가능</li></ul><blockquote><p>강의 멘트:<br>“Root DBFS는 ‘워크스페이스의 기본 공용창고’ 같은 느낌이고,<br>우리 회사의 핵심 데이터는 ‘별도의 공식 창고(ADLS Gen2)’에 보관하는 게 정석입니다.”</p></blockquote><hr><h2 id="6-인증과-접근-방식-—-SSO-API-CLI"><a href="#6-인증과-접근-방식-—-SSO-API-CLI" class="headerlink" title="6. 인증과 접근 방식 — SSO &#x2F; API &#x2F; CLI"></a>6. 인증과 접근 방식 — SSO &#x2F; API &#x2F; CLI</h2><h3 id="6-1-사용자-Web-UI-접근"><a href="#6-1-사용자-Web-UI-접근" class="headerlink" title="6.1 사용자(Web UI) 접근"></a>6.1 사용자(Web UI) 접근</h3><ul><li>일반적으로 <strong>SSO</strong>(Microsoft Entra ID 기반)로 로그인</li><li>즉, “회사 계정”으로 Databricks에 접속하는 흐름이 자연스럽게 구성됨</li></ul><h3 id="6-2-자동화-도구-접근-REST-API-CLI"><a href="#6-2-자동화-도구-접근-REST-API-CLI" class="headerlink" title="6.2 자동화&#x2F;도구 접근(REST API &#x2F; CLI)"></a>6.2 자동화&#x2F;도구 접근(REST API &#x2F; CLI)</h3><ul><li>운영&#x2F;배포&#x2F;자동화에서는 UI보다 API&#x2F;CLI가 중요할 때가 많음</li><li>예: 노트북 배포, 잡 생성&#x2F;실행, 클러스터 관리, 리포지토리 연동 등</li></ul><blockquote><p>강의 팁:<br>“팀이 커지면 ‘수동 클릭’은 한계가 옵니다.<br>그래서 컨트롤 플레인이 REST API를 제공하는 게 매우 중요해요.”</p></blockquote><hr><h2 id="7-그림에서-보이는-추가-포인트-Serverless-Compute-Plane"><a href="#7-그림에서-보이는-추가-포인트-Serverless-Compute-Plane" class="headerlink" title="7. (그림에서 보이는) 추가 포인트: Serverless Compute Plane"></a>7. (그림에서 보이는) 추가 포인트: Serverless Compute Plane</h2><p>일부 기능(예: Serverless SQL Warehouses, Model Serving 등)은<br>“서버리스” 형태의 컴퓨팅 플레인을 사용하기도 합니다.</p><p>초보자 기준으로는 이렇게만 기억해도 충분합니다.</p><ul><li><strong>Classic compute plane</strong>: 일반적인 Spark 클러스터(드라이버&#x2F;워커 VM 기반)</li><li><strong>Serverless compute plane</strong>: 관리 부담을 더 줄인 실행 방식(서비스형)</li></ul><hr><h2 id="8-실무-베스트-프랙티스-체크리스트-강의에서-꼭-강조"><a href="#8-실무-베스트-프랙티스-체크리스트-강의에서-꼭-강조" class="headerlink" title="8. 실무 베스트 프랙티스 체크리스트 (강의에서 꼭 강조)"></a>8. 실무 베스트 프랙티스 체크리스트 (강의에서 꼭 강조)</h2><h3 id="8-1-리소스-정리-전략"><a href="#8-1-리소스-정리-전략" class="headerlink" title="8.1 리소스 정리 전략"></a>8.1 리소스 정리 전략</h3><ul><li>프로젝트마다 <strong>Resource Group</strong>을 분리 → 나중에 삭제&#x2F;정리 쉬움</li><li>실습 끝난 후 <strong>클러스터 종료&#x2F;삭제</strong> 습관화 (비용 폭탄 방지)</li></ul><h3 id="8-2-데이터-저장소-설계"><a href="#8-2-데이터-저장소-설계" class="headerlink" title="8.2 데이터 저장소 설계"></a>8.2 데이터 저장소 설계</h3><ul><li>Root DBFS에 핵심 데이터 적재 ❌</li><li>별도 <strong>ADLS Gen2 Storage Account</strong> 만들고,<ul><li>Raw &#x2F; Silver &#x2F; Gold 구조(또는 Bronze&#x2F;Silver&#x2F;Gold)</li><li>접근 권한(ACL&#x2F;RBAC) 명확히 설계</li></ul></li></ul><h3 id="8-3-네트워크-보안-기업-환경"><a href="#8-3-네트워크-보안-기업-환경" class="headerlink" title="8.3 네트워크&#x2F;보안(기업 환경)"></a>8.3 네트워크&#x2F;보안(기업 환경)</h3><ul><li>VNet 인젝션, Private Link, 방화벽&#x2F;NSG 정책 등은<br>“데이터 플레인”에서 고객이 통제한다는 점을 연결해서 설명</li></ul><hr><h2 id="9-자격증-인터뷰-포인트-따로-정리"><a href="#9-자격증-인터뷰-포인트-따로-정리" class="headerlink" title="9. 자격증&#x2F;인터뷰 포인트 (따로 정리)"></a>9. 자격증&#x2F;인터뷰 포인트 (따로 정리)</h2><h3 id="9-1-자주-나오는-핵심-질문"><a href="#9-1-자주-나오는-핵심-질문" class="headerlink" title="9.1 자주 나오는 핵심 질문"></a>9.1 자주 나오는 핵심 질문</h3><ol><li><strong>Control Plane과 Data Plane의 차이?</strong><ul><li>Control Plane: 관리&#x2F;UI&#x2F;설정&#x2F;메타데이터</li><li>Data Plane: 실행(클러스터)&#x2F;데이터 저장소(ADLS&#x2F;Blob)</li></ul></li><li><strong>비용은 어디서 발생?</strong><ul><li>대부분 Data Plane(클러스터 VM, 스토리지, 네트워크 egress 등)</li></ul></li><li><strong>노트북&#x2F;잡 설정은 어디에 저장?</strong><ul><li>Control Plane(설정&#x2F;메타), 실행은 Data Plane</li></ul></li><li><strong>DBFS Root Storage를 메인 데이터레이크로 쓰면 안 되는 이유?</strong><ul><li>데이터 거버넌스&#x2F;통제, 워크스페이스와 데이터 생명주기 분리, 공유&#x2F;보안&#x2F;운영 측면</li></ul></li><li><strong>Databricks에 접속하는 방법 2가지 이상 설명</strong><ul><li>Workspace UI(SSO), REST API&#x2F;CLI(토큰&#x2F;자격증명 기반)</li></ul></li></ol><h3 id="9-2-인터뷰에서-“좋아-보이는”-한-문장-답변-예시"><a href="#9-2-인터뷰에서-“좋아-보이는”-한-문장-답변-예시" class="headerlink" title="9.2 인터뷰에서 “좋아 보이는” 한 문장 답변 예시"></a>9.2 인터뷰에서 “좋아 보이는” 한 문장 답변 예시</h3><ul><li>“Databricks는 관리 계층(Control Plane)과 실행&#x2F;데이터 계층(Data Plane)을 분리해서<br><strong>운영&#x2F;보안&#x2F;확장성</strong>을 확보하고, 고객은 Data Plane에서 <strong>비용과 네트워크&#x2F;데이터 통제</strong>를 가져갑니다.”</li></ul><hr><h2 id="10-마무리-다음-단계-연결-멘트"><a href="#10-마무리-다음-단계-연결-멘트" class="headerlink" title="10. 마무리 (다음 단계 연결 멘트)"></a>10. 마무리 (다음 단계 연결 멘트)</h2><p>오늘은 “Databricks가 Azure 안에서 어떻게 나뉘어 배치되는지”를 봤습니다.<br>다음 단계는 아주 자연스럽게 이렇게 이어집니다.</p><ul><li><strong>Compute 만들기(Cluster &#x2F; SQL Warehouse)</strong></li><li><strong>Storage 연결하기(ADLS Gen2 Mount &#x2F; External Location)</strong></li><li><strong>데이터 적재&#x2F;변환 파이프라인 만들기(Notebook, Jobs, Workflows)</strong></li></ul><blockquote><p>엔딩 멘트(강의용):<br>“오늘 아키텍처를 이해하면, 앞으로 어떤 기능을 배워도 ‘이게 어디에서 돌아가는지’가 보이기 시작합니다.”</p></blockquote><hr><h2 id="부록-A-초보자-Q-A-강의-중-자주-받는-질문"><a href="#부록-A-초보자-Q-A-강의-중-자주-받는-질문" class="headerlink" title="부록 A. 초보자 Q&amp;A (강의 중 자주 받는 질문)"></a>부록 A. 초보자 Q&amp;A (강의 중 자주 받는 질문)</h2><h3 id="Q1-“워크스페이스가-서버리스-앱이면-클러스터는-왜-또-만들어요-”"><a href="#Q1-“워크스페이스가-서버리스-앱이면-클러스터는-왜-또-만들어요-”" class="headerlink" title="Q1) “워크스페이스가 서버리스 앱이면, 클러스터는 왜 또 만들어요?”"></a>Q1) “워크스페이스가 서버리스 앱이면, 클러스터는 왜 또 만들어요?”</h3><ul><li>워크스페이스(UI&#x2F;설정)는 “관리용”</li><li>실제 Spark 작업을 돌리는 계산 자원이 “클러스터”</li><li>즉, <strong>워크스페이스는 조종석, 클러스터는 엔진</strong>이라고 비유하면 쉬워요.</li></ul><h3 id="Q2-“Control-Plane이-Databricks에-있으면-데이터도-Databricks로-가나요-”"><a href="#Q2-“Control-Plane이-Databricks에-있으면-데이터도-Databricks로-가나요-”" class="headerlink" title="Q2) “Control Plane이 Databricks에 있으면 데이터도 Databricks로 가나요?”"></a>Q2) “Control Plane이 Databricks에 있으면 데이터도 Databricks로 가나요?”</h3><ul><li>핵심 데이터는 보통 고객 Storage(ADLS Gen2)에 둡니다.</li><li>Databricks는 그 데이터를 “처리”하고 결과를 다시 저장하는 역할.</li></ul><h3 id="Q3-“Data-Plane에-있는-VM은-내가-직접-접속-SSH-하나요-”"><a href="#Q3-“Data-Plane에-있는-VM은-내가-직접-접속-SSH-하나요-”" class="headerlink" title="Q3) “Data Plane에 있는 VM은 내가 직접 접속(SSH)하나요?”"></a>Q3) “Data Plane에 있는 VM은 내가 직접 접속(SSH)하나요?”</h3><ul><li>일반적으로는 Databricks가 관리하므로 직접 SSH는 권장되지 않는 경우가 많습니다.</li><li>운영 방식은 회사 정책&#x2F;보안 구성(VNet, Private Link 등)에 따라 달라질 수 있어요.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-오늘-강의에서-다룰-질문-3가지&quot;&gt;&lt;a href=&quot;#1-오늘-강의에서-다룰-질문-3가지&quot; class=&quot;headerlink&quot; title=&quot;1. 오늘 강의에서 다룰 질문 3가지&quot;&gt;&lt;/a&gt;1. 오늘 강의에서 다룰 질문 3가지&lt;/h2&gt;&lt;p&gt;</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-4</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-4/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-4/</id>
    <published>2025-12-12T16:44:59.000Z</published>
    <updated>2025-12-12T16:54:32.852Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Databricks-Workspace란"><a href="#1-Databricks-Workspace란" class="headerlink" title="1. Databricks Workspace란?"></a>1. Databricks Workspace란?</h2><p>Databricks Workspace는<br>👉 <strong>Databricks 플랫폼을 사용하는 모든 작업의 출발점</strong>입니다.</p><ul><li>코드 작성 (Notebook)</li><li>데이터 처리 (Spark &#x2F; SQL)</li><li>클러스터 생성 및 관리</li><li>워크플로우(Job) 생성</li><li>데이터 거버넌스, ML, SQL 분석</li></ul><p>👉 <strong>Databricks에서 하는 모든 작업은 Workspace에서 시작</strong>합니다.</p><hr><h2 id="2-Workspace-홈-화면-개요"><a href="#2-Workspace-홈-화면-개요" class="headerlink" title="2. Workspace 홈 화면 개요"></a>2. Workspace 홈 화면 개요</h2><h3 id="상단-Home-영역"><a href="#상단-Home-영역" class="headerlink" title="상단(Home) 영역"></a>상단(Home) 영역</h3><ul><li>최근 사용한 항목 (Recent)</li><li>즐겨찾기 (Favorites)</li><li>빠른 시작용 바로가기</li></ul><blockquote><p>강의 팁<br>👉 초반에는 <strong>거의 사용하지 않음</strong><br>👉 실제 작업은 <strong>왼쪽 메뉴가 핵심</strong></p></blockquote><hr><h2 id="3-왼쪽-메뉴-Left-Navigation-Bar"><a href="#3-왼쪽-메뉴-Left-Navigation-Bar" class="headerlink" title="3. 왼쪽 메뉴 (Left Navigation Bar)"></a>3. 왼쪽 메뉴 (Left Navigation Bar)</h2><h3 id="메뉴-확장-축소"><a href="#메뉴-확장-축소" class="headerlink" title="메뉴 확장&#x2F;축소"></a>메뉴 확장&#x2F;축소</h3><ul><li>기본은 축소 상태일 수 있음</li><li>마우스를 가져가면 자동 확장</li><li>⚙️ 강의 시에는 <strong>항상 확장 상태 추천</strong></li></ul><hr><h2 id="4-Workspace-메뉴-가장-중요-⭐"><a href="#4-Workspace-메뉴-가장-중요-⭐" class="headerlink" title="4. Workspace 메뉴 (가장 중요 ⭐)"></a>4. Workspace 메뉴 (가장 중요 ⭐)</h2><h3 id="역할"><a href="#역할" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>코드와 파일이 저장되는 공간</strong></p><h3 id="구조"><a href="#구조" class="headerlink" title="구조"></a>구조</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Workspace</span><br><span class="line"> ├── Shared</span><br><span class="line"> └── Users</span><br><span class="line">     └── &lt;사용자명&gt;</span><br></pre></td></tr></table></figure><h3 id="Users-내-홈-디렉토리"><a href="#Users-내-홈-디렉토리" class="headerlink" title="Users &gt; 내 홈 디렉토리"></a>Users &gt; 내 홈 디렉토리</h3><ul><li>개인 작업 공간</li><li>생성 가능 항목:<ul><li>📁 Folder</li><li>📓 Notebook (Python &#x2F; SQL &#x2F; Scala)</li><li>📄 File</li><li>📊 Dashboard</li><li>🔔 Alert</li></ul></li></ul><blockquote><p>중요 포인트<br>👉 “Notebook은 결국 Workspace 안에 저장된다”</p></blockquote><hr><h2 id="5-Repos-소스코드-관리"><a href="#5-Repos-소스코드-관리" class="headerlink" title="5. Repos (소스코드 관리)"></a>5. Repos (소스코드 관리)</h2><h3 id="기능"><a href="#기능" class="headerlink" title="기능"></a>기능</h3><ul><li>GitHub &#x2F; GitHub Enterprise</li><li>Azure DevOps</li><li>Bitbucket 등 연동 가능</li></ul><h3 id="활용"><a href="#활용" class="headerlink" title="활용"></a>활용</h3><ul><li>Git 기반 개발</li><li>Commit &#x2F; Pull &#x2F; Push 가능</li><li>협업 필수 기능</li></ul><blockquote><p>실무 팁<br>👉 개인 학습: Workspace<br>👉 팀&#x2F;프로젝트: <strong>Repos 필수</strong></p></blockquote><hr><h2 id="6-Catalog-Catalog-Explorer"><a href="#6-Catalog-Catalog-Explorer" class="headerlink" title="6. Catalog (Catalog Explorer)"></a>6. Catalog (Catalog Explorer)</h2><h3 id="역할-1"><a href="#역할-1" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>메타데이터 관리 화면</strong></p><p>표시되는 항목:</p><ul><li>Databases</li><li>Tables</li><li>Views</li><li>Functions</li></ul><blockquote><p>이후 학습 주제</p><ul><li>Unity Catalog</li><li>데이터 권한 관리</li></ul></blockquote><hr><h2 id="7-Workflow-Jobs"><a href="#7-Workflow-Jobs" class="headerlink" title="7. Workflow (Jobs)"></a>7. Workflow (Jobs)</h2><h3 id="역할-2"><a href="#역할-2" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>배치 작업 &amp; 파이프라인 관리</strong></p><p>가능한 작업:</p><ul><li>Job 생성</li><li>Task 간 의존성 설정</li><li>스케줄링 (Cron)</li><li>재시도 &#x2F; 실패 처리</li></ul><h3 id="하위-기능"><a href="#하위-기능" class="headerlink" title="하위 기능"></a>하위 기능</h3><ul><li>Jobs</li><li>Job Runs</li><li>Delta Live Tables (DLT)</li></ul><hr><h2 id="8-Compute-클러스터-관리"><a href="#8-Compute-클러스터-관리" class="headerlink" title="8. Compute (클러스터 관리)"></a>8. Compute (클러스터 관리)</h2><h3 id="역할-3"><a href="#역할-3" class="headerlink" title="역할"></a>역할</h3><p>👉 <strong>Spark 실행을 위한 컴퓨트 리소스 관리</strong></p><p>생성 가능한 리소스:</p><ul><li>All-purpose Cluster</li><li>Job Cluster</li><li>SQL Warehouse</li><li>Cluster Pool</li><li>Cluster Policy</li></ul><blockquote><p>핵심 메시지<br>👉 <strong>Azure Portal이 아니라 Databricks에서 클러스터 생성</strong></p></blockquote><hr><h2 id="9-Data-Ingestion"><a href="#9-Data-Ingestion" class="headerlink" title="9. Data Ingestion"></a>9. Data Ingestion</h2><h3 id="목적"><a href="#목적" class="headerlink" title="목적"></a>목적</h3><p>👉 외부 데이터를 Databricks로 가져오기</p><h3 id="방식"><a href="#방식" class="headerlink" title="방식"></a>방식</h3><ul><li>Native Spark Connectors</li><li>Partner Tools (Fivetran 등)</li></ul><blockquote><p>중요 포인트<br>👉 “Databricks는 처리 플랫폼, 수집은 도구 선택”</p></blockquote><hr><h2 id="10-Delta-Live-Tables-DLT"><a href="#10-Delta-Live-Tables-DLT" class="headerlink" title="10. Delta Live Tables (DLT)"></a>10. Delta Live Tables (DLT)</h2><h3 id="개념"><a href="#개념" class="headerlink" title="개념"></a>개념</h3><ul><li>Declarative ETL</li><li>파이프라인 정의 기반 처리</li></ul><h3 id="위치"><a href="#위치" class="headerlink" title="위치"></a>위치</h3><ul><li>Workflow 메뉴 하위</li></ul><blockquote><p>이후 심화 주제</p></blockquote><hr><h2 id="11-SQL-메뉴"><a href="#11-SQL-메뉴" class="headerlink" title="11. SQL 메뉴"></a>11. SQL 메뉴</h2><p>Databricks는 <strong>서버리스 데이터 웨어하우스</strong> 역할도 수행</p><p>가능한 작업:</p><ul><li>SQL Warehouse 생성</li><li>SQL Editor</li><li>Dashboard</li><li>Alert</li><li>Query History</li></ul><blockquote><p>SQL 중심 분석가 대상 기능</p></blockquote><hr><h2 id="12-Machine-Learning-메뉴"><a href="#12-Machine-Learning-메뉴" class="headerlink" title="12. Machine Learning 메뉴"></a>12. Machine Learning 메뉴</h2><p>ML 관련 기능 제공:</p><ul><li>Experiments</li><li>Models</li><li>Feature Store</li><li>MLflow</li></ul><blockquote><p>데이터 엔지니어 → ML 엔지니어 확장 포인트</p></blockquote><hr><h2 id="13-Marketplace"><a href="#13-Marketplace" class="headerlink" title="13. Marketplace"></a>13. Marketplace</h2><h3 id="기능-1"><a href="#기능-1" class="headerlink" title="기능"></a>기능</h3><ul><li>외부 데이터 구매&#x2F;구독</li><li>무료&#x2F;유료 데이터셋</li></ul><h3 id="기반-기술"><a href="#기반-기술" class="headerlink" title="기반 기술"></a>기반 기술</h3><ul><li>Delta Sharing</li></ul><hr><h2 id="14-Partner-Connect"><a href="#14-Partner-Connect" class="headerlink" title="14. Partner Connect"></a>14. Partner Connect</h2><h3 id="목적-1"><a href="#목적-1" class="headerlink" title="목적"></a>목적</h3><p>👉 외부 솔루션과 <strong>원클릭 연동</strong></p><p>파트너 예시:</p><ul><li>Data Ingestion</li><li>Visualization (Tableau 등)</li><li>Security</li><li>Governance</li><li>ML Tools</li></ul><hr><h2 id="15-우측-상단-메뉴"><a href="#15-우측-상단-메뉴" class="headerlink" title="15. 우측 상단 메뉴"></a>15. 우측 상단 메뉴</h2><h3 id="항목"><a href="#항목" class="headerlink" title="항목"></a>항목</h3><ul><li>User Settings</li><li>Admin Settings</li><li>Manage Account (Admin Console)</li><li>Logout</li></ul><blockquote><p>관리자는 Admin Settings 자주 사용</p></blockquote><hr><h2 id="16-강의용-핵심-요약-한-문장씩"><a href="#16-강의용-핵심-요약-한-문장씩" class="headerlink" title="16. 강의용 핵심 요약 (한 문장씩)"></a>16. 강의용 핵심 요약 (한 문장씩)</h2><ul><li>Workspace &#x3D; Databricks의 모든 작업 시작점</li><li>코드 저장 &#x3D; Workspace</li><li>실행 환경 &#x3D; Compute</li><li>자동화 &#x3D; Workflow</li><li>메타데이터 &#x3D; Catalog</li><li>SQL 분석 &#x3D; SQL 메뉴</li><li>ML &#x3D; Machine Learning 메뉴</li></ul><hr><h2 id="17-추천-강의-흐름"><a href="#17-추천-강의-흐름" class="headerlink" title="17. 추천 강의 흐름"></a>17. 추천 강의 흐름</h2><ol><li>Workspace UI 전체 구조 설명</li><li>Workspace → Notebook 생성</li><li>Compute → Cluster 생성</li><li>Notebook 실행</li><li>Workflow → Job 만들기</li></ol><hr><h2 id="마무리"><a href="#마무리" class="headerlink" title="마무리"></a>마무리</h2><p>Databricks Workspace는 단순한 UI가 아니라<br>👉 <strong>데이터 엔지니어링 작업의 컨트롤 타워</strong>입니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-Databricks-Workspace란&quot;&gt;&lt;a href=&quot;#1-Databricks-Workspace란&quot; class=&quot;headerlink&quot; title=&quot;1. Databricks Workspace란?&quot;&gt;&lt;/a&gt;1. Databricks W</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-3</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-3/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-3/</id>
    <published>2025-12-12T16:27:24.000Z</published>
    <updated>2025-12-12T16:35:20.737Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Azure-Free-Account-생성"><a href="#1-Azure-Free-Account-생성" class="headerlink" title="1. Azure Free Account 생성"></a>1. Azure Free Account 생성</h2><h3 id="Azure-Free-Account-혜택"><a href="#Azure-Free-Account-혜택" class="headerlink" title="Azure Free Account 혜택"></a>Azure Free Account 혜택</h3><ul><li>💳 <strong>$200 크레딧 (30일)</strong></li><li>🆓 인기 Azure 서비스 12개월 무료</li><li>🆓 40개 이상 Always Free 서비스</li></ul><blockquote><p>⚠️ 주의<br>$200 크레딧은 <strong>30일 후 소멸</strong><br>→ 실습은 가급적 한 달 내에 완료 권장</p></blockquote><h3 id="계정-생성-절차-요약"><a href="#계정-생성-절차-요약" class="headerlink" title="계정 생성 절차 요약"></a>계정 생성 절차 요약</h3><ol><li><a href="https://azure.microsoft.com/">https://azure.microsoft.com</a> 접속</li><li><strong>Start free</strong> 클릭</li><li>Microsoft 계정 로그인 또는 신규 생성</li><li>이메일 + 휴대폰 인증</li><li>주소 입력</li><li><strong>Visa &#x2F; MasterCard 카드 등록</strong><ul><li>소액 인증만 발생</li><li>자동 과금 ❌ (수동 업그레이드 전까지)</li></ul></li></ol><hr><h2 id="2-Azure-Portal-기본-사용법"><a href="#2-Azure-Portal-기본-사용법" class="headerlink" title="2. Azure Portal 기본 사용법"></a>2. Azure Portal 기본 사용법</h2><h3 id="Azure-Portal-접속"><a href="#Azure-Portal-접속" class="headerlink" title="Azure Portal 접속"></a>Azure Portal 접속</h3><ul><li><a href="https://portal.azure.com/">https://portal.azure.com</a></li></ul><h3 id="주요-구성-요소"><a href="#주요-구성-요소" class="headerlink" title="주요 구성 요소"></a>주요 구성 요소</h3><ul><li>🔍 <strong>Search Bar</strong>: 모든 Azure 서비스 검색</li><li>📁 <strong>Resource Group</strong>: 리소스 묶음 관리</li><li>💻 <strong>Cloud Shell</strong>: Browser 기반 Bash &#x2F; PowerShell</li><li>🔐 <strong>Azure Active Directory (Entra ID)</strong> 자동 생성</li></ul><blockquote><p>실무 팁<br>👉 서비스 이름을 알고 있으면 <strong>검색창 사용이 가장 빠름</strong></p></blockquote><hr><h2 id="3-Azure-Databricks-Workspace-생성"><a href="#3-Azure-Databricks-Workspace-생성" class="headerlink" title="3. Azure Databricks Workspace 생성"></a>3. Azure Databricks Workspace 생성</h2><h3 id="Workspace란"><a href="#Workspace란" class="headerlink" title="Workspace란?"></a>Workspace란?</h3><ul><li>Azure 위에서 실행되는 <strong>Databricks 전용 관리 단위</strong></li><li>하나의 프로젝트 &#x3D; 하나의 Workspace 권장</li></ul><h3 id="생성-절차"><a href="#생성-절차" class="headerlink" title="생성 절차"></a>생성 절차</h3><ol><li>Azure Portal → 검색창에 <strong>Azure Databricks</strong></li><li><strong>Create</strong> 클릭</li></ol><h3 id="기본-설정-중요"><a href="#기본-설정-중요" class="headerlink" title="기본 설정 (중요)"></a>기본 설정 (중요)</h3><table><thead><tr><th>항목</th><th>권장 값</th></tr></thead><tbody><tr><td>Subscription</td><td>Pay-as-you-go</td></tr><tr><td>Resource Group</td><td>프로젝트별 1개</td></tr><tr><td>Workspace Name</td><td>명확한 이름</td></tr><tr><td>Region</td><td>East US (저렴)</td></tr><tr><td>Pricing Tier</td><td><strong>Premium (권장)</strong></td></tr></tbody></table><blockquote><p>Premium 선택 이유</p><ul><li>Role Based Access Control (RBAC)</li><li>Unity Catalog 사용 가능</li><li>실무 필수 기능 포함</li></ul></blockquote><h3 id="생성-완료"><a href="#생성-완료" class="headerlink" title="생성 완료"></a>생성 완료</h3><ul><li>약 <strong>5~10분 소요</strong></li><li>완료 후 <strong>Launch Workspace</strong> 버튼 활성화</li></ul><hr><h2 id="4-Databricks-Workspace-접속-방법"><a href="#4-Databricks-Workspace-접속-방법" class="headerlink" title="4. Databricks Workspace 접속 방법"></a>4. Databricks Workspace 접속 방법</h2><h3 id="방법-1-Azure-Portal-경유"><a href="#방법-1-Azure-Portal-경유" class="headerlink" title="방법 1: Azure Portal 경유"></a>방법 1: Azure Portal 경유</h3><ul><li>Azure Portal → Workspace → <strong>Launch Workspace</strong></li></ul><h3 id="방법-2-Workspace-URL-직접-접속"><a href="#방법-2-Workspace-URL-직접-접속" class="headerlink" title="방법 2: Workspace URL 직접 접속"></a>방법 2: Workspace URL 직접 접속</h3><ul><li>Workspace URL 복사 → 브라우저 직접 접속</li><li>Azure SSO로 자동 로그인</li></ul><blockquote><p>✔️ 실무에서는 <strong>URL 직접 접속 방식</strong>이 가장 흔함</p></blockquote><hr><h2 id="5-Databricks-Workspace-기본-구성"><a href="#5-Databricks-Workspace-기본-구성" class="headerlink" title="5. Databricks Workspace 기본 구성"></a>5. Databricks Workspace 기본 구성</h2><h3 id="주요-메뉴"><a href="#주요-메뉴" class="headerlink" title="주요 메뉴"></a>주요 메뉴</h3><ul><li><strong>Workspace</strong>: Notebook &#x2F; Folder 관리</li><li><strong>Compute</strong>: Cluster 관리</li><li><strong>Catalog</strong>: Unity Catalog (Premium)</li><li><strong>Jobs</strong>: 배치 작업 스케줄링</li><li><strong>Repos</strong>: Git 연동</li><li><strong>Admin</strong>: 사용자 &#x2F; 권한 &#x2F; 정책 관리</li></ul><hr><h2 id="6-실무에서-가장-먼저-하는-작업-순서"><a href="#6-실무에서-가장-먼저-하는-작업-순서" class="headerlink" title="6. 실무에서 가장 먼저 하는 작업 순서"></a>6. 실무에서 가장 먼저 하는 작업 순서</h2><h3 id="1️⃣-Cluster-생성"><a href="#1️⃣-Cluster-생성" class="headerlink" title="1️⃣ Cluster 생성"></a>1️⃣ Cluster 생성</h3><ul><li>Compute → Create Cluster</li><li>Auto Termination 설정 (비용 절감 필수)</li><li>Small VM으로 시작 권장</li></ul><h3 id="2️⃣-Notebook-생성"><a href="#2️⃣-Notebook-생성" class="headerlink" title="2️⃣ Notebook 생성"></a>2️⃣ Notebook 생성</h3><ul><li>Workspace → Create → Notebook</li><li>Python &#x2F; SQL &#x2F; Scala 선택</li></ul><h3 id="3️⃣-데이터-연결"><a href="#3️⃣-데이터-연결" class="headerlink" title="3️⃣ 데이터 연결"></a>3️⃣ 데이터 연결</h3><ul><li>Azure Blob Storage &#x2F; ADLS Gen2</li><li>Managed Identity 또는 Access Connector 사용</li></ul><h3 id="4️⃣-Delta-Lake-사용"><a href="#4️⃣-Delta-Lake-사용" class="headerlink" title="4️⃣ Delta Lake 사용"></a>4️⃣ Delta Lake 사용</h3><ul><li>ACID 트랜잭션</li><li>Time Travel</li><li>Schema Enforcement</li></ul><hr><h2 id="7-비용-관리-실전-팁-⭐"><a href="#7-비용-관리-실전-팁-⭐" class="headerlink" title="7. 비용 관리 실전 팁 ⭐"></a>7. 비용 관리 실전 팁 ⭐</h2><ul><li>⏱ <strong>Auto Termination 필수</strong></li><li>❌ 사용 안 할 때 Cluster 즉시 종료</li><li>📦 Workspace는 유지 (비용 거의 없음)</li><li>🧹 실습 종료 후 Resource Group 삭제</li></ul><hr><h2 id="8-Azure-Databricks-실무-구조-요약"><a href="#8-Azure-Databricks-실무-구조-요약" class="headerlink" title="8. Azure + Databricks 실무 구조 요약"></a>8. Azure + Databricks 실무 구조 요약</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Azure Subscription</span><br><span class="line"> └── Resource Group</span><br><span class="line">     └── Databricks Workspace</span><br><span class="line">         ├── Clusters</span><br><span class="line">         ├── Notebooks</span><br><span class="line">         ├── Jobs</span><br><span class="line">         └── Unity Catalog</span><br></pre></td></tr></table></figure><hr><h2 id="9-추천-학습-흐름"><a href="#9-추천-학습-흐름" class="headerlink" title="9. 추천 학습 흐름"></a>9. 추천 학습 흐름</h2><ol><li>Azure Account 생성</li><li>Databricks Workspace 1개 생성</li><li>Cluster 생성</li><li>Notebook 실습</li><li>ADLS + Delta Lake 연동</li><li>Jobs &amp; Automation</li></ol><hr><h2 id="마무리"><a href="#마무리" class="headerlink" title="마무리"></a>마무리</h2><p>Azure Databricks는<br>👉 <strong>Azure 인프라 + Databricks 데이터 플랫폼</strong>의 결합입니다.</p><p>처음에는 Azure Portal → Workspace 생성이 핵심이고,<br>그 이후부터는 <strong>Databricks UI 중심으로 작업</strong>하게 됩니다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-Azure-Free-Account-생성&quot;&gt;&lt;a href=&quot;#1-Azure-Free-Account-생성&quot; class=&quot;headerlink&quot; title=&quot;1. Azure Free Account 생성&quot;&gt;&lt;/a&gt;1. Azure Free Ac</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-2</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-2/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-2/</id>
    <published>2025-12-12T15:17:42.000Z</published>
    <updated>2025-12-12T15:18:14.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Databricks-핵심-기능-정리"><a href="#Databricks-핵심-기능-정리" class="headerlink" title="Databricks 핵심 기능 정리"></a>Databricks 핵심 기능 정리</h1><h2 id="1-Databricks란-무엇인가"><a href="#1-Databricks란-무엇인가" class="headerlink" title="1. Databricks란 무엇인가?"></a>1. Databricks란 무엇인가?</h2><p><strong>Databricks</strong>는 Apache Spark 위에 구축된 <strong>엔터프라이즈급 Lakehouse 플랫폼</strong>이다.<br>단순한 Spark 실행 환경이 아니라, <strong>설계 · 개발 · 운영 · 보안 · 자동화</strong>까지 포함한 <strong>완성형 데이터 플랫폼</strong>이다.</p><ul><li>Data Lake + Data Warehouse &#x3D; <strong>Lakehouse</strong></li><li>Medallion Architecture (Bronze &#x2F; Silver &#x2F; Gold) 지원</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Databricks &#x3D; Spark 기반 <strong>플랫폼</strong></li><li>Spark 단독으로 부족한 기능을 보완</li></ul><hr><h2 id="2-Databricks의-핵심-가치"><a href="#2-Databricks의-핵심-가치" class="headerlink" title="2. Databricks의 핵심 가치"></a>2. Databricks의 핵심 가치</h2><p>Databricks는 아래 질문에 대한 해답이다.</p><blockquote><p>“Spark로 엔터프라이즈 데이터 플랫폼을 만들려면 무엇이 더 필요한가?”</p></blockquote><p>정답:</p><ul><li>ACID</li><li>Metadata 관리</li><li>보안</li><li>클러스터 운영</li><li>성능 최적화</li><li>자동화</li></ul><hr><h2 id="3-Databricks-주요-기능-시험-핵심-⭐⭐⭐"><a href="#3-Databricks-주요-기능-시험-핵심-⭐⭐⭐" class="headerlink" title="3. Databricks 주요 기능 (시험 핵심 ⭐⭐⭐)"></a>3. Databricks 주요 기능 (시험 핵심 ⭐⭐⭐)</h2><h3 id="3-1-Cloud-Native-Spark"><a href="#3-1-Cloud-Native-Spark" class="headerlink" title="3.1 Cloud-Native Spark"></a>3.1 Cloud-Native Spark</h3><ul><li>Spark를 <strong>Hadoop(YARN) 의존 없이</strong> 실행</li><li>Cloud 환경에 최적화된 Spark 런타임 제공</li></ul><p>📌 시험 포인트</p><ul><li>Databricks Spark &#x3D; Hadoop 기반 ❌</li><li>Databricks Spark &#x3D; Cloud Native ⭕</li></ul><hr><h3 id="3-2-Secure-Cloud-Storage-Integration"><a href="#3-2-Secure-Cloud-Storage-Integration" class="headerlink" title="3.2 Secure Cloud Storage Integration"></a>3.2 Secure Cloud Storage Integration</h3><ul><li>Cloud Storage와 <strong>네이티브 통합</strong><ul><li>AWS: S3</li><li>Azure: ADLS Gen2</li><li>GCP: GCS</li></ul></li><li>IAM &#x2F; Role 기반 보안 연동</li></ul><p>📌 시험 포인트</p><ul><li>Databricks는 <strong>스토리지를 소유하지 않음</strong></li><li>Storage는 Cloud Native</li></ul><hr><h3 id="3-3-ACID-트랜잭션-–-Delta-Lake-⭐⭐⭐"><a href="#3-3-ACID-트랜잭션-–-Delta-Lake-⭐⭐⭐" class="headerlink" title="3.3 ACID 트랜잭션 – Delta Lake ⭐⭐⭐"></a>3.3 ACID 트랜잭션 – Delta Lake ⭐⭐⭐</h3><ul><li><strong>Delta Lake</strong> &#x3D; 오픈소스 스토리지 레이어</li><li>Spark + Delta Lake → ACID 보장</li></ul><p>지원 기능:</p><ul><li>Atomicity</li><li>Consistency</li><li>Isolation</li><li>Durability</li><li>Time Travel</li><li>Schema Enforcement &#x2F; Evolution</li></ul><p>📌 시험 포인트</p><ul><li>ACID는 Spark 기본 기능 ❌</li><li>ACID는 <strong>Delta Lake 통해 제공</strong> ⭕</li></ul><hr><h3 id="3-4-Unity-Catalog-Metadata-Security-⭐⭐⭐"><a href="#3-4-Unity-Catalog-Metadata-Security-⭐⭐⭐" class="headerlink" title="3.4 Unity Catalog (Metadata + Security) ⭐⭐⭐"></a>3.4 Unity Catalog (Metadata + Security) ⭐⭐⭐</h3><p><strong>Unity Catalog</strong>는 Databricks의 핵심 엔터프라이즈 기능</p><p>기능:</p><ul><li>중앙 메타데이터 관리</li><li>테이블 &#x2F; 뷰 &#x2F; 컬럼 단위 권한</li><li>사용자 &#x2F; 그룹 관리</li><li>감사 로그 (Audit)</li></ul><p>📌 시험 포인트</p><ul><li>Unity Catalog &#x3D; Metadata + Governance + Security</li></ul><hr><h3 id="3-5-Cluster-Management"><a href="#3-5-Cluster-Management" class="headerlink" title="3.5 Cluster Management"></a>3.5 Cluster Management</h3><ul><li>Databricks UI에서 직접:<ul><li>클러스터 생성 &#x2F; 삭제</li><li>Auto Scaling</li><li>Job Cluster &#x2F; All-Purpose Cluster</li></ul></li></ul><p>📌 시험 포인트</p><ul><li>Spark 자체는 클러스터 관리 ❌</li><li>Databricks는 클러스터 관리 ⭕</li></ul><hr><h3 id="3-6-Photon-Query-Engine-⭐⭐⭐"><a href="#3-6-Photon-Query-Engine-⭐⭐⭐" class="headerlink" title="3.6 Photon Query Engine ⭐⭐⭐"></a>3.6 Photon Query Engine ⭐⭐⭐</h3><ul><li>Databricks 전용 <strong>Query Acceleration Engine</strong></li><li>Spark SQL &#x2F; DataFrame 자동 가속</li><li>설정 ON&#x2F;OFF만으로 사용 가능</li></ul><p>📌 시험 포인트</p><ul><li>Photon &#x3D; Transparent Acceleration</li><li>SQL 성능 관련 문제 단골</li></ul><hr><h3 id="3-7-Notebooks-Workspace"><a href="#3-7-Notebooks-Workspace" class="headerlink" title="3.7 Notebooks &amp; Workspace"></a>3.7 Notebooks &amp; Workspace</h3><ul><li>통합 개발 환경 (IDE)</li><li>언어 지원:<ul><li>SQL</li><li>Python</li><li>Scala</li><li>R</li></ul></li><li>Git 연동 지원</li></ul><p>📌 시험 포인트</p><ul><li>Databricks Notebook &#x3D; 협업 중심</li></ul><hr><h3 id="3-8-Administration-Security-Controls"><a href="#3-8-Administration-Security-Controls" class="headerlink" title="3.8 Administration &amp; Security Controls"></a>3.8 Administration &amp; Security Controls</h3><ul><li>Role-Based Access Control (RBAC)</li><li>정책 기반 접근 제어</li><li>Audit Log</li></ul><p>📌 시험 포인트</p><ul><li>엔터프라이즈 보안 &#x3D; Databricks 강점</li></ul><hr><h3 id="3-9-Optimized-Spark-Runtime"><a href="#3-9-Optimized-Spark-Runtime" class="headerlink" title="3.9 Optimized Spark Runtime"></a>3.9 Optimized Spark Runtime</h3><ul><li>Vanilla Spark 대비:<ul><li>성능 최적화</li><li>메모리 &#x2F; 쿼리 튜닝</li></ul></li><li>동일 코드 → Databricks에서 더 빠름</li></ul><p>📌 시험 포인트</p><ul><li>Databricks Runtime ≠ Apache Spark OSS</li></ul><hr><h3 id="3-10-Automation-Tools"><a href="#3-10-Automation-Tools" class="headerlink" title="3.10 Automation Tools"></a>3.10 Automation Tools</h3><ul><li>REST API</li><li>CLI</li><li>SDK</li><li>Terraform 연동</li></ul><p>📌 시험 포인트</p><ul><li>Databricks &#x3D; DevOps 친화적</li></ul><hr><h2 id="4-Databricks-지원-Cloud-플랫폼"><a href="#4-Databricks-지원-Cloud-플랫폼" class="headerlink" title="4. Databricks 지원 Cloud 플랫폼"></a>4. Databricks 지원 Cloud 플랫폼</h2><p>Databricks는 <strong>동일한 기능</strong>을 모든 Cloud에서 제공</p><table><thead><tr><th>Cloud</th><th>Databricks 제공</th></tr></thead><tbody><tr><td>AWS</td><td>Databricks on AWS</td></tr><tr><td>Azure</td><td>Azure Databricks</td></tr><tr><td>GCP</td><td>Databricks on GCP</td></tr></tbody></table><p>📌 시험 포인트</p><ul><li>Databricks 기능은 <strong>Cloud 간 동일</strong></li><li>선택 기준은 조직의 Cloud 전략</li></ul><hr><h2 id="5-Cloud별-주요-연동-서비스-시험-단골"><a href="#5-Cloud별-주요-연동-서비스-시험-단골" class="headerlink" title="5. Cloud별 주요 연동 서비스 (시험 단골)"></a>5. Cloud별 주요 연동 서비스 (시험 단골)</h2><h3 id="CI-CD"><a href="#CI-CD" class="headerlink" title="CI&#x2F;CD"></a>CI&#x2F;CD</h3><ul><li>Azure: Azure DevOps, GitHub</li><li>AWS: CodeBuild, CodePipeline</li><li>GCP: Cloud Build</li></ul><h3 id="Data-Warehouse"><a href="#Data-Warehouse" class="headerlink" title="Data Warehouse"></a>Data Warehouse</h3><ul><li>Azure: Synapse</li><li>AWS: Redshift</li><li>GCP: BigQuery</li></ul><h3 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h3><ul><li>Azure: Event Hubs</li><li>AWS: Kinesis</li><li>GCP: Pub&#x2F;Sub</li></ul><p>📌 시험 포인트</p><ul><li>Databricks는 Cloud 생태계와 강하게 통합됨</li></ul><hr><h2 id="6-Spark-vs-Databricks-시험-비교-문제"><a href="#6-Spark-vs-Databricks-시험-비교-문제" class="headerlink" title="6. Spark vs Databricks (시험 비교 문제)"></a>6. Spark vs Databricks (시험 비교 문제)</h2><table><thead><tr><th>구분</th><th>Spark</th><th>Databricks</th></tr></thead><tbody><tr><td>ACID</td><td>❌</td><td>⭕ (Delta)</td></tr><tr><td>Metadata</td><td>제한적</td><td>Unity Catalog</td></tr><tr><td>Cluster 관리</td><td>❌</td><td>⭕</td></tr><tr><td>성능</td><td>기본</td><td>Photon</td></tr><tr><td>Automation</td><td>제한적</td><td>풍부</td></tr><tr><td>Enterprise Ready</td><td>❌</td><td>⭕</td></tr></tbody></table><hr><h2 id="7-시험-한-줄-요약-⭐⭐⭐"><a href="#7-시험-한-줄-요약-⭐⭐⭐" class="headerlink" title="7. 시험 한 줄 요약 ⭐⭐⭐"></a>7. 시험 한 줄 요약 ⭐⭐⭐</h2><ul><li>Databricks &#x3D; Spark + Delta Lake + Unity Catalog</li><li>ACID는 Delta Lake</li><li>Governance는 Unity Catalog</li><li>성능은 Photon</li><li>Databricks는 <strong>플랫폼</strong>, Spark는 <strong>엔진</strong></li></ul><hr><h3 id="✅-최종-암기-문장"><a href="#✅-최종-암기-문장" class="headerlink" title="✅ 최종 암기 문장"></a>✅ 최종 암기 문장</h3><blockquote><p>Databricks는 Apache Spark 기반의 Cloud Native Lakehouse 플랫폼으로,<br>Delta Lake를 통해 ACID를 제공하고 Unity Catalog를 통해 메타데이터와 보안을 관리한다.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Databricks-핵심-기능-정리&quot;&gt;&lt;a href=&quot;#Databricks-핵심-기능-정리&quot; class=&quot;headerlink&quot; title=&quot;Databricks 핵심 기능 정리&quot;&gt;&lt;/a&gt;Databricks 핵심 기능 정리&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>DATABRICKS-Fundamentals-1</title>
    <link href="https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-1/"/>
    <id>https://kish191919.github.io/2025/12/12/DATABRICKS-Fundamentals-1/</id>
    <published>2025-12-12T14:54:44.000Z</published>
    <updated>2025-12-12T15:01:56.698Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Apache-Spark란"><a href="#1-Apache-Spark란" class="headerlink" title="1. Apache Spark란?"></a>1. Apache Spark란?</h2><p><strong>Apache Spark</strong>는<br>👉 <strong>분산 클러스터 환경에서 대규모 데이터를 빠르게 처리하기 위한 데이터 처리 엔진(Engine)</strong> 입니다.</p><p>Spark는 다음 작업을 <strong>하나의 통합된 프레임워크</strong>에서 처리할 수 있습니다.</p><ul><li>배치 데이터 처리 (Batch Processing)</li><li>스트리밍 데이터 처리 (Stream Processing)</li><li>머신러닝 (Machine Learning)</li><li>그래프 처리 (Graph Processing)</li><li>SQL 기반 데이터 분석</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark는 <strong>데이터베이스가 아니다</strong></li><li>Spark는 <strong>스토리지 시스템이 아니다</strong></li><li>Spark는 <strong>데이터 처리 엔진(Processing Engine)</strong> 이다</li></ul><hr><h2 id="2-Spark가-제공하는-주요-API-Unified-Framework"><a href="#2-Spark가-제공하는-주요-API-Unified-Framework" class="headerlink" title="2. Spark가 제공하는 주요 API (Unified Framework)"></a>2. Spark가 제공하는 주요 API (Unified Framework)</h2><p>Spark는 하나의 엔진 위에서 여러 API를 제공합니다.</p><h3 id="1-Spark-SQL-DataFrame-API"><a href="#1-Spark-SQL-DataFrame-API" class="headerlink" title="(1) Spark SQL &amp; DataFrame API"></a>(1) Spark SQL &amp; DataFrame API</h3><ul><li>SQL 기반 데이터 처리</li><li>ANSI SQL 호환</li><li>가장 많이 사용됨 ⭐⭐⭐</li></ul><h3 id="2-Structured-Streaming"><a href="#2-Structured-Streaming" class="headerlink" title="(2) Structured Streaming"></a>(2) Structured Streaming</h3><ul><li>스트리밍 데이터를 <strong>배치처럼 처리</strong></li><li>Kafka, Kinesis 등과 연동</li></ul><h3 id="3-MLlib"><a href="#3-MLlib" class="headerlink" title="(3) MLlib"></a>(3) MLlib</h3><ul><li>머신러닝 라이브러리</li><li>분류, 회귀, 클러스터링 등 제공</li></ul><h3 id="4-GraphX"><a href="#4-GraphX" class="headerlink" title="(4) GraphX"></a>(4) GraphX</h3><ul><li>그래프 기반 연산 (노드, 엣지)</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>RDD API는 <strong>존재하지만 권장되지 않음</strong></li><li>시험에서는 <strong>DataFrame &#x2F; Spark SQL 중심</strong></li></ul><hr><h2 id="3-Spark-아키텍처-Spark-Stack"><a href="#3-Spark-아키텍처-Spark-Stack" class="headerlink" title="3. Spark 아키텍처 (Spark Stack)"></a>3. Spark 아키텍처 (Spark Stack)</h2><h3 id="Spark-동작-구조-아래-→-위"><a href="#Spark-동작-구조-아래-→-위" class="headerlink" title="Spark 동작 구조 (아래 → 위)"></a>Spark 동작 구조 (아래 → 위)</h3><ol><li><p><strong>Distributed Storage</strong></p><ul><li>HDFS</li><li>Amazon S3</li><li>Azure Data Lake Storage (ADLS)</li><li>Google Cloud Storage (GCS)</li></ul></li><li><p><strong>Compute Cluster</strong></p><ul><li>여러 대의 서버로 구성된 클러스터</li></ul></li><li><p><strong>Resource Manager (Cluster Manager)</strong></p><ul><li>YARN</li><li>Kubernetes</li><li>Standalone</li><li>Mesos (과거)</li></ul></li><li><p><strong>Spark Framework</strong></p><ul><li>Spark Core</li><li>Spark SQL</li><li>Streaming</li><li>MLlib</li><li>GraphX</li></ul></li><li><p><strong>Programming API &#x2F; DSL</strong></p><ul><li>Scala</li><li>Java</li><li>Python (PySpark)</li><li>R</li></ul></li></ol><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark는 <strong>반드시 Cluster Manager 위에서 실행</strong></li><li>Spark는 <strong>스토리지와 분리된 구조</strong></li></ul><hr><h2 id="4-Spark-Core-언어-지원"><a href="#4-Spark-Core-언어-지원" class="headerlink" title="4. Spark Core &amp; 언어 지원"></a>4. Spark Core &amp; 언어 지원</h2><h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><ul><li>Spark의 핵심 실행 엔진</li><li>RDD 기반 API 포함</li></ul><h3 id="지원-언어"><a href="#지원-언어" class="headerlink" title="지원 언어"></a>지원 언어</h3><ul><li>Scala (Spark의 원래 언어)</li><li>Java</li><li>Python (PySpark) ⭐</li><li>R</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark Core &#x3D; RDD API</li><li>실무 &amp; 시험에서는 <strong>DataFrame API 사용</strong></li></ul><hr><h2 id="5-Spark가-인기-있는-이유"><a href="#5-Spark가-인기-있는-이유" class="headerlink" title="5. Spark가 인기 있는 이유"></a>5. Spark가 인기 있는 이유</h2><h3 id="1-높은-추상화-High-Abstraction"><a href="#1-높은-추상화-High-Abstraction" class="headerlink" title="(1) 높은 추상화 (High Abstraction)"></a>(1) 높은 추상화 (High Abstraction)</h3><ul><li>분산 처리 복잡성 숨김</li><li>개발자는 SQL 또는 DataFrame만 작성</li></ul><h3 id="2-사용하기-쉬움"><a href="#2-사용하기-쉬움" class="headerlink" title="(2) 사용하기 쉬움"></a>(2) 사용하기 쉬움</h3><ul><li>SQL 기반 접근 가능</li><li>다양한 언어 지원</li></ul><h3 id="3-Unified-Platform"><a href="#3-Unified-Platform" class="headerlink" title="(3) Unified Platform"></a>(3) Unified Platform</h3><ul><li>SQL + Batch + Streaming + ML + Graph</li><li>하나의 엔진에서 모두 처리</li></ul><h3 id="4-Open-Source-풍부한-생태계"><a href="#4-Open-Source-풍부한-생태계" class="headerlink" title="(4) Open Source &amp; 풍부한 생태계"></a>(4) Open Source &amp; 풍부한 생태계</h3><ul><li>수많은 기업 사용</li><li>Fortune 500의 약 80% 사용</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark의 핵심 장점 &#x3D; <strong>Unified + Abstraction</strong></li></ul><hr><h2 id="6-Apache-Spark가-“아닌-것”-중요-⭐⭐⭐"><a href="#6-Apache-Spark가-“아닌-것”-중요-⭐⭐⭐" class="headerlink" title="6. Apache Spark가 “아닌 것” (중요 ⭐⭐⭐)"></a>6. Apache Spark가 “아닌 것” (중요 ⭐⭐⭐)</h2><p>Spark는 강력하지만 <strong>엔터프라이즈 솔루션으로는 부족한 부분</strong>이 있음</p><h3 id="1-자체-스토리지-❌"><a href="#1-자체-스토리지-❌" class="headerlink" title="(1) 자체 스토리지 ❌"></a>(1) 자체 스토리지 ❌</h3><ul><li>Spark는 데이터를 저장하지 않음</li><li>항상 외부 스토리지 필요 (S3, HDFS 등)</li></ul><h3 id="2-ACID-트랜잭션-❌"><a href="#2-ACID-트랜잭션-❌" class="headerlink" title="(2) ACID 트랜잭션 ❌"></a>(2) ACID 트랜잭션 ❌</h3><ul><li>Spark 자체는 ACID 보장 안 함</li><li>Atomicity, Consistency, Isolation, Durability 미지원</li></ul><h3 id="3-중앙-메타데이터-카탈로그-❌"><a href="#3-중앙-메타데이터-카탈로그-❌" class="headerlink" title="(3) 중앙 메타데이터 카탈로그 ❌"></a>(3) 중앙 메타데이터 카탈로그 ❌</h3><ul><li>단순한 내부 카탈로그만 존재</li><li>Enterprise-grade Catalog 없음</li></ul><h3 id="4-클러스터-관리-❌"><a href="#4-클러스터-관리-❌" class="headerlink" title="(4) 클러스터 관리 ❌"></a>(4) 클러스터 관리 ❌</h3><ul><li>Spark로 클러스터 생성&#x2F;삭제 불가</li><li>Cluster Manager의 역할</li></ul><h3 id="5-자동화-도구-부족-❌"><a href="#5-자동화-도구-부족-❌" class="headerlink" title="(5) 자동화 도구 부족 ❌"></a>(5) 자동화 도구 부족 ❌</h3><ul><li>배포, 모니터링, 운영 자동화 기능 미흡</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Spark는 <strong>엔진이지 플랫폼이 아니다</strong></li></ul><hr><h2 id="7-왜-Spark-“플랫폼”이-필요한가"><a href="#7-왜-Spark-“플랫폼”이-필요한가" class="headerlink" title="7. 왜 Spark “플랫폼”이 필요한가?"></a>7. 왜 Spark “플랫폼”이 필요한가?</h2><p>엔터프라이즈 환경에서는 다음이 필요함:</p><ul><li>ACID 보장</li><li>메타데이터 관리</li><li>보안</li><li>자동화</li><li>운영 편의성</li></ul><p>👉 그래서 <strong>Spark + Platform</strong> 조합이 필요</p><hr><h2 id="8-대표적인-Spark-플랫폼들-시험-단골"><a href="#8-대표적인-Spark-플랫폼들-시험-단골" class="headerlink" title="8. 대표적인 Spark 플랫폼들 (시험 단골)"></a>8. 대표적인 Spark 플랫폼들 (시험 단골)</h2><h3 id="1-Cloudera-Hadoop"><a href="#1-Cloudera-Hadoop" class="headerlink" title="(1) Cloudera Hadoop"></a>(1) Cloudera Hadoop</h3><ul><li>온프레미스 Hadoop 플랫폼</li><li>YARN 기반</li><li>Spark 실행 가능</li></ul><h3 id="2-Amazon-EMR"><a href="#2-Amazon-EMR" class="headerlink" title="(2) Amazon EMR"></a>(2) Amazon EMR</h3><ul><li>AWS 관리형 Hadoop&#x2F;Spark</li><li>내부적으로 Hadoop + YARN 사용</li></ul><h3 id="3-Azure-HDInsight"><a href="#3-Azure-HDInsight" class="headerlink" title="(3) Azure HDInsight"></a>(3) Azure HDInsight</h3><ul><li>Azure 기반 Hadoop&#x2F;Spark 서비스</li></ul><h3 id="4-Google-Dataproc"><a href="#4-Google-Dataproc" class="headerlink" title="(4) Google Dataproc"></a>(4) Google Dataproc</h3><ul><li>GCP 기반 Hadoop&#x2F;Spark 서비스</li></ul><p>📌 공통점</p><ul><li><strong>모두 Hadoop 기반</strong></li><li><strong>YARN 사용</strong></li></ul><hr><h2 id="9-Databricks의-차별점-⭐⭐⭐"><a href="#9-Databricks의-차별점-⭐⭐⭐" class="headerlink" title="9. Databricks의 차별점 ⭐⭐⭐"></a>9. Databricks의 차별점 ⭐⭐⭐</h2><h3 id="Databricks-특징"><a href="#Databricks-특징" class="headerlink" title="Databricks 특징"></a>Databricks 특징</h3><ul><li><strong>Hadoop 기반 아님</strong></li><li><strong>YARN 사용 안 함</strong></li><li>Spark 전용 Cloud Native 플랫폼</li><li>클라우드 최적화</li></ul><h3 id="Databricks는"><a href="#Databricks는" class="headerlink" title="Databricks는?"></a>Databricks는?</h3><ul><li>Spark + ACID + Metadata + Automation 제공</li><li>Delta Lake 기반</li><li>Medallion Architecture 구현 가능</li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Databricks &#x3D; <strong>Pure Spark Platform</strong></li><li>On-Premise ❌, Cloud Only ⭕</li></ul><hr><h2 id="10-시험에-자주-나오는-한-줄-요약"><a href="#10-시험에-자주-나오는-한-줄-요약" class="headerlink" title="10. 시험에 자주 나오는 한 줄 요약"></a>10. 시험에 자주 나오는 한 줄 요약</h2><ul><li>Spark는 <strong>데이터 처리 엔진</strong></li><li>Spark는 <strong>스토리지를 포함하지 않는다</strong></li><li>Spark는 <strong>ACID를 기본 제공하지 않는다</strong></li><li>Spark는 <strong>YARN &#x2F; Kubernetes 위에서 실행</strong></li><li>Databricks는 <strong>Spark 기반 엔터프라이즈 플랫폼</strong></li><li>Hadoop 기반 플랫폼 ≠ Databricks</li></ul><hr><h2 id="11-Medallion-Architecture와-Spark-보너스"><a href="#11-Medallion-Architecture와-Spark-보너스" class="headerlink" title="11. Medallion Architecture와 Spark (보너스)"></a>11. Medallion Architecture와 Spark (보너스)</h2><ul><li>Spark 단독 ❌</li><li>Spark + Delta Lake ⭕</li><li>Databricks에서 Medallion Architecture 구현 가능<ul><li>Bronze</li><li>Silver</li><li>Gold</li></ul></li></ul><p>📌 <strong>시험 포인트</strong></p><ul><li>Medallion Architecture &#x3D; Databricks + Delta Lake</li></ul><hr><h3 id="✅-마무리-한-문장-시험용"><a href="#✅-마무리-한-문장-시험용" class="headerlink" title="✅ 마무리 한 문장 (시험용)"></a>✅ 마무리 한 문장 (시험용)</h3><blockquote><p>Apache Spark는 분산 환경에서 대규모 데이터를 처리하기 위한 <strong>Unified Data Processing Engine</strong>이며,<br>엔터프라이즈 환경에서는 Databricks 같은 <strong>플랫폼과 함께 사용된다</strong>.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-Apache-Spark란&quot;&gt;&lt;a href=&quot;#1-Apache-Spark란&quot; class=&quot;headerlink&quot; title=&quot;1. Apache Spark란?&quot;&gt;&lt;/a&gt;1. Apache Spark란?&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Apache</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/categories/CERTIFICATION/DATABRICKS-FUNDAMENTALS/"/>
    
    
    <category term="DATABRICKS" scheme="https://kish191919.github.io/tags/DATABRICKS/"/>
    
    <category term="DATABRICKS_FUNDAMENTALS" scheme="https://kish191919.github.io/tags/DATABRICKS-FUNDAMENTALS/"/>
    
  </entry>
  
  <entry>
    <title>Interview_Q&amp;A_1</title>
    <link href="https://kish191919.github.io/2025/12/12/Interview-Q-A-1/"/>
    <id>https://kish191919.github.io/2025/12/12/Interview-Q-A-1/</id>
    <published>2025-12-12T14:51:06.000Z</published>
    <updated>2025-12-12T14:53:32.603Z</updated>
    
    <content type="html"><![CDATA[<p><a id="top"></a></p><style>.toc-grid{  display:grid;  grid-template-columns:1fr 1fr; /* 화면 반반 */  gap:6px 20px;  align-items:start;}@media (max-width:760px){  .toc-grid{ grid-template-columns:1fr; }}/* 섹션 타이틀은 두 칼럼 전체를 차지 */.toc-section{  grid-column:1 / -1;  margin:12px 0 6px;  font-size:1.1rem;  font-weight:700;}/* 링크 공통 스타일 */.toc-grid a{ display:block; padding:2px 0; word-break:keep-all; }</style><div class="toc-grid"><h3 class="toc-section">내 소개 및 전반적인 질문</h3><a href="#a1">나의 소개</a><a href="#a2">왜 이직하니?</a><a href="#a4">보잉이란 그리고 지원동기?</a><a href="#recent-project">최근 프로젝트</a><a href="#issue-data-volume">이슈 - 데이터 볼륨증가 3~4TB</a><a href="#issue-schema-change">이슈 - 스키마 변경</a><a href="#data-warehouse-tech">데이터 웨어하우스 설계</a><a href="#data-warehouse-problem">데이터 웨어하우스 지연속도 문제 및 해결</a><a href="#dq-multi-point-good">데이터 품질관리 - 여러포인트 검증 &amp; good data</a><a href="#etl-design-ca7-glue">ETL (오케스트레이션) 설계 - CA7와 Glue 이용</a><h3 class="toc-section">데이터 품질과 보안</h3><a href="#dq-unit-test">데이터 품질관리 - 유닛테스트</a><a href="#security-compliance">보안과 규정 (e.g. AWS, Azure)</a><h3 class="toc-section">기술 내용</h3><a href="#alteryx-usage">Alteryx 사용기간 및 경험</a><a href="#teradata-usage">테라데이터 사용기간 및 경험</a><a href="#teradata-definition">테라데이터 정의 및 사용사례</a><a href="#teradata-performance">테라데이터가 높은 성능을 발휘할수 있는 이유</a><a href="#teradata-pi">테라데이터의 Primary Index (PI) 역할</a><a href="#teradata-skew">테라데이터 데이터 불균형을 어떻게 해결?</a><a href="#teradata-secondary-index">테라데이터의 Secondary Index 이란</a><a href="#teradata-partition">Teradata 파티션</a><a href="#teradata-troubleshooting">테라데이터 문제해결</a><a href="#alteryx-teradata">Alteryx와 Teradata 사용</a><a href="#neo4j">Neo4J 관련해서</a><h3 class="toc-section">행동 규정</h3><a href="#manager-absence-decision">매니저가 부재시 결정해야할 경우</a><a href="#team-disagreement-sla">팀원 의견 다름 - SLA 20분 지연 - 품질해결</a><a href="#manager-disagreement-refresh-partition">상사 의견 다름 - refresh only 변경된 파티션만</a><a href="#diffent-personality">다른 성향의 사람과 협동</a><a href="#helped-teammate">팀동료 성공시키기</a><a href="#urgent-vs-important">급한일과 중요한일의 우선순위 - 급한것 먼저</a><a href="#tech-challenge-1-5tb">기술 문제 도전 - 1.5TB 처리</a><a href="#improvement-large-data">개선 사례 - 큰 데이터 처리 (위 동일)</a><a href="#production-connect-stop">생산 문제 - 커넥트 Stop</a><a href="#process-improvement-connect-stop">자발적 프로세스 개선 - 커넥트 Stop</a><a href="#kafka-realtime-connect-check">Kafka 실시간 데이터에서 고려할 부분 - 커넥트 상태확인</a><a href="#quality-issue-currency">품질 문제 - 통화단위 에러</a><a href="#failure-column-validation">실패,실수 - 컬럼검증X, 통화단위 에러</a><a href="#project-delay-schema-change">프로젝트 지연 - 스키마 변경</a><a href="#team-lead-source-missing">팀 리드 &amp; 솔선 - 소스 입력 안됨</a><a href="#team-lead-datatype-mismatch">리더싶 사례 - data type mismatch</a><a href="#above-and-beyond-responsibility">지시받지않은 일 - add load_date / 상사 의견다름 동일</a><a href="#tight-schedule-pressure">타이트한 스케줄 &amp; 압박 - 일 나누고 대화, 트랙</a><a href="#multi-national-collaboration">여러 민족 같이 근무 - sync 미팅, 미팅요약</a><a href="#cross-team-collaboration">다른 팀 협업 - 용어 통일</a><a href="#customer-request">고객이 마지막에 변경요청시 - 스키마변경</a><a href="#customer-request-offen">고객이 자주 변경사항을 요청할때 - 요청을 그룹핑함</a><a href="#non-technical">비개발자에게 기술적인 내용을 쉽게 설명할 수 있나요?</a><a href="#issue-spark-memory">이슈 - Spark memory 문제</a><h3 class="toc-section">ETL / 오케스트레이션</h3><a href="#data-orchestration">Data Orchestration - (CA7, Glue, Airflow)</a><a href="#etl-pipeline-optimization">ETL pipeline 최적화 - SLA 6시</a><a href="#aws-glue-experience">AWS Glue 사용경험 - ETL services</a><a href="#no-current-glue">현재 glue를 사용하지는 않는다</a><h3 class="toc-section">데이터 웨어하우스</h3><a href="#redshift-intro">Redshift 란?</a><a href="#redshift-columnar">Redshift Columnar Storage</a><a href="#snowflake-advantages">Snowflake 장점 (zero-copy, time travel)</a><a href="#databricks-experience">Databricks 사용경험 - Anomaly detection</a><a href="#partition-strategy">파티션 전략</a><a href="#oracle-range-partitioning">데이터 모델링 & Architecture - 오라클 range partitioning<</a><a href="#data-normalization">정규화 vs 비정규화</a><a href="#star-snowflake-schema">Star &amp; Snowflake 스키마</a><h3 class="toc-section">파이썬/스파크/하둡</h3><a href="#python-sql-spark">Python, SQL, Spark, PySpark</a><a href="#spark-hadoop-ingestion">경험 - 스파크/하둡 ingestion</a><a href="#aws-experience">경험 - AWS 많이 사용했니?</a><a href="#strengths-weaknesses">장점과 단점</a><a href="#stress">스트레스를 어떻게 풉니까?</a><a href="#motto">삶의 모토는?</a><a href="#powertech">파워텍에서 머신러닝 모델 사용</a><a href="#final-remarks">마지막으로 하고 싶은 말</a><a href="#python-typ">파이썬 타입비교</a></div><h3 id="a1">나의 소개</h3>Thank you for having me for an interview and my name is Sunghwan ki but you can go by DannyI work as Data Engineer with 6 years experience in building ETL process, especially in the financial industry.Currently I lead the projects that use the Kafka, Oracle, and Spark where I focus on near real-time data processing and optimization.I primarily use Python to build data pipelines, and recently, I completed on a project where I built a data warehouse using AWS Glue and Redshift.Before joining PNC, I spent roughly seven years working in data analytics, where I primarily used Tableau and MySql to analyze the data<p>To better performance, I completed the Master’s degree in Data Science last year and also I hold the AWS certifications and continue to pursue additional cloud-related credentials to further strengthen my expertise</p><p><a href="#top">맨 위로</a></p><h3 id="a2">왜 이직하니?</h3>I’ve truly enjoyed my time at PNC and  I’ve spent over six years working on meaningful projects and improved my technical skills.  Now I feel I ready for a new challenge that allows me to expand further.  Technology is evolving faster than ever, and I want to keep learning and developing new skills.for me It’s not about leaving something behind — it’s about taking the next step toward work I’m truly passionate about.<p><a href="#top">맨 위로</a></p><h3 id="a4">보잉이란 그리고 지원동기</h3>Boeing is one of the world’s largest aerospace and defense companies. It designs and builds commercial airplanes like the 737 and 777.Also, Boeing’s work connects people, supports global transportation, and contributes to national security and space exploration.That's why I applied to this company to builds products with real-world impact.<p><a href="#top">맨 위로</a></p><h3 id="recent-project">Recent project (최근 프로젝트)</h3>Currently, I am working on building a near real time pipeline that ingests kafka topic data into Oracle Exadata and then into Hadoop platform.In the past, stakeholders had to rely on the previous day’s data to make decisions. But now with this new pipeline, data from Kafka is ingested into Hadoop in every 10 minutes and then visualized through Tableau dashboards.This project significantly reduced data latency and helped business team to make faster decision.<p><a href="#top">맨 위로</a></p><h3 id="issue-data-volume">이슈 - 데이터 볼륨증가 3~4TB </h3>One of the biggest challenges I faced recently was with a Kafka-to-Hadoop data pipeline, where Oracle Exadata was used as a staging area.<p>Initially, the volume of data coming from Kafka was about 1 TB per day, but it suddenly increased to 3 or 4 TB per day. Even though the data was automatically deleted after being loaded into Hadoop, new data was coming in faster than it could be deleted, so Exadata started running out of space. To handle this, I increased the number of Spark jobs to speed up data movement into Hadoop. But this caused to slow down the Exadata and it created a bottleneck issue. Then I suddenly thought about compressing the data at exadata base, and luckly, I discovered that EXADATA has a built-in compression feature — and the best part is that the data doesn’t need to be decompressed when it’s moved to Hadoop. Using this compression method, I was able to reduce the data size by almost 70% in Exadata.  After that, I reduced the number of Spark jobs, which helped Exadata run better and stabilized the pipeline.</p><p><a href="#top">맨 위로</a></p><h3 id="issue-schema-change">이슈 - 스키마 변경</h3>I remember there was a project that we were integrating data from multiple sources into a central data warehouse.<p>The challenge was that one of the upstream systems frequently changed its schema without notice. And it caused our ETL jobs to fail and delayed reporting for business users. My responsibility was to make the pipeline more resilient so that these schema changes would not break the entire data flow. I implemented a schema validation and auto-adjustment process. I updated the code to compare the incoming data schemas against our expected schema. If a non-critical column changed such as a new column being added, the pipeline could adapt automatically without failing. But For critical mismatches, the system flagged the issue, generated incident, and provided fallback logic to continue processing the data. This reduced ETL job failures by more than 90% and ensured that the business team continued to receive the data even when upstream systems changed its schema unexpectedly.</p><p>Valid records are loaded into the main HDFS path, and Invalid records are redirected to a separate reject HDFS path.</p><ul><li>When a string value like “ABC” appears in a numeric column ,</li><li>When a NULL value is provided for a NOT NULL column ,</li><li>When a date column receives a value in a completely different or invalid format ,</li><li>When the data type is correct but the length exceeds the limit (e.g., a 50-character string for a VARCHAR2(10) column)</li></ul><p>오라클에 데이터가 저장될때 : A CLOB (Character Large Object) is a data type used to store very large text data.</p><p><a href="#top">맨 위로</a></p><h3 id="data-warehouse-tech">데이터 웨어하우스 설계 (e.g. Amazon Redshift, Snowflake 사용경험)</h3>I have experience using Redshift to build the cloud data warehousing. In one of my project, I built an analytics pipeline to process and analyze mobile user login data.To achieve this, I set up a pipeline where the log data was first stored in Amazon S3. From there, AWS Glue processed and loaded data into Redshift. Once the data was in Redshift, I used Amazon QuickSight to build interactive dashboards. And it visualized key user activity such as session duration, clickstream patterns, and device usage. This solution provided business stakeholders with actionable insights.<p><a href="#top">맨 위로</a></p><h3 id="data-warehouse-problem">데이터 웨어하우스 지연속도 문제 및 해결 </h3><p>One of the challenges I ran into was that loading JSON files from S3 into Redshift was much slower than I expected.<br>Because the data was in JSON, Redshift had to parse every row, and the file sizes were all different.<br>This caused performance issues and even led to uneven data distribution across Redshift nodes.</p><p>To fix this, I redesigned the ingestion process in AWS Glue.<br>I converted the JSON data into Parquet and saved the files in S3 with same sizes—around 128 MB.<br>Since in Parquet the data is already fully parsed, Redshift didn’t have to do extra parsing during the load, which significantly sped up the loading process.<br>I also updated the DISTKEY and SORTKEY based on how the data was being queried. And it helped prevent data skew and allowed Redshift to process data more evenly across all nodes.</p><ul><li>왜 128MB 사이즈로 데이터를 만드는지?<br>I stored the Parquet files in 128 MB chunks because Redshift performs best when it reads multiple files of similar size in parallel. Consistent file sizes help avoid data skew, reduce S3 overhead, and allow Redshift to distribute the workload evenly across all nodes, which results in much faster COPY performance.</li></ul><p><a href="#top">맨 위로</a></p><h3 id="data-normalization">데이터 정규화와 비정규화의 차이점</h3>Normalized data is typically used in OLTP systems. It separates data into multiple related tables to reduce redundancy and maintain data integrity. This helps ensure consistency during insert, update, and delete operations, but often requires multiple joins to retrieve data.Denormalized data is more common in OLAP systems. It intentionally duplicates data by combining related fields into fewer tables, which improves read performance and speeds up complex analytical queries.<p><a href="#top">맨 위로</a></p><h3 id="star-snowflake-schema">Star & Snowflake 스키마 (데이터 웨어하우스에서 스타 스키마 사용)</h3>In most data warehouses, the Star Schema is used because it provides high query performance, especially for analytical workloads, and has a simple structure consisting of a central fact table connected to denormalized dimension tables. This simplicity also makes it well suited for BI tools like Tableau or Power BI.But the Snowflake Schema is also used—especially when storage efficiency or data normalization is a higher priority. It tends to introduce more joins, which can affect query performance. Therefore, Star Schema is generally preferred in data warehouse environments.<p><a href="#top">맨 위로</a></p><h3 id="dq-multi-point-good">데이터 품질관리 - 여러포인트 검증 &amp; good data</h3>When it comes to data quality, I apply validation at multiple points of the pipeline.During ingestion, I perform schema validation and basic checks such as null values, data types, and duplicates. As the data moves through transformations, I apply additional business-rule validations to ensure the results make sense before loading them into data warehouse.In addition, I worked closely with business team to define what “good data” means for their use cases.  And I ensured that the dashboards in Tableau reflected the reliable information for decision-making.<p>In one project, I worked with the fraud prevention team, where my role was to deliver data they could fully trust. For them, “good data” meant accurate, up-to-date, and reliable information without duplication or errors. Because the quality of data directly impacted their fraud detection models, I focused not only on data delivery but also on maintaining high quality through the validation and monitoring.</p><p><a href="#top">맨 위로</a></p><h3 id="etl-design-ca7-glue">ETL (오케스트레이션) 설계 - CA7와 Glue 이용</h3>I’ve designed and implemented both batch and near real-time ETL pipelines. For near real-time workloads, I built pipelines that ingest Kafka streaming data into Oracle Exadata and Hadoop every ten minutes. I used PySpark for transformations and I used the CA7, a mainframe-based scheduler, to orchestrate the dependencies across these jobs. CA7 ensured that each PySpark workflow ran in the correct sequence and at the right time and it was critical for the batch operations.I also have experience building cloud-native ETL solutions. In one project, I used AWS Glue studio to design the ETL workflows. Glue’s built-in transformations, and job orchestration features made it easier to manage the logic.<p><a href="#top">맨 위로</a></p><h2 id="기술-내용"><a href="#기술-내용" class="headerlink" title="기술 내용"></a>기술 내용</h2><h3 id="alteryx-usage">Alteryx 사용기간 및 경험</h3>I used Alteryx for about a year in the past, mainly for data preparation and automation tasks such as joining datasets, performing aggregations, and creating analytical outputs. However, I didn’t use it as the primary tool in any large-scale enterprise projects. These days, I mainly work with AWS Glue Studio. When dealing with larger datasets, I noticed that Alteryx tends to slow down since it’s not optimized for big-data workloads. But AWS Glue Studio runs on Apache Spark, which provides much better performance and scalability for heavy ETL processing.<p><a href="#top">맨 위로</a></p><h3 id="teradata-usage">테라데이터 사용기간 및 경험</h3>I have around two years of experience with Teradata, mainly using it with Hadoop system. In our setup, Hadoop stored the customer's account data, and Teradata accessed that data through QueryGrid, which allowed us to easily combine and query Hadoop datasets. We also connected Tableau to Teradata and set up hourly refreshes so the dashboards always reflected the latest customer's account data insights.<ul><li>사용할때 문제 : Network latency 이슈<br>Network latency was the biggest issue we faced. Because Teradata had to retrieve large volumes of detailed data from Hadoop over QueryGrid, there were many cases where the query didn’t return on time or failed altogether. To address this, we changed the approach so that heavy processing happened in Hadoop first. We aggregated and filtered the data using Spark, and then used QueryGrid only to bring back a much smaller dataset. This significantly reduced the amount of data being transferred, which helped avoid latency issues and made the overall query performance much more stable.</li></ul><p><a href="#top">맨 위로</a></p><h3 id="teradata-troubleshooting">테라데이터 문제해결</h3>One of the most common issues I faced with Teradata was slow query performance when working with large tables, especially when the table wasn’t partitioned. In those cases, Teradata had to scan the entire table, which made daily jobs take much longer than expected.To fix this, I added date-based partitions so Teradata only scanned the specific partition needed for each query. This small change made a big difference. the queries became much faster and more stable. It also helped reduce the load on the system and improved overall performance.<p><a href="#top">맨 위로</a></p><h3 id="teradata-performance">테라데이터가 높은 성능을 발휘할수 있는 이유</h3>Teradata leverages an MPP architecture where data is distributed across multiple AMPs (Access Module Processors). Each AMP works independently to store and process its portion of data and it enables parallel execution. Because of this distribution mechanism, Teradata can handle large volumes of data with high performance.<p><a href="#top">맨 위로</a></p><h3 id="teradata-pi">테라데이터의 Primary Index (PI) 역할</h3>A Primary Index determines how data is distributed across AMPs. Choosing the right PI is crucial because it ensures even data distribution. A well-chosen PI improves join performance and overall query efficiency. (Teradata 내부에서는 데이터를 저장할 때 Primary Index 컬럼에 해시 함수를 적용해서 Hash Value를 만들고, 그 값을 기반으로 어떤 AMP에 저장할지를 결정합니다.)<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> customer (</span><br><span class="line">    customer_id <span class="type">INTEGER</span>,</span><br><span class="line">    name <span class="type">VARCHAR</span>(<span class="number">100</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">PRIMARY</span> INDEX (customer_id);</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="teradata-skew">테라데이터 데이터 불균형을 어떻게 해결?</h3>Data skew occurs when data is unevenly distributed across AMPs, causing some AMPs to process significantly more data than others. This leads to slower query performance. To handle this Data skew , I typically review PI selection and check for unique columns. Sometimes, creating a multicolumn PI can help balance the distribution.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> customer_new</span><br><span class="line"><span class="keyword">PRIMARY</span> INDEX (new_column)</span><br><span class="line"><span class="keyword">AS</span> customer</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">NO</span> DATA;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT INTO</span> customer_new</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> customer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> customer;</span><br><span class="line"></span><br><span class="line">RENAME <span class="keyword">TABLE</span> customer_new <span class="keyword">TO</span> customer;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># orders_stage : 스테이징 테이블</span><br><span class="line"># orders_nopi : NoPI 테이블 (<span class="keyword">Primary</span> Index 없음)</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT INTO</span> orders_nopi</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> orders_stage</span><br><span class="line">HASH <span class="keyword">BY</span> customer_id;</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="teradata-secondary-index">테라데이터의 Secondary Index 이란</h3>A Secondary Index is useful when frequently queried columns are not part of the Primary Index. It accelerates data access without re-distributing data. However, because Secondary Indexes require additional maintenance, I usually add them only when a business-critical query pattern consistently needs optimization.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># SI 추가하기</span><br><span class="line"><span class="keyword">ALTER TABLE</span> your_table_name</span><br><span class="line"><span class="keyword">ADD</span> INDEX (column_name);</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="teradata-partition">Teradata 파티션</h3>Partitioning allows tables to be divided into manageable segments, usually based on date. This improves query performance because Teradata only scans relevant partitions instead of the whole table. I commonly used date-based partitioning.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> sales_daily (</span><br><span class="line">    order_id     <span class="type">INTEGER</span>,</span><br><span class="line">    order_date   <span class="type">DATE</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> RANGE_N(order_date <span class="operator">&gt;=</span> <span class="type">DATE</span> <span class="string">&#x27;2023-01-01&#x27;</span></span><br><span class="line">                     <span class="keyword">EACH</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> sales</span><br><span class="line"><span class="keyword">WHERE</span> order_date <span class="operator">&gt;=</span> <span class="type">DATE</span> <span class="string">&#x27;2023-05-01&#x27;</span></span><br><span class="line">  <span class="keyword">AND</span> order_date <span class="operator">&lt;</span>  <span class="type">DATE</span> <span class="string">&#x27;2023-06-01&#x27;</span>;</span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="alteryx-teradata">Alteryx와 Teradata 사용</h3>I think ETL pipelines between Alteryx and Teradata are built using Alteryx’s In-DB tools. Alteryx generates SQL and pushes all heavy transformations to the Teradata MPP engine, which handles large-scale joins and aggregations efficiently. Alteryx simply orchestrates the workflow, while Teradata performs the actual processing. This approach combines the ease of use of Alteryx with the scalability of Teradata.<h3 id="neo4j">Neo4J 관련해서</h3>Although I haven’t used Neo4j in production, But I’m interested in graph databases. I would like to have the opportunity to learn and apply Neo4j in future projects.<p><a href="#top">맨 위로</a></p><h3 id="multi-national-collaboration">여러 민족 같이 근무 - sync 미팅, 미팅요약</h3>Currently, I work on a data integration project with team members from the U.S., India, and Europe. At first, coordination was difficult because of time zone differences and different communication styles. To improve collaboration, I organized short daily sync meetings that overlapped our working hours and encouraged open discussions so everyone could share progress or blockers. I also started sending clear written summaries after each meeting so teammates in different time zones could stay updated. As a result, we reduced misunderstandings and improved task handoffs between regions.<p><a href="#top">맨 위로</a></p><h3 id="cross-team-collaboration">다른 팀 협업 - 용어 통일</h3>In one of my projects, I worked closely with software engineers and business analysts to improve how we tracked and analyzed user behavior. The engineers were responsible for sending user activity data into our database, and my role was to clean and transform that data so it could be used for reporting and analysis. I noticed that each team had slightly different definitions for key metrics, like “active users” or “sessions,” which caused confusion in reports. So, I organized a short meeting to align on clear definitions and updated our data dictionary to make sure everyone used the same terms. After that, the reports became much more consistent, and the business team was able to make decisions faster and with more confidence. It was a great experience showing how clear communication and teamwork can really improve data quality and trust.<p><a href="#top">맨 위로</a></p><h3 id="tight-schedule-pressure">타이트한 스케줄 & 압박 - 일 나누고 대화, 트랙</h3>When I face tight deadlines or high-pressure situations, I stay calm and break the work into smaller parts. For example, in one project, our team had to build a new ETL workflow in less than two weeks because of a last-minute client request. Instead of stressing out, I focused on what was most important, assigned tasks clearly, and set up short daily check-ins to track progress. I also kept open communication with both the team and stakeholders, making sure everyone understood what we could realistically deliver. By staying organized and working together, we completed the project on time with great results. This experience taught me that under pressure, clear priorities, steady communication, and teamwork are the keys to success.<p><a href="#top">맨 위로</a></p><h2 id="품질과-보안-내용"><a href="#품질과-보안-내용" class="headerlink" title="품질과 보안 내용"></a>품질과 보안 내용</h2><h3 id="dq-unit-test">데이터 품질관리 - 유닛테스트</h3>I usually use Pytest for unit testing in Python. It’s simpler and more readable than the built-in unittest module, and it allows to write tests quickly without creating test classes. In Pytest, test functions simply start with test_, and I use the assert statement to verify the results.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"><span class="keyword">from</span> calculator <span class="keyword">import</span> add, divide  <span class="comment"># calculator.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_add</span>():</span><br><span class="line">    <span class="keyword">assert</span> add(<span class="number">2</span>, <span class="number">3</span>) == <span class="number">5</span></span><br><span class="line">    <span class="keyword">assert</span> add(-<span class="number">1</span>, <span class="number">1</span>) == <span class="number">0</span></span><br></pre></td></tr></table></figure><p><a href="#top">맨 위로</a></p><h3 id="quality-issue-currency">품질 문제 - 통화단위 에러</h3>During a new ETL release, one of our reports was showing incorrect revenue numbers. After investigating, I found that the issue came from an incorrect currency conversion in the transformation logic. I quickly fixed the script, reprocessed the data, and added the automated checks to compare daily results with historical trends. After that, the data became much more accurate, and the same issue never happened again.This experience reminded me how important it is to validate data thoroughly before going to production.<p><a href="#top">맨 위로</a></p><h3 id="security-compliance">보안과 규정 (AWS, Azure)</h3>In my current role, we have a dedicated security and compliance team that handles overall data governance. So if I need access to certain sensitive databases or tables, I first have to get approval from that team. This helps only the right people can have an access to the data. On the data engineering side, I am responsible for protecting sensitive data during our ETL processes. That means identifying PII data and masking them, so even if someone sees this data, it’s not readable. Actually, We strictly follow the principle of least privilege. We assign only the minimum required permissions.<p><a href="#top">맨 위로</a></p><h2 id="행동-면접-질문"><a href="#행동-면접-질문" class="headerlink" title="행동 면접 질문"></a>행동 면접 질문</h2><h3 id="manager-absence-decision">매니저가 부재시 결정해야할 경우</h3>A few months ago, there is a configuration issue during a production release. And it caused to delay the data. At that time, my manager was not available, but the fix required his approval. I quickly analyzed the impact, documented the issue and solution, and escalated it to the department lead for temporary approval. I clearly communicated the risk and rollback plan, implemented the fix, and monitored results until the system was stable. When my manager returned, I shared a full summary and documentation for review. This taught me how to act responsibly under pressure. And I also learned to make quick but careful decisions and keep communication clear with others.<p><a href="#top">맨 위로</a></p><h3 id="team-disagreement-sla">팀원 의견 다름 - SLA 20분 지연 - 품질해결</h3>Yes, I’ve experienced disagreements within the team. One example was during a near real-time data pipeline project. We were loading Kafka data into Hadoop, and the pipeline often missed our 10-minute SLA — sometimes it took over 20 minutes. some of the team members wanted to focus on improving Spark performance, while others, including me, thought the main issue was data quality because of inconsistent records and schema mismatches. To find the real cause, I checked the logs and monitoring reports and found that about 70% of the delays were due to data validation errors, not Spark processing speed. Based on that insight, I proposed a short proof-of-concept to implement stronger schema validation and fallback rules in QA environment and it worked. After implementing it in production, the number of failures dropped significantly and we regained our SLA. Once the team saw the data and results, everyone agreed to proceed with quality improvements first, then revisit performance tuning. This experience taught me that using measurable data, clear communication, and structured tests is much more effective than letting opinions dominate technical decisions.<p><a href="#top">맨 위로</a></p><h3 id="manager-disagreement-refresh-partition">상사 의견 다름 - refresh only 변경된 파티션만</h3>If I don’t agree with my manager’s opinion, I first make sure I fully understand their reasoning and goals. Then I share my perspective, supported by data or examples, rather than emotion. For instance, in one project, my manager suggested running a full data reload every day to ensure data completeness. I understood his concern but also knew it would be inefficient, since most of the customer master data rarely changed. So I analyzed the update patterns and designed a process to refresh only the partitions containing changed customer records, instead of reloading the entire dataset. After testing it together in staging, we confirmed that this approach maintained accuracy while reducing runtime from over 3 hours to about 40 minutes. That experience taught me that presenting clear evidence and focusing on the common goal — not on who’s right or wrong. It helps turn disagreements into productive discussions.<p><a href="#top">맨 위로</a></p><h3 id="diffent-personality">다른 성향의 사람과 협동</h3>In one of my previous projects, I worked closely with a senior engineer whose personality was quite different from mine. I’m generally organized and prefer to plan tasks carefully before execution, but he preferred to take a more spontaneous (스팬테니어스), “just try it and fix later” approach. At first, this difference caused some tension because I wanted to review the design and test cases before deployment, while he wanted to move fast to meet deadlines. Instead of arguing, I suggested we combine both styles — I would document the structure and validation rules, and he could focus on rapid prototyping. By dividing responsibilities that way, we were able to deliver the pipeline faster and maintain quality. Over time, I also learned to be more flexible and open to trying quick experiments. And he started to appreciate the value of planning and testing as well. That experience taught me that personality differences can actually strengthen a team when you focus on complementary strengths rather than conflicts.<p><a href="#top">맨 위로</a></p><h3 id="helped-teammate">팀동료 성공시키기</h3>In one project, I worked with a junior data engineer who was new to our Spark-based ETL environment. She was struggling to understand how our partitioning and scheduling logic worked, and her job often failed in production. Instead of just fixing it for her, I scheduled a short session to walk her through how the Spark job read data from Oracle, wrote it into Hadoop platform. I also helped her debug one of her failing jobs step by step and showed her how to check logs and handle schema mismatches. Within a few weeks, she became confident enough to manage her own pipelines and even automated some validation scripts. Seeing her grow and succeed made me realize that helping others not only strengthens the team but also improves overall project quality.<p><a href="#top">맨 위로</a></p><h3 id="urgent-vs-important">급한일과 중요한일의 우선순위 - 급한것 먼저</h3>I usually start by understanding the impact of each task. If something is urgent and affects business operations or other teams, I handle it first. But I also make sure not to ignore important long-term work. For example, once our production ETL job failed right before a reporting deadline. I paused my ongoing optimization task, fixed the ETL issue immediately, and restored the pipeline so the business could get their reports on time. After that, I resumed my optimization work. I believe good prioritization means balancing immediate needs with long-term improvements.<p><a href="#top">맨 위로</a></p><h3 id="tech-challenge-1-5tb">기술 문제 도전 - 1.5TB 처리</h3>One of the most challenging technical problems I faced was optimizing a large Spark ETL job that processed about 1.5 TB everyday. The job was taking more than 5 hours to complete, which caused delays in our downstream dashboards and reports. I started by analyzing the Spark UI and noticed heavy shuffling and many small output files. To fix it, I adjusted the partition strategy, used broadcast joins for smaller tables, and combined small files before writing to Hadoop. I also added data filtering early in the pipeline to reduce unnecessary computation. After these changes, the runtime dropped from 5 hours to under 3 hours, and the cluster cost was reduced by almost 30%. That experience taught me how small technical optimizations can have a big business impact when working with large-scale data.<p><a href="#top">맨 위로</a></p><h3 id="improvement-large-data">개선 사례 - 큰 데이터 처리 (위 동일)</h3>I remember that one of our daily ETL jobs was taking more than 5 hours to finish. It processed a large amount of log data from multiple sources, and sometimes it even failed because of memory issues. I reviewed the Spark job and found that it was using too many small files and unnecessary joins. I optimized the job by adjusting the partition size, adding proper filters early in the transformation, and combining small files before loading. After the changes, the job ran in less than 3 hours and became much more stable. This improvement not only saved computing costs but also made our data available earlier for reporting every morning.<p><a href="#top">맨 위로</a></p><h3 id="production-connect-stop">생산 문제 - 커넥트 Stop</h3>When a production issue happens, I stay calm and focus on finding the root cause quickly. For example, one night our Kafka-to-Hadoop pipeline failed, and the business dashboards in Tableau were missing data the next morning. I immediately checked the Kafka Connect logs and found that the sink connector had stopped due to a network issue. I manually restarted the connector and confirmed that the data started flowing again. Afterward, I created a monitoring script using curl commands that checks the connector status every 10 minutes. If it fails, the script automatically creates an incident and sends an alert to our team. This experience taught me the importance of not only fixing issues quickly but also building automation to prevent the same problem from happening again.<p><a href="#top">맨 위로</a></p><h3 id="process-improvement-connect-stop">자발적 프로세스 개선 - 커넥트 Stop</h3>In one project, I noticed that one of our data pipelines sometimes failed overnight, but the team would only find out the next morning. This caused delays in daily reports and frustration for analysts waiting for updated data. Even though it wasn’t part of my assigned tasks, I decided to create an automatic alert system. I built a small Python script that checked job completion logs and create INC if a failure occurred. After testing it for a week, I presented it to the team, and we integrated it into our pipeline. Since then, we’ve been able to respond to failures immediately, reducing downtime and improving data reliability. That experience taught me the value of being proactive — small improvements can make a big difference to the whole team.<p><a href="#top">맨 위로</a></p><h3 id="kafka-realtime-connect-check">Kafka 실시간 데이터에서 고려할 부분 - 커넥트 상태확인</h3>When designing Kafka pipelines, I focus on a few key areas to ensure performance and reliability. First, I choose the right topic partitioning strategy based on data size. And then I make sure that Kafka connectors are properly configured with retry mechanisms in case of failures. For monitoring, I built a script that uses a curl command to check the status of the all Kafka sink connectors every 10 minutes. If the one of the connectors is down or there’s an issue with the Kafka broker, the script automatically generates an incident, triggering an alert to my team. This setup helped us catch issues and significantly reduced downtime.<p><a href="#top">맨 위로</a></p><h3 id="failure-column-validation">실패,실수 - 컬럼검증X, 통화단위 에러</h3>Yes, I made a mistake during a data validation process. I was in charge of checking the output of a new ETL job before it went live. I verified the total record count but forgot to double-check the column-level transformations. After deployment, we found that one column had an incorrect currency conversion rate, and it caused wrong numbers to show up in a few business reports. As soon as I realized the issue, I corrected the transformation logic, reprocessed the data, and updated the reports. After that, I added column-level validation rules and a simple Python script that automatically compares key fields between the source and target tables before deployment. That experience taught me how even a small mistake can affect business reports, so now I always check both the data structure and actual values carefully before sign-off.<p><a href="#top">맨 위로</a></p><h3 id="project-delay-schema-change">프로젝트 지연 - 스키마 변경</h3>Yes, I’ve experienced that before.In one project, our data ingestion pipeline was delayed because the upstream system changed its schema without notice. This caused our ETL jobs to fail and delayed daily reports for the client. As soon as the client raised concerns, I explained the issue clearly, shared the revised delivery plan, and sent daily updates so they could see our progress. Meanwhile, I worked with my team to add automatic schema validation and fallback logic in the pipeline, so future schema changes wouldn’t break the process again. After we implemented the fix, the pipeline became more stable, and the client appreciated our quick communication and the long-term solution we put in place.<p><a href="#top">맨 위로</a></p><h3 id="team-lead-source-missing">팀 리드 & 솔선 - 소스 입력 안됨</h3>In one project, we had a very tight deadline to deliver a new ETL workflow for daily reporting. However, a few tasks were delayed because some external data sources were not delivered on schedule, and that caused downstream jobs in Spark to fail during testing. To get things back on track, I took the initiative to organize short daily stand-up meetings and created a shared progress tracker in Confluence so everyone — including the data and QA teams — could see real-time task status. This helped us identify blockers early, communicate clearly, and reassign tasks based on team availability. Within a week, we recovered the lost time and successfully completed the workflow before the deadline. The reporting system went live as planned, and we avoided last-minute production issues. That experience taught me that strong coordination and clear communication are just as important as technical skills when leading a project under tight timelines.<p><a href="#top">맨 위로</a></p><h3 id="team-lead-datatype-mismatch">리더싶 사례 - data type mismatch</h3>During a data migration project, we faced a serious issue when one of the ETL jobs started failing right before a major release. Everyone was under pressure, and the team was unsure how to proceed. Even though I wasn’t the official lead, I took the initiative to organize an emergency meeting with the data, QA, and infrastructure teams. I divided the investigation into parts — one team checked data source changes, another team looked at schema issues, and I focused on debugging the Spark job logic. After identifying that a data type mismatch in one column was causing the failure, we quickly fixed it and ran validation tests together. The release went smoothly, and my manager later recognized my leadership for coordinating the teams under tight deadlines. That experience taught me that real leadership often means stepping up and guiding the team toward a solution — even without having a formal title.<p><a href="#top">맨 위로</a></p><h3 id="above-and-beyond-responsibility">지시받지않은 일 - add load_date / 상사와 의견이 맞지 안음사례와 동일</h3>A few months ago, I noticed that one of our nightly ETL jobs in production was running slower and occasionally failing, even though it wasn’t part of the pipelines I was directly responsible for. Instead of ignoring it, I decided to investigate on my own time because it was delaying downstream reports for the business team. After checking the logs, I found that the job was performing a full table scan on a very large dataset every night. To fix the issue, I first added a new LOAD_DATE column to the target table to track daily data loads. Then, I rewrote the logic to process only new and updated records based on this column and created partitions on LOAD_DATE to improve query performance and data management efficiency. After validating the logic, I worked with the scheduler team to test and deploy the fix safely. The result was dramatic — runtime dropped from over 3 hour to under forty minutes, and the business team could access their dashboards much earlier every morning. Even though it wasn’t my assigned task, I took ownership because I knew the issue was affecting overall business operations. That experience taught me that going above and beyond means proactively solving problems that impact the team — not just completing my own tickets.<p><a href="#top">맨 위로</a></p><h3 id="customer-request">고객이 마지막에 변경요청시 - 스키마변경</h3>When a customer requests changes right before the final release, I believe it’s important to balance flexibility with stability. First, I listen carefully to understand why the change is needed — whether it’s a business-critical fix or just a nice-to-have improvement. Then I assess the impact on scope, timeline, and quality. If the change is minor and doesn’t risk the release, I coordinate quickly with the team to implement and test it. However, if the change is major or could affect stability, I clearly communicate the risks and propose alternatives — such as including it in the next patch or minor release. The most important thing is to be honest about the situation, and focus on finding solutions. This way, the customer knows you’re listening, and the project stays on track.For example, in one project, a client requested a schema change right before deployment. I analyzed the dependency, explained that it would delay the release by two days, and suggested deploying the current version first and adding the change in the next release. The client agreed, and we delivered on time without compromising quality.<p><a href="#top">맨 위로</a></p><h3 id="customer-request-offen">고객이 자주 변경사항을 요청할때 - 요청을 그룹핑함</h3>When customers often ask for changes, I try to handle it in a clear way. First, I listen carefully to understand why they want the change — maybe their business needs have changed or something wasn’t clear before. Then I explain what the change means for the project — like how it might affect the schedule or workload — so they can decide what’s most important. If there are many small requests, I suggest grouping them together or saving them for the next update.In one project, the customer kept asking for new data checks. I made a simple list to track all the requests and talked with them once a week to decide which ones to do first. That way, they felt listened to, and our team could work in an organized way without confusion.<p><a href="#top">맨 위로</a></p><h3 id="non-technical">비개발자에게 기술적인 내용을 쉽게 설명할 수 있나요?</h3>Yes, I always try to explain technical topics in a simple and clear way, especially when talking to non-technical people. I focus on using everyday language instead of technical terms, and I give real examples that relate to their work or daily life. For example, when explaining data pipelines, I might say it’s like a factory line — data comes in as raw material, goes through cleaning and transformation, and comes out as a finished product ready for analysis. I believe being able to translate complex ideas into simple concepts is an important skill for teamwork and communication.<p><a href="#top">맨 위로</a></p><h3 id="issue-spark-memory">이슈 - Spark memory 문제</h3>One of the most common issues I encounter is out-of-memory (OOM) errors. To address this, First, I review the PySpark code to identify any operational command like collect() or toPandas() that might be pulling too much data into the driver. If I find them, I either remove or replace them. I also use broadcast joins when dealing with small tables to minimize shuffle operations, it can reduce memory usage. Another important step is avoiding Python UDFs if it is possible to use native Spark SQL functions. Additionally, when I need to reuse the intermediate results, I use caching it and also I use MEMORY_AND_DISK storage option to avoid overwhelming the memory. Finally, I adjust partition sizes using coalesce() or repartition() to optimize resource usage during shuffle operations. By applying these techniques, I’ve been able to effectively prevent and troubleshoot memory-related issues in Spark jobs.<p><a href="#top">맨 위로</a></p><h3 id="etl-pipeline-optimization">ETL pipeline 최적화 - SLA 6시로 맞추기</h3>I had a situation where our SLA required the data to be fully available by 6 AM, But one day, the amount of source data suddenly increased — almost three times more than usual. Because of that, our Spark job didn’t finish until 8 AM. So I increased the number of partitions to allow more parallel processing. I also checked our resource settings and made sure the job had enough memory and CPU by adjusting the scheduler pool and YARN resorce manager.After these changes, the job completed before 6 AM the next day, and we were able to meet the SLA again. This experience helped me understand how important it is to tune the Spark jobs and monitor them carefully, especially when data volume suddenly increase.<p><a href="#top">맨 위로</a></p><h3 id="aws-glue-experience">AWS Glue 사용경험 - ETL services</h3>I have experience building ETL workflows using AWS Glue. In one of my project, I built an analytics pipeline to process and analyze mobile user log data. I’ve used Glue to extract data from S3 and then transform and load into Redshift every 10 minutes Also, I’ve utilized Glue Crawlers to automatically detect schema changes and keep the data catalog updated for querying in Athena as well.<p><a href="#top">맨 위로</a></p><h3 id="no-current-glue">현재 glue를 사용하지는 않는다.</h3>We are currently using CA7 mainframe along with PySpark scripts for our ETL processes mainly. CA7 is a mainframe-based job scheduling and workflow automation tool. It's used to manage and schedule batch jobs, ensuring the tasks run in the right order and at the right time. We have not changed this Orchestration tool Because Our data workflows have been integrated into a mainframe-based CA7 scheduling system for a long time and switching would introduce additional operational costs. Lastly Our team continues to manage and monitor all ETL workflows within the CA7 environment.<p><a href="#top">맨 위로</a></p><h3 id="redshift-intro">Redshift 란?</h3>Redshift is designed for high-speed querying using massively parallel processing (MPP). This makes it great for analyzing large datasets quickly. we can start small and scale up by increasing the node size or number of nodes as the data grows. Data is stored in columnar format, which speeds up analytical queries and reduces I/O.<p><a href="#top">맨 위로</a></p><h3 id="redshift-columnar">Redshift Columnar Storage</h3>When we execute queries like SUM(), AVG(), or filtering on specific columns, the database only needs to read the relevant columns, not entire rows. This speeds up reading performance in data warehousing and analytics. Since each column typically contains similar types of data, it compresses more efficiently than row-based data. Also, it only reads the selected columns, the amount of data scanned is reduced.<p><a href="#top">맨 위로</a></p><h3 id="snowflake-advantages">스노우플레이크 장점 (zero-copy cloning and time travel)</h3>I can instantly clone entire databases or tables without duplicating data and it saves cost and time. Also, there is a Time Travel function and it lets us query or restore data from a previous point in time. It is useful for recovering the data.<p><a href="#top">맨 위로</a></p><h3 id="databricks-experience">Databricks 사용경험 - Anomaly detection</h3>On the Databricks side, I primarily work with the Azure-hosted version of Databricks. Recently, I developed an end-to-end scalable pipeline for computer vision anomaly detection. As you can see my portfolio website. You can see its notebook and model. I use the PyTorch and Hugging Face to train and build the model.<p><a href="#top">맨 위로</a></p><h3 id="partition-strategy">파티션 전략 (Spark, Redshift, Snowflake)</h3>Partitioning strategy depends on query patterns and data volume. Regarding the Oracle Exadata, I Used range partitioning by date column to support daily ingestion and quickly delete old data by simply dropping partition.In Spark, I used dynamic partition overwrite with partitionBy("date") when writing Parquet files, and adjusted the number of partitions with coalesce or repartition commands to avoid creating too many small files.In Redshift, I defined DISTKEY and SORTKEY based on the columns that were most frequently used in joins and filters, which helped improve query performance and reduce data movement across nodes.In Snowflake, I rely on its automatic micro-partitioning feature, which breaks data into 16MB blocks and optimizes storage and query performance without any manual intervention. However, for very large tables where queries frequently filter on specific columns—such as date, I define a cluster key to further improve performance and these approach improved query speed as well.<p><a href="#top">맨 위로</a></p><h3 id="oracle-range-partitioning">데이터 모델링 & Architecture - 오라클 range partitioning</h3>I have strong experience in data modeling and architecture, especially in designing data pipelines at PNC. In Oracle Exadata, I designed the tables using range partitioning by day, so that Kafka data was automatically separated into daily partitions. This made it much easier to manage large volumes of data, speed up queries, and improve overall performance. For example, instead of using a traditional delete command, we could simply drop an entire partition when the data was no longer needed.  And This is not only optimized storage space but also kept query performance fast and efficient, since queries only scanned the relevant partitions rather than the entire table.<p><a href="#top">맨 위로</a></p><h3 id="python-sql-spark">Python, SQL, Spark, and PySpark</h3>I am working with Python, SQL, Spark, and PySpark throughout my career as a data engineer. Python has been my primary programming language for building ETL pipelines. I've used it in both production and QA environments, including developing data ingestion frameworks.SQL is a core part of my daily workflow. I’ve written complex analytical queries and optimized SQL for performance on databases like Oracle Exadata.With Spark, I’ve built scalable data processing pipelines for both batch and near real-time use cases. I’ve used Spark in distributed environments, primarily through PySpark, to perform transformations, aggregations, and joins on large datasets.<p><a href="#top">맨 위로</a></p><h3 id="spark-hadoop-ingestion">경험 - 스파크/하둡 데이터 ingestion</h3>On the Hadoop and Spark side, I designed frameworks to handle large-scale data ingestion and transformation. For example, data coming from Oracle first needed to be cleaned before it could be used for reporting. I built PySpark jobs that automatically parse the data and removed duplicate records, handled missing values, and converted the data into optimized formats like Parquet and stored it at hadoop platform. At the same time, I added metadata and validation rules so that we could easily track the data and confirm its accuracy.<p><a href="#top">맨 위로</a></p><h3 id="aws-experience">경험 - AWS 많이 사용했니?</h3>If you take a look at my portfolio website, you’ll see that most of my projects are built using AWS. I actively use AWS to quickly build and experiment with different data architectures. Since data tools are evolving so fast, I use EMR service to easily install and try out big data tools like Spark, Hadoop, and Kafka. For storing data, I normally use RDS or S3. Overall, AWS has been a great platform for me to learn, experiment, and build end-to-end data pipelines.<p><a href="#top">맨 위로</a></p><h3 id="strengths-weaknesses">장점과 단점</h3>My biggest strengths are my flexibility and adaptability. Wherever I work, work environments change daily and throughout the day. And there are certain projects that require individual attention and others that involve a teamwork approach. My flexibility and adaptability have allowed me to meet the expectations and even go beyond them. Also, I get along with people around me. This kind of personality makes the work environment more comfortable and easierAs far as my weaknesses, I sometimes put in too much time on what I like to do. With my mentor’s help, I started using a daily checklist to plan and prioritize my work. Now I make sure I pace myself better and focus on finishing the most important tasks first. It’s helped me become more balanced and efficient.<p><a href="#top">맨 위로</a></p><h3 id="stress">스트레스를 어떻게 풉니까?</h3>When I feel stressed, I try to handle it in a healthy and productive way. First, I take a short break to clear my mind — even a short walk or a few minutes of quiet time helps me refocus. I also like to organize my tasks and set priorities. Once I have a clear plan, the stress usually goes down because I can see what needs to be done first. Outside of work, I relieve stress by exercising and spending time with my family or friends. These activities help me recharge and come back to work with more energy and focus.<p><a href="#top">맨 위로</a></p><h3 id="motto">삶의 모토는?</h3>My life motto is “Stay curious, stay humble, and keep growing.” I believe learning never stops, no matter how much experience you have. Staying curious helps me discover new ideas, staying humble keeps me open to feedback, and continuous growth gives me purpose in both my career and personal life.<p><a href="#top">맨 위로</a></p><h3 id="powertech">파워텍에서 머신러닝 모델 사용</h3>When I worked at Hyundai Powertech, we produced car transmissions. Each transmission needed a small gasket to fill the gap between parts, but the gap size was different for each transmission — sometimes 1 mm, 1.5 mm, or 2 mm. Because of this, the company had to keep all gasket sizes in stock, which wasted storage space and money. I collected the production log from the machines on the floor and analyze them and trained machine learning models to predict which gasket size would be needed for each transmission. Among several models, XGBoost performed the best. By using this prediction model, we reduced inventory levels and saved costs by ordering only the needed gasket sizes.- XGBoost is an efficient and high-performance boosting algorithm that combines many small decision trees to make strong and accurate predictions.<p><a href="#top">맨 위로</a></p><h3 id="final-remarks">마지막으로 하고 싶은 말</h3>May I ask which technologies your team works with most often, and what types of projects are currently the main focus?May I ask what qualities you think are most important to succeed in this position?May I ask which projects are currently the highest priority?<p><a href="#top">맨 위로</a></p><h3 id="python-type">파이썬 타입비교</h3>Strings are  immutable but maintain order and allow duplicate characters.Lists are mutable, ordered, and allow duplicates.Tuples are similar to lists but immutable.Dictionaries are mutable and ordered (as of Python 3.7+), but their keys must be unique.Sets are mutable but unordered and do not allow duplicate elements.]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a id=&quot;top&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
.toc-grid{
  display:grid;
  grid-template-columns:1fr 1fr; /* 화면 반반 */
  gap:6px 20px;
  align-items:start;</summary>
      
    
    
    
    <category term="OTHERS" scheme="https://kish191919.github.io/categories/OTHERS/"/>
    
    
    <category term="INTERVIEW" scheme="https://kish191919.github.io/tags/INTERVIEW/"/>
    
  </entry>
  
  <entry>
    <title>TERADATA(3)-History View</title>
    <link href="https://kish191919.github.io/2025/12/03/TERADATA-3-History-View/"/>
    <id>https://kish191919.github.io/2025/12/03/TERADATA-3-History-View/</id>
    <published>2025-12-03T17:12:57.000Z</published>
    <updated>2025-12-03T17:17:58.173Z</updated>
    
    <content type="html"><![CDATA[<h1 id="📘-Teradata-Studio-–-SQL-History-창-완전-정리"><a href="#📘-Teradata-Studio-–-SQL-History-창-완전-정리" class="headerlink" title="📘 Teradata Studio – SQL History 창 완전 정리"></a>📘 Teradata Studio – SQL History 창 완전 정리</h1><p><em>Teradata SQL History 창 설정, 컬럼 선택, 노트 기능, 복원 방법</em></p><hr><h2 id="📍-1-SQL-History-창-복원하기"><a href="#📍-1-SQL-History-창-복원하기" class="headerlink" title="📍 1. SQL History 창 복원하기"></a>📍 1. SQL History 창 복원하기</h2><p>SQL History 창을 실수로 닫았거나 사라졌다면:</p><p><strong>Window → Reset Perspective</strong></p><p>를 눌러주면 기본 레이아웃으로 복원됩니다.<br>→ History 창, Result Viewer 등 모든 패널이 원래 위치로 돌아옵니다.</p><hr><h2 id="📍-2-SQL-History-창-컬럼-Columns-커스터마이징"><a href="#📍-2-SQL-History-창-컬럼-Columns-커스터마이징" class="headerlink" title="📍 2. SQL History 창 컬럼(Columns) 커스터마이징"></a>📍 2. SQL History 창 컬럼(Columns) 커스터마이징</h2><p>SQL History는 기본적으로 매우 많은 컬럼을 보여줍니다.<br>예: Timestamp, Source, User, Destination, Row Count, Result, SQL Statement 등</p><p>필요 없는 컬럼이 많으면 화면이 복잡해지므로 <strong>원하는 항목만 선택</strong>할 수 있습니다.</p><h3 id="🔧-설정-경로"><a href="#🔧-설정-경로" class="headerlink" title="🔧 설정 경로"></a>🔧 설정 경로</h3><p><strong>Window → Preferences → Teradata Datatools → SQL History</strong></p><p>여기서 다음을 할 수 있습니다:</p><h3 id="🔹-1-표시할-컬럼-선택"><a href="#🔹-1-표시할-컬럼-선택" class="headerlink" title="🔹 1) 표시할 컬럼 선택"></a>🔹 1) 표시할 컬럼 선택</h3><ul><li>Removed Columns → 표시 안 함</li><li>Displayed Columns → 표시됨</li></ul><p>예시: 아래 항목만 선택하여 필요한 정보만 보이게 할 수 있음</p><ul><li>Timestamp</li><li>SQL Statement</li><li>Result (에러 메시지 포함)</li><li>Row Count</li><li>Elapsed Time</li></ul><p>적용 후 History 창에는 선택한 컬럼만 깔끔하게 표시됩니다.</p><hr><h2 id="📍-3-설정-적용-후-결과-확인"><a href="#📍-3-설정-적용-후-결과-확인" class="headerlink" title="📍 3. 설정 적용 후 결과 확인"></a>📍 3. 설정 적용 후 결과 확인</h2><p>예시 쿼리를 실행한 후 History 창에서 다음 정보를 확인할 수 있습니다:</p><ul><li><strong>Row Count</strong>: 조회된 레코드 수</li><li><strong>Result</strong>:<ul><li>성공 시 “Executed as Single Statement”</li><li>2000개 넘으면 “Canceled” (제한 때문에 일부만 조회됨)</li></ul></li><li><strong>Elapsed Time</strong>: 실행 소요 시간</li><li><strong>SQL Statement</strong>: 실행한 SQL 원문</li><li><strong>Timestamp</strong>: 실행한 날짜&#x2F;시간</li></ul><p>👉 불필요한 데이터 없이 핵심 정보만 남아서 훨씬 읽기 쉬워짐</p><hr><h2 id="📍-4-SQL-Notes-기능-사용하기"><a href="#📍-4-SQL-Notes-기능-사용하기" class="headerlink" title="📍 4. SQL Notes 기능 사용하기"></a>📍 4. SQL Notes 기능 사용하기</h2><p>SQL Editor에서 쿼리를 실행할 때 <strong>노트 입력 팝업</strong>이 뜰 수 있습니다.</p><p>예시 팝업 내용:</p><blockquote><p>“Enter a note for this query”</p></blockquote><p>이 기능은 쿼리 실행 시 <strong>개인 메모</strong>를 기록해두는 기능입니다.</p><p>예:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">My first query</span><br></pre></td></tr></table></figure><p>쿼리를 실행하고 History 창에 “Notes” 컬럼을 추가하면 해당 내용이 표시됩니다.</p><hr><h2 id="📍-5-Notes-컬럼-추가하는-방법"><a href="#📍-5-Notes-컬럼-추가하는-방법" class="headerlink" title="📍 5. Notes 컬럼 추가하는 방법"></a>📍 5. Notes 컬럼 추가하는 방법</h2><p>경로:</p><p><strong>Window → Preferences → Teradata Datatools → SQL History</strong></p><ol><li>Available Columns 목록에서 “Notes”를 선택</li><li>Displayed Columns로 이동</li><li>필요하면 <strong>순서 이동(Up&#x2F;Down)</strong> 기능으로 원하는 위치 조정</li><li>Apply and Close</li></ol><p>이제 History 창에 Notes가 표시됩니다:</p><table><thead><tr><th>Timestamp</th><th>SQL Statement</th><th>Result</th><th>Notes</th></tr></thead><tbody><tr><td>2025-02-10</td><td>SELECT * FROM …</td><td>OK</td><td>My first query</td></tr></tbody></table><p>💡 SQL을 많이 작성하는 경우 쿼리 목적이나 특이사항을 기록해두면 유용함.</p><hr><h2 id="📍-6-Notes-기능-비활성화"><a href="#📍-6-Notes-기능-비활성화" class="headerlink" title="📍 6. Notes 기능 비활성화"></a>📍 6. Notes 기능 비활성화</h2><p>Notes 기능을 사용하지 않는다면 다음과 같이 끌 수 있습니다.</p><p>방법 ①</p><ul><li>SQL 실행 후 뜨는 노트 팝업을 “비활성화” 체크</li></ul><p>방법 ②</p><ul><li>Notes 컬럼을 다시 Removed Columns로 이동<br>→ History 창에서 제거됨</li></ul><hr><h2 id="📍-7-History-창-즉시-설정하는-빠른-방법"><a href="#📍-7-History-창-즉시-설정하는-빠른-방법" class="headerlink" title="📍 7. History 창 즉시 설정하는 빠른 방법"></a>📍 7. History 창 즉시 설정하는 빠른 방법</h2><p>History 창 오른쪽 상단의 <strong>🔧 (Wrench) 아이콘</strong>을 클릭하면<br>바로 같은 설정 화면으로 이동할 수 있습니다.</p><p><strong>Window → Preferences</strong> 메뉴를 타고 갈 필요 없음.</p><hr><h2 id="📍-8-언제든-기본값으로-복원-가능"><a href="#📍-8-언제든-기본값으로-복원-가능" class="headerlink" title="📍 8. 언제든 기본값으로 복원 가능"></a>📍 8. 언제든 기본값으로 복원 가능</h2><p>History 설정이 꼬이거나 너무 복잡해졌다면:</p><p><strong>Restore Defaults</strong> 버튼 클릭</p><p>→ Teradata Studio의 기본 설정으로 깔끔하게 되돌아옵니다.</p><hr><h2 id="🎯-최종-요약"><a href="#🎯-최종-요약" class="headerlink" title="🎯 최종 요약"></a>🎯 최종 요약</h2><p>이번 강의에서는 SQL History 창을 다음과 같이 정리했습니다:</p><h3 id="✔-원하는-컬럼만-표시하도록-커스터마이징"><a href="#✔-원하는-컬럼만-표시하도록-커스터마이징" class="headerlink" title="✔ 원하는 컬럼만 표시하도록 커스터마이징"></a>✔ 원하는 컬럼만 표시하도록 커스터마이징</h3><ul><li>Timestamp</li><li>Row Count</li><li>Result</li><li>SQL Statement</li><li>Elapsed Time</li><li>Notes (필요 시)</li></ul><h3 id="✔-Notes-기능"><a href="#✔-Notes-기능" class="headerlink" title="✔ Notes 기능"></a>✔ Notes 기능</h3><ul><li>쿼리에 개인 메모 추가 가능</li><li>History 창에 Notes 컬럼으로 표시됨</li><li>사용하지 않으면 비활성화 가능</li></ul><h3 id="✔-빠른-설정"><a href="#✔-빠른-설정" class="headerlink" title="✔ 빠른 설정"></a>✔ 빠른 설정</h3><ul><li>History창의 🔧 아이콘으로 즉시 이동</li></ul><h3 id="✔-레이아웃-복원"><a href="#✔-레이아웃-복원" class="headerlink" title="✔ 레이아웃 복원"></a>✔ 레이아웃 복원</h3><ul><li>Window → Reset Perspective 로 전체 UI 초기화 가능</li></ul><hr><p>필요하시면 다음 강의도 정리해서 연결된 md 파일로 만들어드릴게요!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;📘-Teradata-Studio-–-SQL-History-창-완전-정리&quot;&gt;&lt;a href=&quot;#📘-Teradata-Studio-–-SQL-History-창-완전-정리&quot; class=&quot;headerlink&quot; title=&quot;📘 Teradata </summary>
      
    
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/categories/TERADATA/"/>
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/tags/TERADATA/"/>
    
  </entry>
  
  <entry>
    <title>TERADATA(2)-Customizing the Interface and Result Set Viewer</title>
    <link href="https://kish191919.github.io/2025/12/03/TERADATA-2-Customizing-the-Interface-and-Result-Set-Viewer/"/>
    <id>https://kish191919.github.io/2025/12/03/TERADATA-2-Customizing-the-Interface-and-Result-Set-Viewer/</id>
    <published>2025-12-03T16:38:31.000Z</published>
    <updated>2025-12-03T16:43:04.692Z</updated>
    
    <content type="html"><![CDATA[<h1 id="📘-Teradata-Studio-환경-설정-및-화면-커스터마이징"><a href="#📘-Teradata-Studio-환경-설정-및-화면-커스터마이징" class="headerlink" title="📘 Teradata Studio 환경 설정 및 화면 커스터마이징"></a>📘 Teradata Studio 환경 설정 및 화면 커스터마이징</h1><p><em>Teradata Studio 뷰 변경 &#x2F; 폰트 수정 &#x2F; Result Viewer 설정 &#x2F; 행 색상 변경 등</em></p><hr><h2 id="📍-1-하단-창-History-등-숨기기-보이기"><a href="#📍-1-하단-창-History-등-숨기기-보이기" class="headerlink" title="📍 1. 하단 창(History 등) 숨기기 &#x2F; 보이기"></a>📍 1. 하단 창(History 등) 숨기기 &#x2F; 보이기</h2><p>Teradata Studio에서 하단의 <strong>SQL History</strong> 또는 기타 창이 공간을 많이 차지할 때:</p><ul><li>창 우측 상단의 <strong>minimize( — )</strong> 버튼 클릭 → 창이 접힘</li><li>다시 보고 싶을 때는 아래 도킹된 탭을 클릭하면 다시 나타남</li></ul><p>✔ 화면을 넓게 쓰고 싶을 때 매우 유용한 기능</p><hr><h2 id="📍-2-폰트-크기-스타일-변경하기"><a href="#📍-2-폰트-크기-스타일-변경하기" class="headerlink" title="📍 2. 폰트 크기 &#x2F; 스타일 변경하기"></a>📍 2. 폰트 크기 &#x2F; 스타일 변경하기</h2><p>경로:<br><strong>Window → Preferences → General → Appearance → Colors and Fonts</strong></p><p>여기서 다양한 UI 요소의 폰트와 색상을 조정할 수 있습니다.</p><h3 id="🔸-폰트-종류-설명"><a href="#🔸-폰트-종류-설명" class="headerlink" title="🔸 폰트 종류 설명"></a>🔸 폰트 종류 설명</h3><ul><li>글자 옆에 <strong>A 아이콘</strong>이 있으면 “폰트 관련”</li><li>A가 없으면 배경색&#x2F;전경색 같은 “색상 관련”</li></ul><h3 id="🔸-Dialog-Font-변경하기"><a href="#🔸-Dialog-Font-변경하기" class="headerlink" title="🔸 Dialog Font 변경하기"></a>🔸 Dialog Font 변경하기</h3><p>경로:<br><code>General → Appearance → Colors and Fonts → Dialog Font</code></p><ol><li>원하는 폰트&#x2F;크기 선택</li><li>Preview에서 확인</li><li>Apply and Close</li></ol><p>📌 “Dialog Font”는 Studio 전반의 팝업창, 버튼, 표(Row) 등의 기본 폰트를 의미함.</p><blockquote><p>일부 변경 사항은 창을 <strong>닫았다 다시 열어야 반영됨</strong></p></blockquote><hr><h2 id="📍-3-폰트-변경-후-마음에-들지-않을-때"><a href="#📍-3-폰트-변경-후-마음에-들지-않을-때" class="headerlink" title="📍 3. 폰트 변경 후 마음에 들지 않을 때"></a>📍 3. 폰트 변경 후 마음에 들지 않을 때</h2><p>언제든지 <strong>Restore Defaults(기본값으로 복원)</strong> → Apply 하면 초기 상태로 돌아갑니다.</p><p>특히 폰트를 과하게 크게&#x2F;작게 설정했을 경우 유용합니다.</p><hr><h2 id="📍-4-SQL-Editor-폰트-변경하기"><a href="#📍-4-SQL-Editor-폰트-변경하기" class="headerlink" title="📍 4. SQL Editor 폰트 변경하기"></a>📍 4. SQL Editor 폰트 변경하기</h2><p>경로:<br><strong>Window → Preferences → General → Appearance → Colors and Fonts → Text Font</strong></p><p>이 폰트는 <strong>SQL Editor(쿼리 작성 창)</strong> 에 적용됩니다.</p><p>예:</p><ul><li>굵게(Bold)</li><li>폰트 변경</li><li>크기 변경</li></ul><p>적용 후 SQL 창 글꼴이 즉시 변경됨.</p><p>복원이 필요하면 <strong>Restore Defaults</strong> 사용.</p><hr><h2 id="📍-5-색상-Custom-Colors-커스터마이징"><a href="#📍-5-색상-Custom-Colors-커스터마이징" class="headerlink" title="📍 5. 색상(Custom Colors) 커스터마이징"></a>📍 5. 색상(Custom Colors) 커스터마이징</h2><p>Colors and Fonts 메뉴에는 수십 가지 색상 옵션이 존재합니다.</p><p>예)</p><ul><li>Error Color (에러 메시지 색)</li><li>View and Editor folders</li><li>Non-Focused Part Color<br>등등</li></ul><p>커스터마이징 시 실시간으로 UI에 반영됨.</p><p>→ 잘못 바꿔도 <strong>Restore Defaults</strong>로 언제든 복구 가능</p><hr><h2 id="📍-6-Result-Set-Viewer-결과-창-설정-변경"><a href="#📍-6-Result-Set-Viewer-결과-창-설정-변경" class="headerlink" title="📍 6. Result Set Viewer(결과 창) 설정 변경"></a>📍 6. Result Set Viewer(결과 창) 설정 변경</h2><p>경로:<br><strong>Window → Preferences → Teradata Datatools → Result Set Viewer</strong></p><p>여기서 결과창의 여러 설정을 조정할 수 있습니다.</p><h3 id="🔸-1-교차-색상-Row-Alternate-Color-켜기"><a href="#🔸-1-교차-색상-Row-Alternate-Color-켜기" class="headerlink" title="🔸 1) 교차 색상(Row Alternate Color) 켜기"></a>🔸 1) 교차 색상(Row Alternate Color) 켜기</h3><ul><li>‘Display alternate result set rows in a different color’ 체크<br>→ 짝수 행&#x2F;홀수 행이 다른 색으로 표시되어 가독성이 좋아짐</li></ul><h3 id="🔸-2-Column-Header-숨기기-표시하기"><a href="#🔸-2-Column-Header-숨기기-표시하기" class="headerlink" title="🔸 2) Column Header 숨기기 &#x2F; 표시하기"></a>🔸 2) Column Header 숨기기 &#x2F; 표시하기</h3><ul><li>Column header 옵션 OFF → 헤더(컬럼명) 숨기기<br>※ 비추천 (컬럼을 알아보기 어려워짐)</li></ul><h3 id="🔸-3-기본으로-불러오는-최대-Row-수-변경"><a href="#🔸-3-기본으로-불러오는-최대-Row-수-변경" class="headerlink" title="🔸 3) 기본으로 불러오는 최대 Row 수 변경"></a>🔸 3) 기본으로 불러오는 최대 Row 수 변경</h3><p>기본값: 2000<br>예: 1000으로 변경 가능</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.TablesV;</span><br></pre></td></tr></table></figure><p>실행 시:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2,000 rows exceed limit. Retrieve first 2,000?</span><br></pre></td></tr></table></figure><p>이 메시지에 나오는 숫자가 해당 설정에 의해 조정됨.</p><hr><h2 id="📍-7-Result-Viewer-옵션에-빠르게-접근하는-법"><a href="#📍-7-Result-Viewer-옵션에-빠르게-접근하는-법" class="headerlink" title="📍 7. Result Viewer 옵션에 빠르게 접근하는 법"></a>📍 7. Result Viewer 옵션에 빠르게 접근하는 법</h2><p>Result Viewer 우측 상단의 <strong>wrench(🔧 아이콘)</strong> 클릭하면<br>바로 Preferences의 동일한 화면으로 이동할 수 있습니다.</p><hr><h2 id="📍-8-세미콜론-사용-습관-들이기"><a href="#📍-8-세미콜론-사용-습관-들이기" class="headerlink" title="📍 8. 세미콜론(;) 사용 습관 들이기"></a>📍 8. 세미콜론(;) 사용 습관 들이기</h2><p>VTEQ나 여러 SQL 에디터에서는 <strong>각 SQL 문장 뒤에 반드시 세미콜론이 필요</strong>합니다.</p><p>예:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.TablesV;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_DATE</span>;</span><br></pre></td></tr></table></figure><p>여러 SQL을 한 번에 실행할 때 오류를 방지하기 위해<br>항상 세미콜론을 붙이는 것이 좋은 습관입니다.</p><hr><h2 id="📍-9-설정-적용-후-테스트-방법"><a href="#📍-9-설정-적용-후-테스트-방법" class="headerlink" title="📍 9. 설정 적용 후 테스트 방법"></a>📍 9. 설정 적용 후 테스트 방법</h2><p>예를 들어 Dialog Font 크기를 조정한 후 결과를 보고 싶다면:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HELP SESSION;</span><br></pre></td></tr></table></figure><p>혹은</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.TablesV;</span><br></pre></td></tr></table></figure><p>을 실행하면 Result Viewer의 폰트&#x2F;색상 적용 상태를 쉽게 확인할 수 있습니다.</p><hr><h2 id="🎯-최종-요약"><a href="#🎯-최종-요약" class="headerlink" title="🎯 최종 요약"></a>🎯 <strong>최종 요약</strong></h2><p>이번 강의에서는 다음을 배웠습니다:</p><h3 id="✔-화면-레이아웃-조절"><a href="#✔-화면-레이아웃-조절" class="headerlink" title="✔ 화면 레이아웃 조절"></a>✔ 화면 레이아웃 조절</h3><ul><li>하단 창 숨기기&#x2F;보이기</li><li>Reset Perspective로 초기화 가능</li></ul><h3 id="✔-UI-커스터마이징"><a href="#✔-UI-커스터마이징" class="headerlink" title="✔ UI 커스터마이징"></a>✔ UI 커스터마이징</h3><ul><li>Dialog Font</li><li>Text Font (SQL Editor)</li><li>색상(Color)</li><li>Row Alternate Color</li></ul><h3 id="✔-Result-Set-Viewer-설정"><a href="#✔-Result-Set-Viewer-설정" class="headerlink" title="✔ Result Set Viewer 설정"></a>✔ Result Set Viewer 설정</h3><ul><li>최대 row 수 변경</li><li>헤더 표시&#x2F;숨김</li><li>행 색상 교차 표시</li></ul><h3 id="✔-실용-팁"><a href="#✔-실용-팁" class="headerlink" title="✔ 실용 팁"></a>✔ 실용 팁</h3><ul><li>🔧 아이콘으로 빠르게 환경설정 이동</li><li>세미콜론(;) 습관화</li><li>HELP SESSION으로 적용 결과 확인</li></ul><hr><p>필요하다면:</p><ul><li>다음 강의 내용도 md 파일로 생성</li><li>이미지 포함 버전</li><li>GitHub README 스타일</li><li>PDF 변환</li></ul><p>언제든 요청해주세요!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;📘-Teradata-Studio-환경-설정-및-화면-커스터마이징&quot;&gt;&lt;a href=&quot;#📘-Teradata-Studio-환경-설정-및-화면-커스터마이징&quot; class=&quot;headerlink&quot; title=&quot;📘 Teradata Studio 환</summary>
      
    
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/categories/TERADATA/"/>
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/tags/TERADATA/"/>
    
  </entry>
  
  <entry>
    <title>TERADATA(1)-Overview of Teradata Studio Modules</title>
    <link href="https://kish191919.github.io/2025/12/03/TERADATA-1-Overview-of-Teradata-Studio-Modules/"/>
    <id>https://kish191919.github.io/2025/12/03/TERADATA-1-Overview-of-Teradata-Studio-Modules/</id>
    <published>2025-12-03T16:27:48.000Z</published>
    <updated>2025-12-03T16:36:04.790Z</updated>
    
    <content type="html"><![CDATA[<h1 id="📘-Teradata-Studio-기본-화면-이해하기"><a href="#📘-Teradata-Studio-기본-화면-이해하기" class="headerlink" title="📘 Teradata Studio 기본 화면 이해하기"></a>📘 Teradata Studio 기본 화면 이해하기</h1><p><em>기초 UI 설명 + SQL 실행 방법 + 트랜잭션 모드 개념</em></p><h2 id="📍-1-Teradata-Studio-첫-화면-구성"><a href="#📍-1-Teradata-Studio-첫-화면-구성" class="headerlink" title="📍 1. Teradata Studio 첫 화면 구성"></a>📍 1. Teradata Studio 첫 화면 구성</h2><p>Teradata Studio를 실행하면 다음과 같은 주요 영역을 보게 됩니다.</p><h3 id="🔹-1-상단-메뉴-File-Edit-Window-등"><a href="#🔹-1-상단-메뉴-File-Edit-Window-등" class="headerlink" title="🔹 1) 상단 메뉴(File &#x2F; Edit &#x2F; Window 등)"></a>🔹 1) 상단 메뉴(File &#x2F; Edit &#x2F; Window 등)</h3><p>일반적인 파일 관리, 보기 설정, 환경설정 변경 등을 수행합니다.</p><h3 id="🔹-2-Studio-Toolbar"><a href="#🔹-2-Studio-Toolbar" class="headerlink" title="🔹 2) Studio Toolbar"></a>🔹 2) Studio Toolbar</h3><p>쿼리 실행, 저장, 새 연결 생성 등 자주 사용하는 기능 아이콘들이 위치합니다.</p><h3 id="🔹-3-Data-Source-Explorer-좌측-영역"><a href="#🔹-3-Data-Source-Explorer-좌측-영역" class="headerlink" title="🔹 3) Data Source Explorer (좌측 영역)"></a>🔹 3) Data Source Explorer (좌측 영역)</h3><p>Teradata에 연결된 <strong>데이터베이스 &#x2F; 유저 &#x2F; 테이블 &#x2F; 뷰</strong> 등을 탐색할 수 있는 가장 중요한 패널입니다.</p><h3 id="🔹-4-Project-Explorer"><a href="#🔹-4-Project-Explorer" class="headerlink" title="🔹 4) Project Explorer"></a>🔹 4) Project Explorer</h3><p>SQL 파일이나 프로젝트를 로컬에 저장하여 관리할 때 사용합니다.</p><h3 id="🔹-5-SQL-Editor-중앙-상단"><a href="#🔹-5-SQL-Editor-중앙-상단" class="headerlink" title="🔹 5) SQL Editor (중앙 상단)"></a>🔹 5) SQL Editor (중앙 상단)</h3><p>SQL을 작성하는 공간입니다.<br>예:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_TIME</span>;</span><br></pre></td></tr></table></figure><h3 id="🔹-6-Result-Set-Viewer-중앙-하단"><a href="#🔹-6-Result-Set-Viewer-중앙-하단" class="headerlink" title="🔹 6) Result Set Viewer (중앙 &#x2F; 하단)"></a>🔹 6) Result Set Viewer (중앙 &#x2F; 하단)</h3><p>쿼리 실행 결과를 표시합니다.</p><h3 id="🔹-7-SQL-History-하단"><a href="#🔹-7-SQL-History-하단" class="headerlink" title="🔹 7) SQL History (하단)"></a>🔹 7) SQL History (하단)</h3><p>실행한 SQL 기록을 확인할 수 있습니다.</p><h3 id="🔹-8-Perspective-메뉴-오른쪽-상단"><a href="#🔹-8-Perspective-메뉴-오른쪽-상단" class="headerlink" title="🔹 8) Perspective 메뉴 (오른쪽 상단)"></a>🔹 8) Perspective 메뉴 (오른쪽 상단)</h3><ul><li>Administration</li><li>Query Development</li><li>Data Transfer</li></ul><hr><h2 id="📍-2-화면이-망가졌을-때-복구-방법"><a href="#📍-2-화면이-망가졌을-때-복구-방법" class="headerlink" title="📍 2. 화면이 망가졌을 때 복구 방법"></a>📍 2. 화면이 망가졌을 때 복구 방법</h2><p><strong>Window → Reset Perspective</strong></p><hr><h2 id="📍-3-SQL-실행하는-방법"><a href="#📍-3-SQL-실행하는-방법" class="headerlink" title="📍 3. SQL 실행하는 방법"></a>📍 3. SQL 실행하는 방법</h2><h3 id="🔸-방법-1-—-F5-가장-많이-사용"><a href="#🔸-방법-1-—-F5-가장-많이-사용" class="headerlink" title="🔸 방법 1 — F5 (가장 많이 사용)"></a>🔸 방법 1 — <code>F5</code> (가장 많이 사용)</h3><ul><li>선택한 SQL만 실행</li><li>선택하지 않으면 전체 실행</li></ul><h3 id="🔸-방법-2-—-Ctrl-Alt-X"><a href="#🔸-방법-2-—-Ctrl-Alt-X" class="headerlink" title="🔸 방법 2 — Ctrl + Alt + X"></a>🔸 방법 2 — <code>Ctrl + Alt + X</code></h3><p>모든 SQL을 하나의 탭으로 실행합니다.</p><h3 id="🔸-방법-3-—-실행-버튼-클릭"><a href="#🔸-방법-3-—-실행-버튼-클릭" class="headerlink" title="🔸 방법 3 — 실행 버튼 클릭"></a>🔸 방법 3 — 실행 버튼 클릭</h3><p>상단의 ▶️ 아이콘 클릭</p><hr><h2 id="📍-4-SQL-실행-테스트"><a href="#📍-4-SQL-실행-테스트" class="headerlink" title="📍 4. SQL 실행 테스트"></a>📍 4. SQL 실행 테스트</h2><h3 id="🔸-현재-시간-확인"><a href="#🔸-현재-시간-확인" class="headerlink" title="🔸 현재 시간 확인"></a>🔸 현재 시간 확인</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_TIME</span>;</span><br></pre></td></tr></table></figure><h3 id="🔸-DBC-데이터베이스-조회"><a href="#🔸-DBC-데이터베이스-조회" class="headerlink" title="🔸 DBC 데이터베이스 조회"></a>🔸 DBC 데이터베이스 조회</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.DatabasesV;</span><br></pre></td></tr></table></figure><p>DBC는 Teradata의 메타데이터를 보관하는 핵심 데이터베이스입니다.</p><hr><h2 id="📍-5-다중-SQL-실행-시-주의점"><a href="#📍-5-다중-SQL-실행-시-주의점" class="headerlink" title="📍 5. 다중 SQL 실행 시 주의점"></a>📍 5. 다중 SQL 실행 시 주의점</h2><p>각 SQL 문 뒤에는 반드시 **세미콜론(;)**이 필요합니다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> DBC.DatabasesV;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_DATE</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CURRENT_TIME</span>;</span><br></pre></td></tr></table></figure><hr><h2 id="📍-6-HELP-SESSION"><a href="#📍-6-HELP-SESSION" class="headerlink" title="📍 6. HELP SESSION"></a>📍 6. HELP SESSION</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HELP SESSION;</span><br></pre></td></tr></table></figure><p>세션 정보(사용자, 로그인 시간, 모드 등)을 확인할 수 있습니다.</p><hr><h2 id="📍-7-트랜잭션-모드-ANSI-vs-Teradata"><a href="#📍-7-트랜잭션-모드-ANSI-vs-Teradata" class="headerlink" title="📍 7. 트랜잭션 모드 (ANSI vs Teradata)"></a>📍 7. 트랜잭션 모드 (ANSI vs Teradata)</h2><h3 id="🔸-Teradata-모드"><a href="#🔸-Teradata-모드" class="headerlink" title="🔸 Teradata 모드"></a>🔸 Teradata 모드</h3><ul><li>대소문자 구분 적음</li><li>기본 추천 모드</li></ul><h3 id="🔸-ANSI-모드"><a href="#🔸-ANSI-모드" class="headerlink" title="🔸 ANSI 모드"></a>🔸 ANSI 모드</h3><ul><li>표준 SQL 준수</li><li>COMMIT 필요</li><li>대소문자 구분 엄격</li></ul><hr><h2 id="📍-8-트랜잭션-UNIT-OF-WORK-개념"><a href="#📍-8-트랜잭션-UNIT-OF-WORK-개념" class="headerlink" title="📍 8. 트랜잭션(UNIT OF WORK) 개념"></a>📍 8. 트랜잭션(UNIT OF WORK) 개념</h2><p>트랜잭션 &#x3D; 작업 단위<br>전부 성공하거나, 실패 시 전체 롤백되는 방식입니다.</p><p>예: INSERT 중 정전 → 일부 컬럼만 저장되지 않고 전체 취소됨</p><hr><h2 id="🎯-최종-정리"><a href="#🎯-최종-정리" class="headerlink" title="🎯 최종 정리"></a>🎯 최종 정리</h2><ul><li>UI 구성 이해</li><li>SQL 실행 단축키(F5 &#x2F; Ctrl+Alt+X)</li><li>DBC 데이터 조회</li><li>트랜잭션 모드 차이</li><li>HELP SESSION 활용</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;📘-Teradata-Studio-기본-화면-이해하기&quot;&gt;&lt;a href=&quot;#📘-Teradata-Studio-기본-화면-이해하기&quot; class=&quot;headerlink&quot; title=&quot;📘 Teradata Studio 기본 화면 이해하기&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/categories/TERADATA/"/>
    
    
    <category term="TERADATA" scheme="https://kish191919.github.io/tags/TERADATA/"/>
    
  </entry>
  
  <entry>
    <title>Coding_Quiz_SQL_1</title>
    <link href="https://kish191919.github.io/2025/09/17/Coding-Quiz-SQL-1/"/>
    <id>https://kish191919.github.io/2025/09/17/Coding-Quiz-SQL-1/</id>
    <published>2025-09-18T02:07:37.000Z</published>
    <updated>2025-11-20T13:04:44.634Z</updated>
    
    
    
    
    <category term="DEV" scheme="https://kish191919.github.io/categories/DEV/"/>
    
    <category term="CODING_QUIZ_SQL" scheme="https://kish191919.github.io/categories/DEV/CODING-QUIZ-SQL/"/>
    
    
    <category term="LEETCODE" scheme="https://kish191919.github.io/tags/LEETCODE/"/>
    
    <category term="SQL" scheme="https://kish191919.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Coding_Quiz_Python_1</title>
    <link href="https://kish191919.github.io/2025/09/17/Coding-Quiz-Python-1/"/>
    <id>https://kish191919.github.io/2025/09/17/Coding-Quiz-Python-1/</id>
    <published>2025-09-17T20:29:17.000Z</published>
    <updated>2025-11-20T13:04:44.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Two-Sum"><a href="#1-Two-Sum" class="headerlink" title="1. Two Sum"></a>1. Two Sum</h1><p>Link : <a href="https://leetcode.com/problems/two-sum/description/?difficulty=EASY">https://leetcode.com/problems/two-sum/description/?difficulty=EASY</a></p><h2 id="Hash-Map-딕셔너리-이용"><a href="#Hash-Map-딕셔너리-이용" class="headerlink" title="Hash Map (딕셔너리 이용)"></a>Hash Map (딕셔너리 이용)</h2><p>보조 공간을 이용해 빠르게 차이를 찾는 방식.</p><ul><li>시간 복잡도: <strong>O(n)</strong></li><li>공간 복잡도: <strong>O(n)</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        num_map = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            diff = target - num</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> diff <span class="keyword">in</span> num_map:</span><br><span class="line">                <span class="keyword">return</span> [num_map[diff], index]</span><br><span class="line">            </span><br><span class="line">            num_map[num] = index</span><br></pre></td></tr></table></figure><h2 id="Sorting-Two-Pointers-정렬-투-포인터"><a href="#Sorting-Two-Pointers-정렬-투-포인터" class="headerlink" title="Sorting + Two Pointers (정렬 + 투 포인터)"></a>Sorting + Two Pointers (정렬 + 투 포인터)</h2><p>정렬 후 양 끝에서 합을 비교.</p><ul><li>시간 복잡도: <strong>O(n log n)</strong></li><li>공간 복잡도: <strong>O(n)</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line"></span><br><span class="line">    arr = [(num, i) <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums)]</span><br><span class="line">    arr.sort()</span><br><span class="line"></span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        s = arr[left][<span class="number">0</span>] + arr[right][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> s == target:</span><br><span class="line">            <span class="keyword">return</span> [arr[left][<span class="number">1</span>], arr[right][<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">elif</span> s &lt; target:</span><br><span class="line">            left +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right -=<span class="number">1</span></span><br></pre></td></tr></table></figure><h1 id="2-Add-Two-Numbers"><a href="#2-Add-Two-Numbers" class="headerlink" title="2. Add Two Numbers"></a>2. Add Two Numbers</h1><p>Link : <a href="https://leetcode.com/problems/add-two-numbers/description/?difficulty=EASY">https://leetcode.com/problems/add-two-numbers/description/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1: <span class="type">Optional</span>[ListNode], l2: <span class="type">Optional</span>[ListNode], c=<span class="number">0</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">    sum_ = l1.val + l2.val + c</span><br><span class="line">    remain = sum_ % <span class="number">10</span></span><br><span class="line">    carry = sum_ // <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    new_node = ListNode(remain)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> l1.<span class="built_in">next</span> != <span class="literal">None</span> <span class="keyword">or</span> l2.<span class="built_in">next</span> !=<span class="literal">None</span> <span class="keyword">or</span> carry != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> l1.<span class="built_in">next</span> == <span class="literal">None</span>:</span><br><span class="line">            l1.<span class="built_in">next</span> = ListNode(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> l2.<span class="built_in">next</span> == <span class="literal">None</span>:</span><br><span class="line">            l2.<span class="built_in">next</span> = ListNode(<span class="number">0</span>)</span><br><span class="line">        new_node.<span class="built_in">next</span> = <span class="variable language_">self</span>.addTwoNumbers(l1.<span class="built_in">next</span>, l2.<span class="built_in">next</span>, carry)</span><br><span class="line">    <span class="keyword">return</span> new_node</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1: <span class="type">Optional</span>[ListNode], l2: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">        current = dummy</span><br><span class="line">        carry = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2 <span class="keyword">or</span> carry:</span><br><span class="line">            val1 = l1.val <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            val2 = l2.val <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            total = val1 + val2 + carry</span><br><span class="line">            remain = total % <span class="number">10</span></span><br><span class="line">            carry = total // <span class="number">10</span></span><br><span class="line"></span><br><span class="line">            current.<span class="built_in">next</span> = ListNode(remain)</span><br><span class="line">            current = current.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> l1: l1 = l1.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> l2: l2 = l2.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure><h1 id="3-Longest-Substring-Without-Repeating-Characters"><a href="#3-Longest-Substring-Without-Repeating-Characters" class="headerlink" title="3. Longest Substring Without Repeating Characters"></a>3. Longest Substring Without Repeating Characters</h1><p>Link : <a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/?difficulty=EASY">https://leetcode.com/problems/longest-substring-without-repeating-characters/description/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        max_num = <span class="number">0</span></span><br><span class="line">        temp = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            <span class="keyword">while</span> s[i] <span class="keyword">in</span> temp:</span><br><span class="line">                temp.pop(<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            temp.append(s[i])</span><br><span class="line">            max_num = <span class="built_in">max</span>(<span class="built_in">len</span>(temp), max_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_num</span><br></pre></td></tr></table></figure><h2 id="투-포인터-set"><a href="#투-포인터-set" class="headerlink" title="투 포인터 + set"></a>투 포인터 + set</h2><ul><li>같은 슬라이딩 윈도우인데 set으로 포함 여부를 O(1)에 체크.</li><li>왼쪽 포인터(left)를 움직이며 중복을 제거.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        max_num = <span class="number">0</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        temp = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> right, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> ch <span class="keyword">in</span> temp:</span><br><span class="line">                temp.remove(s[left])</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            temp.add(ch)</span><br><span class="line"></span><br><span class="line">            max_num = <span class="built_in">max</span>(max_num, <span class="built_in">len</span>(temp))</span><br><span class="line">        <span class="keyword">return</span> max_num</span><br></pre></td></tr></table></figure><h1 id="5-Longest-Palindromic-Substring"><a href="#5-Longest-Palindromic-Substring" class="headerlink" title="5. Longest Palindromic Substring"></a>5. Longest Palindromic Substring</h1><p>Link : <a href="https://leetcode.com/problems/longest-palindromic-substring/?difficulty=EASY">https://leetcode.com/problems/longest-palindromic-substring/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right +=<span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> s[left+<span class="number">1</span> : right]</span><br><span class="line">        max_str = s[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)-<span class="number">1</span>):</span><br><span class="line">            odd = expand(i, i)</span><br><span class="line">            even = expand(i, i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(odd) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = odd</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(even) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = even</span><br><span class="line">        <span class="keyword">return</span> max_str</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">            <span class="comment"># valid palindrome is s[left+1 : right] (right is exclusive)</span></span><br><span class="line">            <span class="keyword">return</span> left+<span class="number">1</span>, right</span><br><span class="line">        max_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            l1, r1 = expand(i, i)</span><br><span class="line">            l2, r2 = expand(i, i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            odd_str = s[l1:r1]</span><br><span class="line">            even_str = s[l2:r2]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(odd_str) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = odd_str</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(even_str) &gt; <span class="built_in">len</span>(max_str):</span><br><span class="line">                max_str = even_str</span><br><span class="line">        <span class="keyword">return</span> max_str</span><br></pre></td></tr></table></figure><h1 id="6-Zigzag-Conversion"><a href="#6-Zigzag-Conversion" class="headerlink" title="6. Zigzag Conversion"></a>6. Zigzag Conversion</h1><p>Link : <a href="https://leetcode.com/problems/zigzag-conversion/description/?difficulty=EASY">https://leetcode.com/problems/zigzag-conversion/description/?difficulty=EASY</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">self, s: <span class="built_in">str</span>, numRows: <span class="built_in">int</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> numRows == <span class="number">1</span> <span class="keyword">or</span> numRows &gt;= <span class="built_in">len</span>(s):</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        </span><br><span class="line">        rows = [[] <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(numRows)]</span><br><span class="line"></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> s:</span><br><span class="line">            rows[index].append(char)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">                step = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> index == numRows-<span class="number">1</span>:</span><br><span class="line">                step = -<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            index += step</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numRows):</span><br><span class="line">            rows[i] = <span class="string">&#x27;&#x27;</span>.join(rows[i])</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(rows)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-Two-Sum&quot;&gt;&lt;a href=&quot;#1-Two-Sum&quot; class=&quot;headerlink&quot; title=&quot;1. Two Sum&quot;&gt;&lt;/a&gt;1. Two Sum&lt;/h1&gt;&lt;p&gt;Link : &lt;a href=&quot;https://leetcode.com/pro</summary>
      
    
    
    
    <category term="DEV" scheme="https://kish191919.github.io/categories/DEV/"/>
    
    <category term="CODING_QUIZ_PYTHON" scheme="https://kish191919.github.io/categories/DEV/CODING-QUIZ-PYTHON/"/>
    
    
    <category term="PYTHON" scheme="https://kish191919.github.io/tags/PYTHON/"/>
    
    <category term="LEETCODE" scheme="https://kish191919.github.io/tags/LEETCODE/"/>
    
  </entry>
  
  <entry>
    <title>Databricks CV Anomaly Detection</title>
    <link href="https://kish191919.github.io/2025/09/15/Databricks-CV-Anomaly-Detection/"/>
    <id>https://kish191919.github.io/2025/09/15/Databricks-CV-Anomaly-Detection/</id>
    <published>2025-09-15T20:40:23.000Z</published>
    <updated>2025-09-16T01:28:50.347Z</updated>
    
    <content type="html"><![CDATA[<h2 id="👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-Deployment"><a href="#👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-Deployment" class="headerlink" title="👁️ Databricks + Computer Vision Anomaly Detection &amp; Model Deployment"></a>👁️ Databricks + Computer Vision Anomaly Detection &amp; Model Deployment</h2><p><em>A complete guide to anomaly detection with Databricks and Apache Spark</em>  </p><blockquote><p>“From data ingestion to real-time serving — build and deploy scalable computer vision anomaly detection models.”</p></blockquote><p>📎 <strong>Full Project</strong>:<br><a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection">👉 View Jupyter Notebooks on GitHub</a></p><p align="center">  <img src="/images/Anomaly.png" width="80%"></p><hr><h3 id="📌-One-Line-Summary"><a href="#📌-One-Line-Summary" class="headerlink" title="📌 One-Line Summary"></a>📌 One-Line Summary</h3><p>This project provides a full pipeline for <strong>computer vision–based anomaly detection</strong>, covering <strong>data ingestion, preprocessing, model training, deployment, and REST API serving</strong> — all within <strong>Databricks</strong> and powered by <strong>Apache Spark</strong>.</p><hr><h2 id="1️⃣-How-It-Was-Built"><a href="#1️⃣-How-It-Was-Built" class="headerlink" title="1️⃣ How It Was Built"></a>1️⃣ How It Was Built</h2><h3 id="1-Utilities-00-utils-ipynb"><a href="#1-Utilities-00-utils-ipynb" class="headerlink" title="1. Utilities (00_utils.ipynb)"></a><strong>1. Utilities (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/00_utils.ipynb">00_utils.ipynb</a>)</strong></h3><ul><li>Common helper functions for preprocessing and visualization  </li><li>Reusable utilities to streamline workflows</li></ul><hr><h3 id="2-Data-Ingestion-ETL-01-Ingestion-ETL-ipynb"><a href="#2-Data-Ingestion-ETL-01-Ingestion-ETL-ipynb" class="headerlink" title="2. Data Ingestion &amp; ETL (01_Ingestion_ETL.ipynb)"></a><strong>2. Data Ingestion &amp; ETL (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/01_Ingestion_ETL.ipynb">01_Ingestion_ETL.ipynb</a>)</strong></h3><ul><li>Ingested large-scale image datasets into Databricks  </li><li>Implemented Spark-based ETL for scalability  </li><li>Optimized storage and partitioning for performance and cost efficiency </li><li>Image Processing Visualization</li></ul><p align="center">  <img src="/images/image_processing_visualization.png" width="80%"></p><hr><h3 id="3-Deep-Learning-Training-02-HF-Deep-Learning-ipynb"><a href="#3-Deep-Learning-Training-02-HF-Deep-Learning-ipynb" class="headerlink" title="3. Deep Learning Training (02_HF_Deep_Learning.ipynb)"></a><strong>3. Deep Learning Training (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/02_HF_Deep_Learning.ipynb">02_HF_Deep_Learning.ipynb</a>)</strong></h3><ul><li>Applied image preprocessing and augmentation  </li><li>Trained models using <strong>PyTorch + Hugging Face</strong>  </li><li>Evaluated performance with metrics like <strong>Accuracy</strong>, <strong>Loss</strong>, and <strong>PR-AUC</strong></li></ul><hr><h3 id="4-Model-Deployment-03-Model-Deployment-ipynb"><a href="#4-Model-Deployment-03-Model-Deployment-ipynb" class="headerlink" title="4. Model Deployment (03_Model_Deployment.ipynb)"></a><strong>4. Model Deployment (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/03_Model_Deployment.ipynb">03_Model_Deployment.ipynb</a>)</strong></h3><ul><li>Registered trained models in <strong>MLflow</strong>  </li><li>Managed versions for reproducibility  </li><li>Optimized inference pipelines for deployment</li></ul><hr><h3 id="5-Model-Serving-04-Model-Serving-ipynb"><a href="#5-Model-Serving-04-Model-Serving-ipynb" class="headerlink" title="5. Model Serving (04_Model_Serving.ipynb)"></a><strong>5. Model Serving (<a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection/blob/master/Databricks_Code/04_Model_Serving.ipynb">04_Model_Serving.ipynb</a>)</strong></h3><ul><li>Deployed models with <strong>Databricks Model Serving</strong>  </li><li>Exposed REST API endpoints for real-time predictions  </li><li>Integrated anomaly detection into external systems</li></ul><hr><h2 id="2️⃣-Optimization-Best-Practices"><a href="#2️⃣-Optimization-Best-Practices" class="headerlink" title="2️⃣ Optimization &amp; Best Practices"></a>2️⃣ Optimization &amp; Best Practices</h2><ul><li>Spark optimizations for large-scale image data  </li><li>Databricks cluster configuration for <strong>cost efficiency</strong>  </li><li>Strategies for balancing performance and resource usage</li></ul><hr><h2 id="🛠-Technologies-Used"><a href="#🛠-Technologies-Used" class="headerlink" title="🛠 Technologies Used"></a>🛠 Technologies Used</h2><table><thead><tr><th>Step</th><th>Technology</th></tr></thead><tbody><tr><td>Data Processing</td><td>Apache Spark, Databricks</td></tr><tr><td>Deep Learning</td><td>PyTorch, Hugging Face</td></tr><tr><td>Experiment Mgmt</td><td>MLflow</td></tr><tr><td>Deployment</td><td>Databricks Model Registry</td></tr><tr><td>Serving</td><td>REST API, Databricks Serving</td></tr></tbody></table><hr><h2 id="💡-Key-Learnings"><a href="#💡-Key-Learnings" class="headerlink" title="💡 Key Learnings"></a>💡 Key Learnings</h2><ul><li>Full lifecycle ML on Databricks: ingestion → training → deployment → serving  </li><li>How to optimize Databricks for <strong>low-cost, high-performance workflows</strong>  </li><li>Practical experience with model versioning, reproducibility, and API integration</li></ul><hr><h2 id="🔗-GitHub-Repository"><a href="#🔗-GitHub-Repository" class="headerlink" title="🔗 GitHub Repository"></a>🔗 GitHub Repository</h2><p>📂 <a href="https://github.com/kish191919/Databricks_CV_Anomaly_Detection">View Project on GitHub</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-Deployment&quot;&gt;&lt;a href=&quot;#👁️-Databricks-Computer-Vision-Anomaly-Detection-Model-</summary>
      
    
    
    
    <category term="Showcase" scheme="https://kish191919.github.io/categories/Showcase/"/>
    
    
    <category term="Databricks" scheme="https://kish191919.github.io/tags/Databricks/"/>
    
    <category term="Apache Spark" scheme="https://kish191919.github.io/tags/Apache-Spark/"/>
    
    <category term="Computer Vision" scheme="https://kish191919.github.io/tags/Computer-Vision/"/>
    
    <category term="Deep Learning" scheme="https://kish191919.github.io/tags/Deep-Learning/"/>
    
    <category term="MLflow" scheme="https://kish191919.github.io/tags/MLflow/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (6) - Amazon S3 핵심 정리</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-6/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-6/</id>
    <published>2025-09-15T03:49:25.000Z</published>
    <updated>2025-09-15T04:13:20.934Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Amazon-S3-보안-Amazon-S3-Security"><a href="#Amazon-S3-보안-Amazon-S3-Security" class="headerlink" title="Amazon S3 보안 (Amazon S3 Security)"></a>Amazon S3 보안 (Amazon S3 Security)</h1><p>Amazon S3는 단순한 저장소 서비스지만, <strong>보안(Security)</strong> 을 제대로 설정하지 않으면 데이터 유출(Data Leak)과 같은 심각한 문제가 발생할 수 있습니다.  AWS Certified Machine Learning Engineer – Associate 시험에서도 자주 출제되는 주제이므로 꼭 이해해야 합니다.</p><hr><h2 id="1-S3-보안-유형"><a href="#1-S3-보안-유형" class="headerlink" title="1. S3 보안 유형"></a>1. S3 보안 유형</h2><h3 id="🔹-User-Based-IAM-기반"><a href="#🔹-User-Based-IAM-기반" class="headerlink" title="🔹 User-Based (IAM 기반)"></a>🔹 User-Based (IAM 기반)</h3><ul><li><strong>IAM Policies</strong><br>IAM(Identity and Access Management)에서 특정 사용자(User) 또는 그룹(Group)에 대해 어떤 API 호출(API Calls)을 허용할지 정의합니다.<br>→ 예: <code>s3:GetObject</code> 권한 부여.</li></ul><h3 id="🔹-Resource-Based-리소스-기반"><a href="#🔹-Resource-Based-리소스-기반" class="headerlink" title="🔹 Resource-Based (리소스 기반)"></a>🔹 Resource-Based (리소스 기반)</h3><ul><li><p><strong>Bucket Policies</strong>  </p><ul><li>JSON 기반 정책으로, 버킷 전체에 대한 접근 권한을 설정합니다.  </li><li><strong>Cross-Account Access</strong>(계정 간 접근)도 허용 가능.  </li><li>버킷 정책은 <strong>S3 콘솔</strong>에서 직접 작성&#x2F;관리.</li></ul></li><li><p><strong>Object ACL (Access Control List)</strong>  </p><ul><li>객체 단위로 세밀하게 접근 제어.  </li><li>하지만 현재는 <strong>비추천(Deprecated)</strong> → 대부분 <strong>버킷 정책</strong>으로 대체.</li></ul></li><li><p><strong>Bucket ACL</strong>  </p><ul><li>버킷 단위 ACL. 거의 사용하지 않으며 역시 비추천.</li></ul></li></ul><hr><h2 id="2-IAM-권한-평가-규칙"><a href="#2-IAM-권한-평가-규칙" class="headerlink" title="2. IAM 권한 평가 규칙"></a>2. IAM 권한 평가 규칙</h2><p>S3 객체에 접근하려면 다음 조건을 만족해야 합니다:</p><ul><li><strong>IAM 정책이 ALLOW</strong> 이거나 <strong>리소스 정책이 ALLOW</strong>  </li><li>그리고 <strong>명시적 DENY가 없어야 함</strong></li></ul><p>👉 즉, <code>ALLOW OR ALLOW</code> 이면서 동시에 <code>NO DENY</code> 조건이어야 함.</p><hr><h2 id="3-S3-버킷-정책-Bucket-Policies"><a href="#3-S3-버킷-정책-Bucket-Policies" class="headerlink" title="3. S3 버킷 정책 (Bucket Policies)"></a>3. S3 버킷 정책 (Bucket Policies)</h2><ul><li><strong>형식</strong>: JSON 기반 문서  </li><li><strong>구성 요소</strong><ul><li><strong>Resource</strong>: 적용 대상 (버킷&#x2F;객체 ARN)  </li><li><strong>Effect</strong>: <code>Allow</code> 또는 <code>Deny</code>  </li><li><strong>Action</strong>: 허용&#x2F;거부할 API 목록 (<code>s3:GetObject</code>, <code>s3:PutObject</code> 등)  </li><li><strong>Principal</strong>: 정책 적용 대상 (계정, 사용자, 역할, <code>*</code> &#x3D; 모든 사용자)</li></ul></li></ul><h3 id="예시-1-퍼블릭-읽기-Public-Access"><a href="#예시-1-퍼블릭-읽기-Public-Access" class="headerlink" title="예시 1: 퍼블릭 읽기 (Public Access)"></a>예시 1: 퍼블릭 읽기 (Public Access)</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Principal&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;s3:GetObject&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:s3:::my-example-bucket/*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>이 정책은 <code>my-example-bucket</code> 안의 모든 객체(<code>*</code>)를 <strong>누구나 읽기 가능</strong>하도록 허용.</li></ul><hr><h2 id="4-보안-시나리오별-접근-방법"><a href="#4-보안-시나리오별-접근-방법" class="headerlink" title="4. 보안 시나리오별 접근 방법"></a>4. 보안 시나리오별 접근 방법</h2><ul><li><strong>퍼블릭 접근 (Public Access)</strong>  <ul><li>버킷 정책으로 <code>GetObject</code> 권한을 열어줌.  </li><li>단, <strong>Block Public Access 설정</strong>을 해제해야 함.</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-02.png" width="80%"></p><ul><li><strong>내부 사용자 (IAM User)</strong>  <ul><li>IAM 정책으로 권한 부여 (<code>s3:ListBucket</code>, <code>s3:GetObject</code> 등).</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-03.png" width="80%"></p><ul><li><strong>EC2 인스턴스에서 접근</strong>  <ul><li>IAM User 사용 ❌ → <strong>IAM Role</strong>을 EC2에 부여해야 함.</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-04.png" width="80%"></p><ul><li><strong>Cross-Account Access (계정 간 접근)</strong>  <ul><li>다른 AWS 계정의 IAM User가 접근하려면 → <strong>버킷 정책</strong>으로 허용.</li></ul></li></ul><p align="center">  <img src="/images/aws-ml-05.png" width="80%"></p><hr><h2 id="5-Block-Public-Access-설정"><a href="#5-Block-Public-Access-설정" class="headerlink" title="5. Block Public Access 설정"></a>5. Block Public Access 설정</h2><ul><li>AWS가 <strong>데이터 유출 방지(Data Leak Prevention)</strong> 를 위해 도입한 기능.  </li><li>기본적으로 모든 퍼블릭 접근을 막음.  </li><li>✅ 시험 포인트:  <ul><li>버킷을 공개해야 한다면 반드시 <strong>Block Public Access를 해제</strong>해야 함.  </li><li>하지만 <strong>기업 환경에서는 대부분 항상 켜둔다</strong>.  </li><li>계정 레벨에서도 적용 가능 → <strong>모든 버킷이 퍼블릭 차단됨</strong>.</li></ul></li></ul><hr><h2 id="6-암호화-Encryption"><a href="#6-암호화-Encryption" class="headerlink" title="6. 암호화 (Encryption)"></a>6. 암호화 (Encryption)</h2><ul><li><strong>SSE-S3</strong>: S3가 자체적으로 키 관리 (간단, 기본 옵션).  </li><li><strong>SSE-KMS</strong>: AWS KMS(Key Management Service)를 사용해 키 관리 (더 세밀한 보안, 로깅 가능).  </li><li>시험에서는 <strong>SSE-S3와 SSE-KMS 차이점</strong>을 잘 물어봄.</li></ul><hr><h1 id="✅-시험-대비-핵심-정리"><a href="#✅-시험-대비-핵심-정리" class="headerlink" title="✅ 시험 대비 핵심 정리"></a>✅ 시험 대비 핵심 정리</h1><ol><li><p><strong>IAM Policy vs Bucket Policy</strong>  </p><ul><li>IAM Policy: 사용자(User) 관점에서 권한 관리.  </li><li>Bucket Policy: 리소스(Resource) 관점에서 권한 관리.</li></ul></li><li><p><strong>객체 접근 조건</strong>  </p><ul><li>ALLOW(사용자 정책 OR 리소스 정책) + NO DENY.</li></ul></li><li><p><strong>EC2에서 S3 접근</strong> → <strong>IAM Role 사용</strong>.  </p></li><li><p><strong>Cross-Account Access</strong> → <strong>버킷 정책 필요</strong>.  </p></li><li><p><strong>Block Public Access</strong>  </p><ul><li>기본 ON (데이터 유출 방지).  </li><li>시험에서는 “퍼블릭 버킷이 동작하지 않는다 → Block Public Access 때문” 자주 출제.</li></ul></li><li><p><strong>암호화 옵션</strong>  </p><ul><li>SSE-S3 (자동, 간단) vs SSE-KMS (보안·규제 요구사항 대응).</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Amazon-S3-보안-Amazon-S3-Security&quot;&gt;&lt;a href=&quot;#Amazon-S3-보안-Amazon-S3-Security&quot; class=&quot;headerlink&quot; title=&quot;Amazon S3 보안 (Amazon S3 Securi</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
  <entry>
    <title>(한국어) AWS ML Associate (5) - Amazon S3 핵심 정리</title>
    <link href="https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-5/"/>
    <id>https://kish191919.github.io/2025/09/14/KO-AWS-ML-Associate-5/</id>
    <published>2025-09-15T02:33:24.000Z</published>
    <updated>2025-09-15T03:49:06.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Amazon-S3-핵심-정리"><a href="#Amazon-S3-핵심-정리" class="headerlink" title="Amazon S3 핵심 정리"></a>Amazon S3 핵심 정리</h1><blockquote><p><strong>왜 중요한가?</strong><br>Amazon S3(Simple Storage Service)는 AWS의 핵심 스토리지 서비스로, “사실상 무한대(virtually unlimited)” 확장성을 제공하는 <strong>객체 스토리지(Object Storage)</strong> 입니다. 대부분의 데이터&#x2F;AI 워크로드가 S3를 중심으로 연결되며, 다른 AWS 서비스와의 통합성이 매우 뛰어납니다.</p></blockquote><hr><h2 id="1-S3가-쓰이는-곳-Use-Cases"><a href="#1-S3가-쓰이는-곳-Use-Cases" class="headerlink" title="1) S3가 쓰이는 곳 (Use Cases)"></a>1) S3가 쓰이는 곳 (Use Cases)</h2><ul><li><strong>백업 &amp; 스토리지(Backup &amp; Storage)</strong>: 장기 보관, 스냅샷 저장.</li><li><strong>재해복구(Disaster Recovery, DR)</strong>: 다른 <strong>리전(Region)</strong> 으로 복제해 RTO&#x2F;RPO 개선.</li><li><strong>아카이브(Archive)</strong>: 저비용 보관(<strong>S3 Glacier</strong> 계열) 후 필요 시 복원.</li><li><strong>하이브리드 스토리지(Hybrid Cloud Storage)</strong>: 온프레미스 + 클라우드 연동.</li><li><strong>애플리케이션&#x2F;미디어 호스팅(App&#x2F;Media Hosting)</strong>: 이미지&#x2F;동영상&#x2F;정적 파일 제공.</li><li><strong>데이터 레이크(Data Lake) &amp; 빅데이터 분석(Big Data Analytics)</strong>: 원천 데이터 저장 후 <strong>Athena&#x2F;Glue&#x2F;Lake Formation</strong> 등으로 분석.</li><li><strong>소프트웨어 배포(Software Delivery)</strong>: 설치 파일, 모델 아티팩트 배포.</li><li><strong>정적 웹사이트(Static Website)</strong>: 정적 호스팅 + <strong>CloudFront(CDN)</strong> 로 가속.</li></ul><blockquote><p><strong>MLA-C01 포인트</strong>: S3는 <strong>SageMaker</strong>와 함께 자주 출제됩니다. 학습&#x2F;추론 데이터 저장, 모델 아티팩트 저장, <strong>Athena&#x2F;Glue</strong>와 연계한 피처 추출 파이프라인 등. S3 경로 표기(<strong>S3 URI</strong>)와 권한 모델을 익히세요.</p></blockquote><hr><h2 id="2-버킷-Bucket-—-최상위-네임스페이스"><a href="#2-버킷-Bucket-—-최상위-네임스페이스" class="headerlink" title="2) 버킷(Bucket) — 최상위 네임스페이스"></a>2) 버킷(Bucket) — 최상위 네임스페이스</h2><ul><li><strong>정의</strong>: 객체(파일)를 담는 <strong>컨테이너</strong>. UI가 폴더처럼 보여도, 실제로는 <strong>버킷&#x2F;키(key)</strong> 기반의 평면 구조.</li><li><strong>글로벌 유니크 이름(Global Unique Name)</strong>: <strong>전 세계 모든 계정&#x2F;리전에서 유일</strong>해야 함.</li><li><strong>리전 범위(Region-scoped)</strong>: 버킷은 <strong>특정 리전</strong>에 생성됩니다. (S3가 글로벌처럼 보여도 생성 위치는 리전)</li><li><strong>네이밍 규칙(Naming Rules)</strong> (대표 규칙만 발췌)<ul><li>영문 소문자&#x2F;숫자&#x2F;하이픈만 사용, <strong>대문자&#x2F;언더스코어 불가</strong></li><li>길이 3~63자</li><li>IP 형태 금지(예: <code>192.168.0.1</code>)</li><li><code>xn--</code> 로 시작 금지, <code>-s3alias</code> 로 끝 금지</li></ul></li><li><strong>권장 초기 설정</strong><ul><li><strong>ACL 비활성화(ACLs disabled, Bucket owner enforced)</strong>  </li><li><strong>퍼블릭 접근 차단(Block Public Access)</strong>: 기본 on  </li><li><strong>서버사이드 암호화(Server-Side Encryption, SSE)</strong>: 기본 <strong>SSE-S3</strong> 또는 <strong>SSE-KMS</strong></li></ul></li></ul><blockquote><p><strong>시험 포인트</strong>  </p><ul><li>“버킷 이름 글로벌 유일”과 “버킷은 리전에 종속”을 구분하세요.  </li><li>보안 기본값: <strong>Block Public Access &#x3D; ON</strong>, <strong>ACL 비권장</strong>, <strong>SSE 적용</strong>.</li></ul></blockquote><hr><h2 id="3-객체-Object-키-Key"><a href="#3-객체-Object-키-Key" class="headerlink" title="3) 객체(Object) &amp; 키(Key)"></a>3) 객체(Object) &amp; 키(Key)</h2><ul><li><strong>키(Key) &#x3D; 전체 경로(Full Path)</strong>  <ul><li>예: <code>s3://my-bucket/my_folder1/another_folder/my_file.txt</code>  </li><li><strong>Prefix + Object Name</strong> 조합 (디렉터리 개념은 UI 편의일 뿐, 실제로는 긴 문자열 경로)</li></ul></li><li><strong>크기 제한(Size Limits)</strong><ul><li><strong>최대 5TB</strong></li><li><strong>5GB 초과 업로드는 반드시 <em>멀티파트 업로드(Multi-part Upload)</em></strong> 사용</li></ul></li><li><strong>메타데이터(Metadata)</strong>: 시스템&#x2F;사용자 정의 Key-Value</li><li><strong>태그(Tags)</strong>: 최대 10쌍, 비용&#x2F;수명주기&#x2F;보안 정책에 유용</li><li><strong>버전 ID(Version ID)</strong>: <strong>버전 관리(Versioning)</strong> 활성화 시 부여</li></ul><blockquote><p><strong>시험 포인트</strong>  </p><ul><li><strong>5GB 초과 → Multi-part Upload 필수</strong>  </li><li><strong>Versioning</strong>: 삭제&#x2F;덮어쓰기 보호, <strong>MFA Delete</strong> 와 함께 보안 강화</li></ul></blockquote><hr><h2 id="4-접근-방법-퍼블릭-URL-vs-프리사인드-URL"><a href="#4-접근-방법-퍼블릭-URL-vs-프리사인드-URL" class="headerlink" title="4) 접근 방법: 퍼블릭 URL vs 프리사인드 URL"></a>4) 접근 방법: 퍼블릭 URL vs 프리사인드 URL</h2><ul><li><strong>Public URL</strong>: 객체가 <strong>퍼블릭</strong>이어야 접근 가능(기본은 차단됨).</li><li><strong>Pre-signed URL</strong>: <strong>임시 권한이 서명된 URL</strong>. 비공개 객체도 <strong>서명 만료 시간 동안</strong> 접근 가능. 안전한 1회성 공유에 적합.</li></ul><blockquote><p><strong>시험 포인트</strong>: “객체는 비공개인데 외부와 잠깐 공유하고 싶다” → <strong>Pre-signed URL</strong> 정답.</p></blockquote><hr><h2 id="5-필수-보안-운영-기능-Security-Operations"><a href="#5-필수-보안-운영-기능-Security-Operations" class="headerlink" title="5) 필수 보안&#x2F;운영 기능 (Security &amp; Operations)"></a>5) 필수 보안&#x2F;운영 기능 (Security &amp; Operations)</h2><ul><li><strong>암호화(Encryption)</strong><ul><li><strong>SSE-S3</strong>(Amazon S3 managed keys) — 가장 간단</li><li><strong>SSE-KMS</strong>(AWS KMS keys) — <strong>키 사용 권한&#x2F;감사</strong> 필요 시</li><li><strong>CSE(Client-Side Encryption)</strong> — 클라이언트에서 암호화 후 업로드</li></ul></li><li><strong>버전 관리(Versioning)</strong> + <strong>MFA Delete</strong>: 실수&#x2F;랜섬웨어 대응</li><li><strong>수명주기(Lifecycle) 정책</strong>: <strong>Standard → IA → Glacier</strong> 티어링, 자동 만료&#x2F;아카이브</li><li><strong>복제(Replication)</strong><ul><li><strong>CRR(Cross-Region Replication)</strong>: DR&#x2F;지연단축</li><li><strong>SRR(Same-Region Replication)</strong>: 멀티계정&#x2F;멀티버킷 분리</li></ul></li><li><strong>액세스 제어(Access Control)</strong><ul><li><strong>IAM 정책(IAM Policy)</strong>, <strong>버킷 정책(Bucket Policy)</strong> 중심</li><li><strong>S3 Access Points</strong>&#x2F;**VPC 엔드포인트(Interface&#x2F;Gateway)**로 네트워크 격리</li></ul></li><li><strong>감사&#x2F;로깅(Audit&#x2F;Logging)</strong>: <strong>CloudTrail</strong>, <strong>Server Access Logging</strong>, <strong>S3 Object Ownership</strong></li></ul><blockquote><p><strong>시험 포인트</strong>  </p><ul><li><strong>SSE-KMS</strong> 선택 시 <strong>KMS 키 권한</strong>(Encrypt&#x2F;Decrypt&#x2F;API호출)이 추가로 필요.  </li><li><strong>CRR</strong>은 <strong>버전닝이 양쪽 모두 활성화</strong>되어야 작동. 소유권&#x2F;권한 이슈 자주 출제.</li></ul></blockquote><hr><h2 id="6-정적-웹사이트-CDN"><a href="#6-정적-웹사이트-CDN" class="headerlink" title="6) 정적 웹사이트 &amp; CDN"></a>6) 정적 웹사이트 &amp; CDN</h2><ul><li><strong>Static Website Hosting</strong>: S3 정적 웹 사이트 엔드포인트 사용(퍼블릭 접근 필요).  </li><li><strong>CloudFront</strong> 앞단 배치 권장: OAC(Origin Access Control)로 <strong>S3는 비공개</strong>, <strong>CloudFront만 접근</strong> → 성능&#x2F;보안 모두 향상.</li></ul><blockquote><p><strong>시험 포인트</strong>: “S3를 퍼블릭으로 열지 않고 정적 사이트 제공?” → <strong>CloudFront + OAC</strong> 패턴.</p></blockquote><hr><h2 id="7-데이터-레이크-패턴-for-ML-Analytics"><a href="#7-데이터-레이크-패턴-for-ML-Analytics" class="headerlink" title="7) 데이터 레이크 패턴 (for ML&#x2F;Analytics)"></a>7) 데이터 레이크 패턴 (for ML&#x2F;Analytics)</h2><ul><li><strong>원천 저장</strong>: S3에 원본 데이터 적재(스키마 온 리드, <em>Schema-on-Read</em>).</li><li><strong>카탈로그</strong>: <strong>AWS Glue Data Catalog</strong></li><li><strong>쿼리&#x2F;탐색</strong>: <strong>Amazon Athena</strong>(서버리스 SQL), <strong>Redshift Spectrum</strong></li><li><strong>거버넌스</strong>: <strong>Lake Formation</strong>(권한&#x2F;데이터 액세스 제어)</li><li><strong>형식 최적화</strong>: <strong>Parquet&#x2F;ORC</strong> 등 <strong>컬럼나형(Columnar)</strong> 포맷 권장 (스캔&#x2F;비용 절감)</li></ul><blockquote><p><strong>시험 포인트</strong>: “S3 + Athena 비용 최적화?” → <strong>Parquet + 파티셔닝(Partitioning) + 프리픽스 설계</strong>.</p></blockquote><hr><h2 id="8-콘솔에서-자주-하는-작업-요약"><a href="#8-콘솔에서-자주-하는-작업-요약" class="headerlink" title="8) 콘솔에서 자주 하는 작업 요약"></a>8) 콘솔에서 자주 하는 작업 요약</h2><ol><li><strong>버킷 생성(Create Bucket)</strong>: 리전 선택 → 이름 지정(글로벌 유일) → <strong>Block Public Access ON</strong>, <strong>SSE 설정</strong>  </li><li><strong>객체 업로드(Upload)</strong>: 단일&#x2F;멀티파트 선택(5GB 초과 시 멀티파트)  </li><li><strong>폴더처럼 사용하기</strong>: 접두사(prefix)로 논리적 구분(실제 디렉터리는 아님)  </li><li><strong>프리사인드 URL 생성</strong>: 일시적 외부 공유  </li><li><strong>버킷 정책&#x2F;액세스 포인트</strong>: 세밀 권한&#x2F;네트워크 격리 설정</li></ol><hr><h2 id="9-자주-나오는-함정-Exam-Gotchas"><a href="#9-자주-나오는-함정-Exam-Gotchas" class="headerlink" title="9) 자주 나오는 함정(Exam Gotchas)"></a>9) 자주 나오는 함정(Exam Gotchas)</h2><ul><li><strong>디렉터리 개념 없음</strong>: 키 문자열에 <code>/</code>를 써서 <strong>prefix</strong>를 흉내낼 뿐.</li><li><strong>5GB 업로드 제한</strong>: 초과 시 <strong>Multi-part Upload</strong> 필수.</li><li><strong>CRR 조건</strong>: <strong>양쪽 버킷 Versioning ON</strong> + 권한&#x2F;소유권 고려.</li><li><strong>KMS 사용 시 권한 오류</strong>: KMS 키 정책&#x2F;Grant&#x2F;IAM 권한 누락 체크.</li><li><strong>퍼블릭 차단이 기본</strong>: 정적 사이트&#x2F;퍼블릭 파일 배포는 <strong>CloudFront</strong> 경유가 안전.</li></ul><hr><h2 id="10-ML-엔지니어를-위한-S3-빠른-체크리스트"><a href="#10-ML-엔지니어를-위한-S3-빠른-체크리스트" class="headerlink" title="10) ML 엔지니어를 위한 S3 빠른 체크리스트"></a>10) ML 엔지니어를 위한 S3 빠른 체크리스트</h2><ul><li><input disabled="" type="checkbox"> <strong>데이터 저장 포맷</strong>: CSV → <strong>Parquet</strong> 변환 고려(성능&#x2F;비용)  </li><li><input disabled="" type="checkbox"> <strong>접근 제어</strong>: IAM Role 기반 최소 권한(least privilege)  </li><li><input disabled="" type="checkbox"> <strong>암호화</strong>: SSE-KMS 기본, 키 권한 점검(파이프라인&#x2F;노트북&#x2F;배치)  </li><li><input disabled="" type="checkbox"> <strong>수명주기 정책</strong>: 학습 로그&#x2F;중간 산출물 자동 정리  </li><li><input disabled="" type="checkbox"> <strong>경로 규약</strong>: <code>s3://bucket/project/dataset/partition=.../</code> 일관성  </li><li><input disabled="" type="checkbox"> <strong>프리사인드 URL</strong>: 일시적 데이터 공유&#x2F;검수 자동화에 활용</li></ul><hr><h3 id="핵심-용어-요약-KR-EN"><a href="#핵심-용어-요약-KR-EN" class="headerlink" title="핵심 용어 요약 (KR&#x2F;EN)"></a>핵심 용어 요약 (KR&#x2F;EN)</h3><ul><li>버킷 <strong>Bucket</strong> &#x2F; 객체 <strong>Object</strong> &#x2F; 키 <strong>Key</strong> &#x2F; 접두사 <strong>Prefix</strong> &#x2F; 버전 관리 <strong>Versioning</strong>  </li><li>프리사인드 URL <strong>Pre-signed URL</strong> &#x2F; 수명주기 정책 <strong>Lifecycle Policy</strong>  </li><li>서버사이드 암호화 <strong>Server-Side Encryption (SSE-S3 &#x2F; SSE-KMS)</strong>  </li><li>교차 리전 복제 <strong>Cross-Region Replication (CRR)</strong> &#x2F; 동일 리전 복제 <strong>Same-Region Replication (SRR)</strong>  </li><li>데이터 레이크 <strong>Data Lake</strong> &#x2F; 스키마 온 리드 <strong>Schema-on-Read</strong> &#x2F; 컬럼나 <strong>Columnar</strong></li></ul><hr><h2 id="추가-참고-심화"><a href="#추가-참고-심화" class="headerlink" title="추가 참고(심화)"></a>추가 참고(심화)</h2><ul><li><strong>S3 Storage Classes</strong>: Standard &#x2F; Standard-IA &#x2F; One Zone-IA &#x2F; Intelligent-Tiering &#x2F; Glacier Instant&#x2F;Flx&#x2F;Deep Archive  </li><li><strong>네트워크 최적화</strong>: <strong>S3 Transfer Acceleration</strong>, 멀티파트 병렬 업로드, VPC 엔드포인트  </li><li><strong>비용 관리</strong>: S3 Storage Lens, Lifecycle&#x2F;Intelligent-Tiering, 파티셔닝&#x2F;프리픽스 설계</li></ul><hr><p><strong>요약</strong>: S3는 ML&#x2F;분석 워크로드의 “공용 데이터 허브”입니다. <strong>보안 기본값</strong>, <strong>멀티파트 업로드</strong>, <strong>Versioning&#x2F;CRR</strong>, <strong>SSE-KMS</strong>, <strong>Athena+Parquet</strong> 같은 패턴을 확실히 익히면 MLA-C01에서 고득점할 수 있습니다.</p><h1 id="Amazon-S3-암호화-방식-쉽게-설명하기"><a href="#Amazon-S3-암호화-방식-쉽게-설명하기" class="headerlink" title="Amazon S3 암호화 방식 쉽게 설명하기"></a>Amazon S3 암호화 방식 쉽게 설명하기</h1><p>Amazon S3에서 데이터를 저장할 때 보안을 위해 <strong>서버 사이드 암호화(Server-side encryption, SSE)</strong> 를 사용할 수 있습니다.<br>대표적으로 자주 쓰이는 두 가지 방식이 있습니다.</p><hr><h2 id="1-SSE-S3-Server-side-encryption-with-Amazon-S3-managed-keys"><a href="#1-SSE-S3-Server-side-encryption-with-Amazon-S3-managed-keys" class="headerlink" title="1. SSE-S3 (Server-side encryption with Amazon S3 managed keys)"></a>1. SSE-S3 (Server-side encryption with Amazon S3 managed keys)</h2><ul><li><p><strong>설명</strong><br>S3가 직접 암호화 키를 관리해주는 방식입니다.<br>사용자가 따로 키를 만들거나 관리할 필요가 없습니다.<br>데이터를 업로드하면 S3가 자동으로 암호화하고, 다운로드하면 자동으로 복호화해 줍니다.</p></li><li><p><strong>특징</strong></p><ul><li>사용하기 가장 쉽습니다 (추가 설정 거의 필요 없음).</li><li>암호화 키는 <strong>Amazon S3가 전적으로 관리</strong>합니다.</li><li><code>AES-256</code> 암호화 알고리즘을 사용합니다.</li><li>비용은 추가로 발생하지 않습니다.</li></ul></li><li><p><strong>시험 포인트 (AWS Certified ML Engineer Associate)</strong>  </p><ul><li><strong>SSE-S3는 S3가 키를 관리한다</strong>라는 점이 핵심.  </li><li>옵션을 활성화하기만 하면 끝 (운영 편리성 ↑).  </li><li>보안 규제가 강하지 않거나 단순 저장이 목적일 때 적합.</li></ul></li></ul><hr><h2 id="2-SSE-KMS-Server-side-encryption-with-AWS-Key-Management-Service-keys"><a href="#2-SSE-KMS-Server-side-encryption-with-AWS-Key-Management-Service-keys" class="headerlink" title="2. SSE-KMS (Server-side encryption with AWS Key Management Service keys)"></a>2. SSE-KMS (Server-side encryption with AWS Key Management Service keys)</h2><ul><li><p><strong>설명</strong><br>AWS Key Management Service(KMS)를 통해 암호화 키를 관리하는 방식입니다.<br>즉, 암호화 키를 직접 생성, 관리, 권한 제어할 수 있습니다.<br>더 세밀한 보안 관리가 필요한 경우에 사용됩니다.</p></li><li><p><strong>특징</strong></p><ul><li><strong>고객이 키를 직접 관리 (Customer managed keys)</strong> 가능.  </li><li>키 사용에 대한 <strong>CloudTrail 로그</strong>로 추적 가능 → 누가, 언제, 어떤 키를 사용했는지 알 수 있음.  </li><li>IAM 정책을 통해 세밀하게 접근 제어 가능 (예: 특정 사용자만 복호화 가능).  </li><li>KMS 호출 비용이 발생합니다.</li></ul></li><li><p><strong>시험 포인트</strong></p><ul><li><strong>SSE-KMS는 KMS와 통합되어 있어 세밀한 보안·감사 관리 가능</strong>.  </li><li>규제가 있는 산업(금융, 헬스케어 등)에서는 SSE-KMS가 요구될 수 있음.  </li><li>비용과 성능(추가 API 호출)도 고려해야 함.</li></ul></li></ul><hr><h2 id="비교-요약"><a href="#비교-요약" class="headerlink" title="비교 요약"></a>비교 요약</h2><table><thead><tr><th>항목</th><th>SSE-S3</th><th>SSE-KMS</th></tr></thead><tbody><tr><td>키 관리</td><td>Amazon S3 자동 관리</td><td>AWS KMS에서 직접 관리 가능</td></tr><tr><td>보안 수준</td><td>기본적 (단순 암호화)</td><td>고급 (세밀한 제어, 로깅)</td></tr><tr><td>비용</td><td>추가 비용 없음</td><td>KMS 사용 비용 발생</td></tr><tr><td>주요 특징</td><td>가장 간단, 자동 처리</td><td>IAM·CloudTrail 통합, 규제 준수에 적합</td></tr></tbody></table><hr><p>✅ <strong>시험 대비 핵심</strong>  </p><ul><li>SSE-S3: <strong>S3가 키 관리</strong>, 간단, 저비용  </li><li>SSE-KMS: <strong>KMS와 통합</strong>, 세밀한 제어, 감사 로그, 비용 발생</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Amazon-S3-핵심-정리&quot;&gt;&lt;a href=&quot;#Amazon-S3-핵심-정리&quot; class=&quot;headerlink&quot; title=&quot;Amazon S3 핵심 정리&quot;&gt;&lt;/a&gt;Amazon S3 핵심 정리&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;stro</summary>
      
    
    
    
    <category term="CERTIFICATION" scheme="https://kish191919.github.io/categories/CERTIFICATION/"/>
    
    <category term="AWS_ML_ASSOCIATE_KR" scheme="https://kish191919.github.io/categories/CERTIFICATION/AWS-ML-ASSOCIATE-KR/"/>
    
    
    <category term="AWS" scheme="https://kish191919.github.io/tags/AWS/"/>
    
    <category term="KOREAN" scheme="https://kish191919.github.io/tags/KOREAN/"/>
    
    <category term="AWS_ML_ASSOCIATE" scheme="https://kish191919.github.io/tags/AWS-ML-ASSOCIATE/"/>
    
  </entry>
  
</feed>
